{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3de49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327ca4ed",
   "metadata": {},
   "source": [
    "Some useful global constants and setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac54c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "CORR_THRESHOLD = 0.85 # correlation threshold for dimensionality reduction\n",
    "TEST_SIZE = 0.30 # train-test split\n",
    "VAL_SIZE = 0.20  # train-val split   \n",
    "N_TRIALS = 50                     \n",
    "MAX_PRETRAIN_EPOCHS = 150\n",
    "MAX_FINETUNE_EPOCHS = 200\n",
    "EARLY_STOPPING_PATIENCE = 30\n",
    "BATCH_SIZE = 2048\n",
    "VIRTUAL_BATCH_SIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FOCAL_ALPHA = 0.75                 # Higher alpha for rare positive class\n",
    "FOCAL_GAMMA = 2.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8958f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee8245",
   "metadata": {},
   "source": [
    "Soem useful helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f425c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc_ci(y_true, y_scores, n_bootstraps=2000, ci=0.95):\n",
    "    \"\"\" \n",
    "    Simple Bootstrapping method to get an confidence interval on the AUROC score.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    aucs = []\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = rng.integers(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        aucs.append(roc_auc_score(y_true[idx], y_scores[idx]))\n",
    "\n",
    "    lower = np.percentile(aucs, (1 - ci) / 2 * 100)\n",
    "    upper = np.percentile(aucs, (1 + ci) / 2 * 100)\n",
    "    return np.mean(aucs), lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933896b6",
   "metadata": {},
   "source": [
    "## Loading the dataset, pre-processing, and analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f71e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>anion_gap_mean</th>\n",
       "      <th>anion_gap_sd</th>\n",
       "      <th>anion_gap_min</th>\n",
       "      <th>anion_gap_max</th>\n",
       "      <th>bicarbonate_mean</th>\n",
       "      <th>bicarbonate_sd</th>\n",
       "      <th>bicarbonate_min</th>\n",
       "      <th>bicarbonate_max</th>\n",
       "      <th>calcium_total_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>urea_nitrogen_min</th>\n",
       "      <th>urea_nitrogen_max</th>\n",
       "      <th>white_blood_cells_mean</th>\n",
       "      <th>white_blood_cells_sd</th>\n",
       "      <th>white_blood_cells_min</th>\n",
       "      <th>white_blood_cells_max</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>icu_los_hours</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200003</td>\n",
       "      <td>13.375000</td>\n",
       "      <td>3.583195</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>3.105295</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.771429</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.471429</td>\n",
       "      <td>13.176711</td>\n",
       "      <td>13.2</td>\n",
       "      <td>43.9</td>\n",
       "      <td>48</td>\n",
       "      <td>M</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200007</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>1.272792</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200009</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.471429</td>\n",
       "      <td>1.471637</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.3</td>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200014</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>2.203028</td>\n",
       "      <td>10.7</td>\n",
       "      <td>14.7</td>\n",
       "      <td>85</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30484</th>\n",
       "      <td>299992</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>2.856153</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.125000</td>\n",
       "      <td>2.609556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.307143</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.134783</td>\n",
       "      <td>3.781727</td>\n",
       "      <td>8.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>299993</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>1.341641</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>2.073644</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.605530</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>299994</td>\n",
       "      <td>16.157895</td>\n",
       "      <td>2.477973</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.631579</td>\n",
       "      <td>3.451417</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>10.076190</td>\n",
       "      <td>2.642329</td>\n",
       "      <td>5.3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>74</td>\n",
       "      <td>F</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>299998</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1.210372</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>87</td>\n",
       "      <td>M</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>299999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>3.394113</td>\n",
       "      <td>15.9</td>\n",
       "      <td>20.7</td>\n",
       "      <td>49</td>\n",
       "      <td>M</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30489 rows Ã— 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       icustay_id  anion_gap_mean  anion_gap_sd  anion_gap_min  anion_gap_max  \\\n",
       "0          200003       13.375000      3.583195            9.0           21.0   \n",
       "1          200007       15.500000      2.121320           14.0           17.0   \n",
       "2          200009        9.500000      2.121320            8.0           11.0   \n",
       "3          200012             NaN           NaN            NaN            NaN   \n",
       "4          200014       10.000000      1.732051            9.0           12.0   \n",
       "...           ...             ...           ...            ...            ...   \n",
       "30484      299992       15.375000      2.856153           11.0           25.0   \n",
       "30485      299993        9.400000      1.341641            8.0           11.0   \n",
       "30486      299994       16.157895      2.477973           13.0           24.0   \n",
       "30487      299998       11.500000      1.732051           10.0           14.0   \n",
       "30488      299999             NaN           NaN            NaN            NaN   \n",
       "\n",
       "       bicarbonate_mean  bicarbonate_sd  bicarbonate_min  bicarbonate_max  \\\n",
       "0             25.250000        3.105295             18.0             28.0   \n",
       "1             23.000000        1.414214             22.0             24.0   \n",
       "2             23.333333        2.081666             21.0             25.0   \n",
       "3                   NaN             NaN              NaN              NaN   \n",
       "4             24.000000        1.000000             23.0             25.0   \n",
       "...                 ...             ...              ...              ...   \n",
       "30484         23.125000        2.609556             15.0             26.0   \n",
       "30485         29.600000        2.073644             26.0             31.0   \n",
       "30486         21.631579        3.451417             17.0             31.0   \n",
       "30487         23.500000        1.290994             22.0             25.0   \n",
       "30488         24.000000             NaN             24.0             24.0   \n",
       "\n",
       "       calcium_total_mean  ...  urea_nitrogen_min  urea_nitrogen_max  \\\n",
       "0                7.771429  ...               10.0               21.0   \n",
       "1                8.900000  ...                8.0               10.0   \n",
       "2                8.000000  ...               15.0               21.0   \n",
       "3                     NaN  ...                NaN                NaN   \n",
       "4                7.733333  ...               21.0               24.0   \n",
       "...                   ...  ...                ...                ...   \n",
       "30484            8.307143  ...                8.0               23.0   \n",
       "30485            8.000000  ...               12.0               15.0   \n",
       "30486            8.100000  ...               28.0               63.0   \n",
       "30487            8.800000  ...               20.0               22.0   \n",
       "30488                 NaN  ...               11.0               13.0   \n",
       "\n",
       "       white_blood_cells_mean  white_blood_cells_sd  white_blood_cells_min  \\\n",
       "0                   26.471429             13.176711                   13.2   \n",
       "1                   10.300000              1.272792                    9.4   \n",
       "2                   12.471429              1.471637                   10.5   \n",
       "3                    4.900000                   NaN                    4.9   \n",
       "4                   13.233333              2.203028                   10.7   \n",
       "...                       ...                   ...                    ...   \n",
       "30484               14.134783              3.781727                    8.1   \n",
       "30485               12.600000              0.605530                   12.0   \n",
       "30486               10.076190              2.642329                    5.3   \n",
       "30487                9.900000              1.210372                    7.9   \n",
       "30488               18.300000              3.394113                   15.9   \n",
       "\n",
       "       white_blood_cells_max  age  gender  icu_los_hours  target  \n",
       "0                       43.9   48       M            141       0  \n",
       "1                       11.2   44       M             30       0  \n",
       "2                       14.3   47       F             51       0  \n",
       "3                        4.9   33       F             10       0  \n",
       "4                       14.7   85       M             41       0  \n",
       "...                      ...  ...     ...            ...     ...  \n",
       "30484                   22.1   41       M            499       0  \n",
       "30485                   13.3   26       M             67       0  \n",
       "30486                   14.5   74       F            152       1  \n",
       "30487                   11.0   87       M             46       1  \n",
       "30488                   20.7   49       M             31       0  \n",
       "\n",
       "[30489 rows x 93 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort_data = pd.read_csv('../cohort_data_new.csv')\n",
    "cohort_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e82ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (30489, 93)\n",
      "Readmission rate: 10.74%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {cohort_data.shape}\")\n",
    "print(f\"Readmission rate: {cohort_data['target'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff3a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_cols = [\n",
    "    'anion_gap_mean', 'anion_gap_min', 'anion_gap_max', 'anion_gap_sd',\n",
    "    'bicarbonate_mean', 'bicarbonate_min', 'bicarbonate_max', 'bicarbonate_sd',\n",
    "    'calcium_total_mean', 'calcium_total_min', 'calcium_total_max', 'calcium_total_sd',\n",
    "    'chloride_mean', 'chloride_min', 'chloride_max', 'chloride_sd',\n",
    "    'creatinine_mean', 'creatinine_min', 'creatinine_max', 'creatinine_sd',\n",
    "    'glucose_mean', 'glucose_min', 'glucose_max', 'glucose_sd',\n",
    "    'hematocrit_mean', 'hematocrit_min', 'hematocrit_max', 'hematocrit_sd',\n",
    "    'hemoglobin_mean', 'hemoglobin_min', 'hemoglobin_max', 'hemoglobin_sd',\n",
    "    'mchc_mean', 'mchc_min', 'mchc_max', 'mchc_sd',\n",
    "    'mcv_mean', 'mcv_min', 'mcv_max', 'mcv_sd',\n",
    "    'magnesium_mean', 'magnesium_min', 'magnesium_max', 'magnesium_sd',\n",
    "    'pt_mean', 'pt_min', 'pt_max', 'pt_sd',\n",
    "    'phosphate_mean', 'phosphate_min', 'phosphate_max', 'phosphate_sd',\n",
    "    'platelet_count_mean', 'platelet_count_min', 'platelet_count_max', 'platelet_count_sd',\n",
    "    'potassium_mean', 'potassium_min', 'potassium_max', 'potassium_sd',\n",
    "    'rdw_mean', 'rdw_min', 'rdw_max', 'rdw_sd',\n",
    "    'red_blood_cells_mean', 'red_blood_cells_min', 'red_blood_cells_max', 'red_blood_cells_sd',\n",
    "    'sodium_mean', 'sodium_min', 'sodium_max', 'sodium_sd',\n",
    "    'urea_nitrogen_mean', 'urea_nitrogen_min', 'urea_nitrogen_max', 'urea_nitrogen_sd',\n",
    "    'white_blood_cells_mean', 'white_blood_cells_min', 'white_blood_cells_max', 'white_blood_cells_sd',\n",
    "    'age', 'icu_los_hours'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66668fd",
   "metadata": {},
   "source": [
    "REmove the ICUstay_id and the gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1464d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [c for c in cohort_data.columns if 'icustay_id' in c.lower() or 'gender' in c.lower()]\n",
    "df = cohort_data.drop(columns=['icustay_id', 'gender'], errors='ignore')\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473f929",
   "metadata": {},
   "source": [
    "Trying out some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial feature matrix shape: (30489, 90)\n",
      "final feature matrix shape: (30489, 98)\n"
     ]
    }
   ],
   "source": [
    "# X = X.select_dtypes(include=['number']).replace([np.inf, -np.inf], np.nan)\n",
    "# print(f\"initial feature matrix shape: {X.shape}\")\n",
    "\n",
    "# def create_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df_eng = df.copy()\n",
    "    \n",
    "#     # BUN/Creatinine ratio (kidney function indicator)\n",
    "#     if 'urea_nitrogen_mean' in df_eng.columns and 'creatinine_mean' in df_eng.columns:\n",
    "#         df_eng['bun_creatinine_ratio'] = (\n",
    "#             df_eng['urea_nitrogen_mean'] / (df_eng['creatinine_mean'] + 1e-6)\n",
    "#         )\n",
    "\n",
    "#     # Variability indices (physiological instability)\n",
    "#     variability_features = []\n",
    "#     for base_name in ['glucose', 'potassium', 'sodium', 'hemoglobin']:\n",
    "#         mean_col = f'{base_name}_mean'\n",
    "#         sd_col = f'{base_name}_sd'\n",
    "#         if mean_col in df_eng.columns and sd_col in df_eng.columns:\n",
    "#             cv_col = f'{base_name}_cv'\n",
    "#             df_eng[cv_col] = df_eng[sd_col] / (df_eng[mean_col] + 1e-6)\n",
    "#             variability_features.append(cv_col)\n",
    "    \n",
    "#     # Range features (max - min)\n",
    "#     for base_name in ['glucose', 'creatinine', 'potassium']:\n",
    "#         max_col = f'{base_name}_max'\n",
    "#         min_col = f'{base_name}_min'\n",
    "#         if max_col in df_eng.columns and min_col in df_eng.columns:\n",
    "#             range_col = f'{base_name}_range'\n",
    "#             df_eng[range_col] = df_eng[max_col] - df_eng[min_col]\n",
    "    \n",
    "#     return df_eng\n",
    "\n",
    "# X_engineered = create_engineered_features(X)\n",
    "# print(f\"final feature matrix shape: {X_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd98d7b",
   "metadata": {},
   "source": [
    "Dimensionality reduction by deleting the columns with high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1931de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = X_engineered.corr().abs()\n",
    "# upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "# to_drop = [col for col in upper.columns if any(upper[col] >= CORR_THRESHOLD)]\n",
    "\n",
    "# print(f\"dropping {len(to_drop)} features\")\n",
    "# X_reduced = X_engineered.drop(columns=to_drop, errors='ignore')\n",
    "# print(f\"final feature count: {X_reduced.shape[1]}\")\n",
    "# feature_names = X_reduced.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8e811",
   "metadata": {},
   "source": [
    "Creating the final train-val-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d3ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 17073 samples (10.74% readmission)\n",
      "Validation set: 4269 samples (10.75% readmission)\n",
      "Test set: 9147 samples (10.75% readmission)\n"
     ]
    }
   ],
   "source": [
    "# separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y.values, test_size=TEST_SIZE, random_state=SEED, stratify=y.values)\n",
    "\n",
    "# separate validation set \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp)\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({y_train.mean()*100:.2f}% readmission)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({y_val.mean()*100:.2f}% readmission)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({y_test.mean()*100:.2f}% readmission)\")\n",
    "\n",
    "# SimpleImputation using median strat and scaling;\n",
    "# Imputation - FIT on train only and avoidning data leakage:\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scaling - FIT on train only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439a817",
   "metadata": {},
   "source": [
    "### Pretraining the TABNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff4acb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 12.34471| val_0_unsup_loss_numpy: 3.271250009536743|  0:00:03s\n",
      "epoch 1  | loss: 11.24094| val_0_unsup_loss_numpy: 3.1070899963378906|  0:00:06s\n",
      "epoch 2  | loss: 9.99972 | val_0_unsup_loss_numpy: 3.025629997253418|  0:00:09s\n",
      "epoch 3  | loss: 9.19899 | val_0_unsup_loss_numpy: 3.0878798961639404|  0:00:12s\n",
      "epoch 4  | loss: 8.46691 | val_0_unsup_loss_numpy: 3.0285000801086426|  0:00:14s\n",
      "epoch 5  | loss: 7.69079 | val_0_unsup_loss_numpy: 3.1906800270080566|  0:00:17s\n",
      "epoch 6  | loss: 7.14098 | val_0_unsup_loss_numpy: 3.362839937210083|  0:00:20s\n",
      "epoch 7  | loss: 6.58374 | val_0_unsup_loss_numpy: 2.852479934692383|  0:00:23s\n",
      "epoch 8  | loss: 6.17838 | val_0_unsup_loss_numpy: 2.8965299129486084|  0:00:25s\n",
      "epoch 9  | loss: 5.66219 | val_0_unsup_loss_numpy: 2.9274098873138428|  0:00:28s\n",
      "epoch 10 | loss: 5.28782 | val_0_unsup_loss_numpy: 2.619230031967163|  0:00:32s\n",
      "epoch 11 | loss: 4.74309 | val_0_unsup_loss_numpy: 2.4046199321746826|  0:00:36s\n",
      "epoch 12 | loss: 4.38431 | val_0_unsup_loss_numpy: 2.471679925918579|  0:00:39s\n",
      "epoch 13 | loss: 4.07394 | val_0_unsup_loss_numpy: 2.308459997177124|  0:00:43s\n",
      "epoch 14 | loss: 3.79128 | val_0_unsup_loss_numpy: 2.2663700580596924|  0:00:46s\n",
      "epoch 15 | loss: 3.46785 | val_0_unsup_loss_numpy: 2.386320114135742|  0:00:50s\n",
      "epoch 16 | loss: 3.27985 | val_0_unsup_loss_numpy: 2.0832700729370117|  0:00:53s\n",
      "epoch 17 | loss: 3.07515 | val_0_unsup_loss_numpy: 2.2126901149749756|  0:00:57s\n",
      "epoch 18 | loss: 2.87484 | val_0_unsup_loss_numpy: 2.0941998958587646|  0:01:01s\n",
      "epoch 19 | loss: 2.71087 | val_0_unsup_loss_numpy: 2.0283899307250977|  0:01:05s\n",
      "epoch 20 | loss: 2.52724 | val_0_unsup_loss_numpy: 2.027549982070923|  0:01:09s\n",
      "epoch 21 | loss: 2.34187 | val_0_unsup_loss_numpy: 2.0918800830841064|  0:01:13s\n",
      "epoch 22 | loss: 2.23084 | val_0_unsup_loss_numpy: 1.815790057182312|  0:01:18s\n",
      "epoch 23 | loss: 2.08277 | val_0_unsup_loss_numpy: 1.5987999439239502|  0:01:23s\n",
      "epoch 24 | loss: 1.95832 | val_0_unsup_loss_numpy: 1.6896799802780151|  0:01:27s\n",
      "epoch 25 | loss: 1.87489 | val_0_unsup_loss_numpy: 1.6670299768447876|  0:01:32s\n",
      "epoch 26 | loss: 1.7572  | val_0_unsup_loss_numpy: 1.6377899646759033|  0:01:36s\n",
      "epoch 27 | loss: 1.68194 | val_0_unsup_loss_numpy: 1.5211800336837769|  0:01:41s\n",
      "epoch 28 | loss: 1.6109  | val_0_unsup_loss_numpy: 1.4351500272750854|  0:01:45s\n",
      "epoch 29 | loss: 1.53306 | val_0_unsup_loss_numpy: 1.4438199996948242|  0:01:48s\n",
      "epoch 30 | loss: 1.50271 | val_0_unsup_loss_numpy: 1.351580023765564|  0:01:52s\n",
      "epoch 31 | loss: 1.43287 | val_0_unsup_loss_numpy: 1.250849962234497|  0:01:56s\n",
      "epoch 32 | loss: 1.38562 | val_0_unsup_loss_numpy: 1.2981300354003906|  0:02:00s\n",
      "epoch 33 | loss: 1.33567 | val_0_unsup_loss_numpy: 1.2021499872207642|  0:02:03s\n",
      "epoch 34 | loss: 1.29299 | val_0_unsup_loss_numpy: 1.229599952697754|  0:02:07s\n",
      "epoch 35 | loss: 1.25298 | val_0_unsup_loss_numpy: 1.1950500011444092|  0:02:12s\n",
      "epoch 36 | loss: 1.20297 | val_0_unsup_loss_numpy: 1.150089979171753|  0:02:16s\n",
      "epoch 37 | loss: 1.18446 | val_0_unsup_loss_numpy: 1.1543300151824951|  0:02:20s\n",
      "epoch 38 | loss: 1.1489  | val_0_unsup_loss_numpy: 1.126420021057129|  0:02:24s\n",
      "epoch 39 | loss: 1.1259  | val_0_unsup_loss_numpy: 1.135640025138855|  0:02:28s\n",
      "epoch 40 | loss: 1.10227 | val_0_unsup_loss_numpy: 1.0897599458694458|  0:02:32s\n",
      "epoch 41 | loss: 1.08839 | val_0_unsup_loss_numpy: 1.0632599592208862|  0:02:36s\n",
      "epoch 42 | loss: 1.06897 | val_0_unsup_loss_numpy: 1.0384299755096436|  0:02:39s\n",
      "epoch 43 | loss: 1.0523  | val_0_unsup_loss_numpy: 1.0222100019454956|  0:02:43s\n",
      "epoch 44 | loss: 1.04993 | val_0_unsup_loss_numpy: 1.0264500379562378|  0:02:47s\n",
      "epoch 45 | loss: 1.03163 | val_0_unsup_loss_numpy: 1.0149799585342407|  0:02:51s\n",
      "epoch 46 | loss: 1.0228  | val_0_unsup_loss_numpy: 1.0179200172424316|  0:02:54s\n",
      "epoch 47 | loss: 1.01488 | val_0_unsup_loss_numpy: 1.0014599561691284|  0:02:58s\n",
      "epoch 48 | loss: 1.00055 | val_0_unsup_loss_numpy: 0.9830600023269653|  0:03:01s\n",
      "epoch 49 | loss: 0.98646 | val_0_unsup_loss_numpy: 0.9732800126075745|  0:03:05s\n",
      "epoch 50 | loss: 0.99061 | val_0_unsup_loss_numpy: 0.9653499722480774|  0:03:08s\n",
      "epoch 51 | loss: 0.9733  | val_0_unsup_loss_numpy: 0.951740026473999|  0:03:11s\n",
      "epoch 52 | loss: 0.96709 | val_0_unsup_loss_numpy: 0.9425600171089172|  0:03:15s\n",
      "epoch 53 | loss: 0.95711 | val_0_unsup_loss_numpy: 0.9445099830627441|  0:03:18s\n",
      "epoch 54 | loss: 0.96163 | val_0_unsup_loss_numpy: 0.9432299733161926|  0:03:21s\n",
      "epoch 55 | loss: 0.95071 | val_0_unsup_loss_numpy: 0.9283199906349182|  0:03:24s\n",
      "epoch 56 | loss: 0.94193 | val_0_unsup_loss_numpy: 0.9254099726676941|  0:03:28s\n",
      "epoch 57 | loss: 0.94925 | val_0_unsup_loss_numpy: 0.9130100011825562|  0:03:32s\n",
      "epoch 58 | loss: 0.92725 | val_0_unsup_loss_numpy: 0.9027000069618225|  0:03:35s\n",
      "epoch 59 | loss: 0.93142 | val_0_unsup_loss_numpy: 0.9023600220680237|  0:03:39s\n",
      "epoch 60 | loss: 0.92774 | val_0_unsup_loss_numpy: 0.9008899927139282|  0:03:42s\n",
      "epoch 61 | loss: 0.91649 | val_0_unsup_loss_numpy: 0.8830599784851074|  0:03:46s\n",
      "epoch 62 | loss: 0.91978 | val_0_unsup_loss_numpy: 0.8878200054168701|  0:03:49s\n",
      "epoch 63 | loss: 0.92216 | val_0_unsup_loss_numpy: 0.8761699795722961|  0:03:53s\n",
      "epoch 64 | loss: 0.91028 | val_0_unsup_loss_numpy: 0.8699899911880493|  0:03:57s\n",
      "epoch 65 | loss: 0.908   | val_0_unsup_loss_numpy: 0.864799976348877|  0:04:00s\n",
      "epoch 66 | loss: 0.89738 | val_0_unsup_loss_numpy: 0.8604300022125244|  0:04:04s\n",
      "epoch 67 | loss: 0.90309 | val_0_unsup_loss_numpy: 0.8554400205612183|  0:04:08s\n",
      "epoch 68 | loss: 0.89525 | val_0_unsup_loss_numpy: 0.8517299890518188|  0:04:12s\n",
      "epoch 69 | loss: 0.90173 | val_0_unsup_loss_numpy: 0.8482900261878967|  0:04:16s\n",
      "epoch 70 | loss: 0.88968 | val_0_unsup_loss_numpy: 0.8443800210952759|  0:04:19s\n",
      "epoch 71 | loss: 0.89017 | val_0_unsup_loss_numpy: 0.8475800156593323|  0:04:23s\n",
      "epoch 72 | loss: 0.8867  | val_0_unsup_loss_numpy: 0.8406400084495544|  0:04:26s\n",
      "epoch 73 | loss: 0.89026 | val_0_unsup_loss_numpy: 0.8394100069999695|  0:04:30s\n",
      "epoch 74 | loss: 0.8915  | val_0_unsup_loss_numpy: 0.8368600010871887|  0:04:34s\n",
      "epoch 75 | loss: 0.88195 | val_0_unsup_loss_numpy: 0.8320099711418152|  0:04:37s\n",
      "epoch 76 | loss: 0.87231 | val_0_unsup_loss_numpy: 0.8280799984931946|  0:04:41s\n",
      "epoch 77 | loss: 0.8731  | val_0_unsup_loss_numpy: 0.8248900175094604|  0:04:44s\n",
      "epoch 78 | loss: 0.87312 | val_0_unsup_loss_numpy: 0.8227699995040894|  0:04:48s\n",
      "epoch 79 | loss: 0.86445 | val_0_unsup_loss_numpy: 0.8212000131607056|  0:04:52s\n",
      "epoch 80 | loss: 0.86331 | val_0_unsup_loss_numpy: 0.8180400133132935|  0:04:55s\n",
      "epoch 81 | loss: 0.8639  | val_0_unsup_loss_numpy: 0.8155999779701233|  0:04:59s\n",
      "epoch 82 | loss: 0.85844 | val_0_unsup_loss_numpy: 0.8124399781227112|  0:05:02s\n",
      "epoch 83 | loss: 0.85718 | val_0_unsup_loss_numpy: 0.8087700009346008|  0:05:06s\n",
      "epoch 84 | loss: 0.86181 | val_0_unsup_loss_numpy: 0.8039100170135498|  0:05:10s\n",
      "epoch 85 | loss: 0.84651 | val_0_unsup_loss_numpy: 0.8044099807739258|  0:05:13s\n",
      "epoch 86 | loss: 0.84989 | val_0_unsup_loss_numpy: 0.7984399795532227|  0:05:17s\n",
      "epoch 87 | loss: 0.85996 | val_0_unsup_loss_numpy: 0.797029972076416|  0:05:20s\n",
      "epoch 88 | loss: 0.84282 | val_0_unsup_loss_numpy: 0.7938200235366821|  0:05:24s\n",
      "epoch 89 | loss: 0.84378 | val_0_unsup_loss_numpy: 0.9176200032234192|  0:05:27s\n",
      "epoch 90 | loss: 0.83923 | val_0_unsup_loss_numpy: 0.8268700242042542|  0:05:31s\n",
      "epoch 91 | loss: 0.8518  | val_0_unsup_loss_numpy: 0.9228900074958801|  0:05:34s\n",
      "epoch 92 | loss: 0.8439  | val_0_unsup_loss_numpy: 0.8267499804496765|  0:05:38s\n",
      "epoch 93 | loss: 0.83763 | val_0_unsup_loss_numpy: 0.824150025844574|  0:05:42s\n",
      "epoch 94 | loss: 0.83556 | val_0_unsup_loss_numpy: 0.7771300077438354|  0:05:45s\n",
      "epoch 95 | loss: 0.83047 | val_0_unsup_loss_numpy: 0.7801499962806702|  0:05:48s\n",
      "epoch 96 | loss: 0.837   | val_0_unsup_loss_numpy: 0.7782099843025208|  0:05:52s\n",
      "epoch 97 | loss: 0.83207 | val_0_unsup_loss_numpy: 0.7758100032806396|  0:05:55s\n",
      "epoch 98 | loss: 0.82785 | val_0_unsup_loss_numpy: 0.772849977016449|  0:05:59s\n",
      "epoch 99 | loss: 0.83046 | val_0_unsup_loss_numpy: 0.7659900188446045|  0:06:02s\n",
      "epoch 100| loss: 0.82468 | val_0_unsup_loss_numpy: 0.7617300152778625|  0:06:05s\n",
      "epoch 101| loss: 0.82628 | val_0_unsup_loss_numpy: 0.7643899917602539|  0:06:09s\n",
      "epoch 102| loss: 0.81656 | val_0_unsup_loss_numpy: 0.7613700032234192|  0:06:12s\n",
      "epoch 103| loss: 0.8303  | val_0_unsup_loss_numpy: 0.7572299838066101|  0:06:16s\n",
      "epoch 104| loss: 0.81969 | val_0_unsup_loss_numpy: 0.7573800086975098|  0:06:19s\n",
      "epoch 105| loss: 0.82372 | val_0_unsup_loss_numpy: 0.7559800148010254|  0:06:23s\n",
      "epoch 106| loss: 0.81909 | val_0_unsup_loss_numpy: 0.7570499777793884|  0:06:26s\n",
      "epoch 107| loss: 0.81827 | val_0_unsup_loss_numpy: 0.7560499906539917|  0:06:30s\n",
      "epoch 108| loss: 0.81425 | val_0_unsup_loss_numpy: 0.7521700263023376|  0:06:33s\n",
      "epoch 109| loss: 0.81274 | val_0_unsup_loss_numpy: 0.746999979019165|  0:06:36s\n",
      "epoch 110| loss: 0.81618 | val_0_unsup_loss_numpy: 0.7434099912643433|  0:06:39s\n",
      "epoch 111| loss: 0.81172 | val_0_unsup_loss_numpy: 0.7434999942779541|  0:06:42s\n",
      "epoch 112| loss: 0.80664 | val_0_unsup_loss_numpy: 0.7411400079727173|  0:06:45s\n",
      "epoch 113| loss: 0.81134 | val_0_unsup_loss_numpy: 0.7385600209236145|  0:06:49s\n",
      "epoch 114| loss: 0.80202 | val_0_unsup_loss_numpy: 0.737280011177063|  0:06:52s\n",
      "epoch 115| loss: 0.80635 | val_0_unsup_loss_numpy: 0.7354599833488464|  0:06:55s\n",
      "epoch 116| loss: 0.80075 | val_0_unsup_loss_numpy: 0.7336699962615967|  0:06:59s\n",
      "epoch 117| loss: 0.80533 | val_0_unsup_loss_numpy: 0.7321900129318237|  0:07:02s\n",
      "epoch 118| loss: 0.79331 | val_0_unsup_loss_numpy: 0.7302299737930298|  0:07:05s\n",
      "epoch 119| loss: 0.79816 | val_0_unsup_loss_numpy: 0.7276999950408936|  0:07:09s\n",
      "epoch 120| loss: 0.80013 | val_0_unsup_loss_numpy: 0.7249199748039246|  0:07:12s\n",
      "epoch 121| loss: 0.78832 | val_0_unsup_loss_numpy: 0.7283400297164917|  0:07:15s\n",
      "epoch 122| loss: 0.79414 | val_0_unsup_loss_numpy: 0.7265400290489197|  0:07:18s\n",
      "epoch 123| loss: 0.80388 | val_0_unsup_loss_numpy: 0.7200800180435181|  0:07:21s\n",
      "epoch 124| loss: 0.78869 | val_0_unsup_loss_numpy: 0.7213799953460693|  0:07:24s\n",
      "epoch 125| loss: 0.7891  | val_0_unsup_loss_numpy: 0.7666000127792358|  0:07:27s\n",
      "epoch 126| loss: 0.79018 | val_0_unsup_loss_numpy: 0.7739999890327454|  0:07:30s\n",
      "epoch 127| loss: 0.78954 | val_0_unsup_loss_numpy: 0.7511799931526184|  0:07:33s\n",
      "epoch 128| loss: 0.79461 | val_0_unsup_loss_numpy: 0.7155200242996216|  0:07:36s\n",
      "epoch 129| loss: 0.78376 | val_0_unsup_loss_numpy: 0.7155799865722656|  0:07:39s\n",
      "epoch 130| loss: 0.78708 | val_0_unsup_loss_numpy: 0.7157300114631653|  0:07:43s\n",
      "epoch 131| loss: 0.77035 | val_0_unsup_loss_numpy: 0.7105600237846375|  0:07:46s\n",
      "epoch 132| loss: 0.78071 | val_0_unsup_loss_numpy: 0.7128099799156189|  0:07:49s\n",
      "epoch 133| loss: 0.78508 | val_0_unsup_loss_numpy: 0.7164899706840515|  0:07:52s\n",
      "epoch 134| loss: 0.78328 | val_0_unsup_loss_numpy: 0.7432199716567993|  0:07:55s\n",
      "epoch 135| loss: 0.78458 | val_0_unsup_loss_numpy: 0.7761300206184387|  0:07:58s\n",
      "epoch 136| loss: 0.78179 | val_0_unsup_loss_numpy: 0.7800700068473816|  0:08:01s\n",
      "epoch 137| loss: 0.78494 | val_0_unsup_loss_numpy: 0.7710599899291992|  0:08:04s\n",
      "epoch 138| loss: 0.78209 | val_0_unsup_loss_numpy: 0.791450023651123|  0:08:07s\n",
      "epoch 139| loss: 0.78235 | val_0_unsup_loss_numpy: 0.7981600165367126|  0:08:10s\n",
      "epoch 140| loss: 0.77128 | val_0_unsup_loss_numpy: 0.7920699715614319|  0:08:13s\n",
      "epoch 141| loss: 0.77719 | val_0_unsup_loss_numpy: 0.8071100115776062|  0:08:16s\n",
      "epoch 142| loss: 0.77626 | val_0_unsup_loss_numpy: 0.7571399807929993|  0:08:19s\n",
      "epoch 143| loss: 0.77481 | val_0_unsup_loss_numpy: 0.7464900016784668|  0:08:22s\n",
      "epoch 144| loss: 0.77672 | val_0_unsup_loss_numpy: 0.7685400247573853|  0:08:25s\n",
      "epoch 145| loss: 0.78392 | val_0_unsup_loss_numpy: 0.7426000237464905|  0:08:28s\n",
      "epoch 146| loss: 0.76594 | val_0_unsup_loss_numpy: 0.7220399975776672|  0:08:31s\n",
      "epoch 147| loss: 0.7744  | val_0_unsup_loss_numpy: 0.7228800058364868|  0:08:34s\n",
      "epoch 148| loss: 0.7559  | val_0_unsup_loss_numpy: 0.7253699898719788|  0:08:37s\n",
      "epoch 149| loss: 0.75757 | val_0_unsup_loss_numpy: 0.7329999804496765|  0:08:41s\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 131 and best_val_0_unsup_loss_numpy = 0.7105600237846375\n",
      "Pretraining complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "def run_pretraining(X_train: np.ndarray, X_val: np.ndarray, pretrain_params: dict):\n",
    "    \"\"\"Run unsupervised pretraining\"\"\"\n",
    "    pretrainer = TabNetPretrainer(**pretrain_params)\n",
    "    pretrainer.fit(\n",
    "        X_train=X_train,\n",
    "        eval_set=[X_val],\n",
    "        max_epochs=MAX_PRETRAIN_EPOCHS,\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    print(\"Pretraining complete!\")\n",
    "    return pretrainer\n",
    "\n",
    "pretrain_params = dict(\n",
    "    n_d=32, \n",
    "    n_a=32,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-3),\n",
    "    mask_type=\"entmax\",\n",
    "    device_name=DEVICE\n",
    ")\n",
    "\n",
    "pretrainer = run_pretraining(X_train_scaled, X_val_scaled, pretrain_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b5c7a",
   "metadata": {},
   "source": [
    "Hyperparam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86a7959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(X_train, y_train, X_val, y_val, pretrainer):\n",
    "    \"\"\"Create Optuna objective function\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Hyperparameters to tune\n",
    "        n_d = trial.suggest_int(\"n_d\", 16, 64)\n",
    "        n_a = trial.suggest_int(\"n_a\", 16, 64)\n",
    "        n_steps = trial.suggest_int(\"n_steps\", 3, 7)\n",
    "        gamma = trial.suggest_float(\"gamma\", 1.0, 2.5)\n",
    "        lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        mask_type = trial.suggest_categorical(\"mask_type\", [\"sparsemax\", \"entmax\"])\n",
    "        \n",
    "        clf = TabNetClassifier(n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma, lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,optimizer_params=dict(lr=lr), mask_type=mask_type, device_name=DEVICE, verbose=0)\n",
    "        \n",
    "        try:\n",
    "            clf.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_name=[\"val\"],\n",
    "                eval_metric=[\"auc\"],\n",
    "                max_epochs=MAX_FINETUNE_EPOCHS,\n",
    "                patience=20, \n",
    "                batch_size=BATCH_SIZE,\n",
    "                virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "                num_workers=0,\n",
    "                drop_last=False,\n",
    "                from_unsupervised=pretrainer,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        pred_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, pred_proba)\n",
    "        \n",
    "        return auc\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f1154f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 08:14:38,931] A new study created in memory with name: tabnet_readmission_10pct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter optimization under way\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e991ef1be5374996a1aa591f4dd01537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 98 with best_epoch = 78 and best_val_auc = 0.60442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 08:19:46,023] Trial 0 finished with value: 0.6044196272851514 and parameters: {'n_d': 18, 'n_a': 16, 'n_steps': 3, 'gamma': 2.033045226025318, 'lambda_sparse': 0.00014661664502985748, 'lr': 0.0002983613163810673, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.6044196272851514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 45 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 22 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 195 with best_epoch = 175 and best_val_auc = 0.6489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 08:31:29,848] Trial 1 finished with value: 0.6489029557579813 and parameters: {'n_d': 22, 'n_a': 45, 'n_steps': 7, 'gamma': 1.5685858968175923, 'lambda_sparse': 3.5651468876369945e-06, 'lr': 0.00043195168127290416, 'mask_type': 'sparsemax'}. Best is trial 1 with value: 0.6489029557579813.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 39 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 44 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 91 with best_epoch = 71 and best_val_auc = 0.62293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 08:38:21,339] Trial 2 finished with value: 0.6229344289480154 and parameters: {'n_d': 44, 'n_a': 39, 'n_steps': 6, 'gamma': 1.311666929999685, 'lambda_sparse': 2.636230921113233e-06, 'lr': 0.000663048492764778, 'mask_type': 'sparsemax'}. Best is trial 1 with value: 0.6489029557579813.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 55 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 138 with best_epoch = 118 and best_val_auc = 0.66382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 08:50:13,908] Trial 3 finished with value: 0.6638195552353342 and parameters: {'n_d': 21, 'n_a': 55, 'n_steps': 6, 'gamma': 1.0984980106465843, 'lambda_sparse': 2.5076929589126155e-06, 'lr': 0.0004933357422732249, 'mask_type': 'entmax'}. Best is trial 3 with value: 0.6638195552353342.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 28 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 132 with best_epoch = 112 and best_val_auc = 0.61762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:00:18,877] Trial 4 finished with value: 0.6176173239782936 and parameters: {'n_d': 32, 'n_a': 28, 'n_steps': 5, 'gamma': 1.9110611282235221, 'lambda_sparse': 0.0005008706036628327, 'lr': 0.00027422576685063536, 'mask_type': 'sparsemax'}. Best is trial 3 with value: 0.6638195552353342.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 17 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 42 and best_val_auc = 0.60574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:04:25,054] Trial 5 finished with value: 0.6057393969544657 and parameters: {'n_d': 32, 'n_a': 17, 'n_steps': 5, 'gamma': 2.383063291676091, 'lambda_sparse': 0.0008669481247850811, 'lr': 0.000935723285923363, 'mask_type': 'entmax'}. Best is trial 3 with value: 0.6638195552353342.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 37 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 46 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 134 with best_epoch = 114 and best_val_auc = 0.61073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:13:28,989] Trial 6 finished with value: 0.6107342791301413 and parameters: {'n_d': 46, 'n_a': 37, 'n_steps': 7, 'gamma': 1.3257625796457084, 'lambda_sparse': 1.2198898031490479e-06, 'lr': 0.00021946096543762233, 'mask_type': 'sparsemax'}. Best is trial 3 with value: 0.6638195552353342.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 28 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_val_auc = 0.52314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:15:02,627] Trial 7 finished with value: 0.523144288336507 and parameters: {'n_d': 28, 'n_a': 63, 'n_steps': 3, 'gamma': 2.370038476932514, 'lambda_sparse': 1.0865607426812601e-06, 'lr': 0.00030585618574312706, 'mask_type': 'entmax'}. Best is trial 3 with value: 0.6638195552353342.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 44 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 54 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_auc = 0.52868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:16:39,379] Trial 8 finished with value: 0.5286806877898432 and parameters: {'n_d': 54, 'n_a': 44, 'n_steps': 6, 'gamma': 2.311996215490513, 'lambda_sparse': 1.1554645357642597e-05, 'lr': 0.00017267253312817468, 'mask_type': 'entmax'}. Best is trial 3 with value: 0.6638195552353342.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 17 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 159 with best_epoch = 139 and best_val_auc = 0.63583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:25:11,278] Trial 9 finished with value: 0.6358296307732776 and parameters: {'n_d': 17, 'n_a': 63, 'n_steps': 5, 'gamma': 2.0076551734606176, 'lambda_sparse': 0.0001626610272939941, 'lr': 0.00046765622380281016, 'mask_type': 'sparsemax'}. Best is trial 3 with value: 0.6638195552353342.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 54 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 50 and best_val_auc = 0.67686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:28:12,811] Trial 10 finished with value: 0.6768622876388819 and parameters: {'n_d': 63, 'n_a': 54, 'n_steps': 4, 'gamma': 1.1067122979255082, 'lambda_sparse': 1.825499161823432e-05, 'lr': 0.002640349555139673, 'mask_type': 'entmax'}. Best is trial 10 with value: 0.6768622876388819.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 54 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 42 and best_val_auc = 0.67627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:30:47,195] Trial 11 finished with value: 0.6762727371496863 and parameters: {'n_d': 63, 'n_a': 54, 'n_steps': 4, 'gamma': 1.0180464216150498, 'lambda_sparse': 2.1932688704884334e-05, 'lr': 0.0029947412787386498, 'mask_type': 'entmax'}. Best is trial 10 with value: 0.6768622876388819.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 53 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 60 with best_epoch = 40 and best_val_auc = 0.68583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:33:33,910] Trial 12 finished with value: 0.6858347771887991 and parameters: {'n_d': 64, 'n_a': 53, 'n_steps': 4, 'gamma': 1.0221896225632354, 'lambda_sparse': 2.7885294628576305e-05, 'lr': 0.0033257933569175143, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 53 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 92 with best_epoch = 72 and best_val_auc = 0.68044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:39:01,400] Trial 13 finished with value: 0.6804396182503332 and parameters: {'n_d': 63, 'n_a': 53, 'n_steps': 4, 'gamma': 1.3409903630528681, 'lambda_sparse': 5.938892311565925e-05, 'lr': 0.004234061731246198, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 49 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 56 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 50 and best_val_auc = 0.68439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:43:39,675] Trial 14 finished with value: 0.6843880626032856 and parameters: {'n_d': 56, 'n_a': 49, 'n_steps': 4, 'gamma': 1.5208466965455254, 'lambda_sparse': 6.842033003026489e-05, 'lr': 0.004703988029622159, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 46 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 53 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 99 with best_epoch = 79 and best_val_auc = 0.66439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:49:43,493] Trial 15 finished with value: 0.6643948101258584 and parameters: {'n_d': 53, 'n_a': 46, 'n_steps': 4, 'gamma': 1.5981776191660957, 'lambda_sparse': 5.3957169312943066e-05, 'lr': 0.0015102890281563108, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 33 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 55 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 84 and best_val_auc = 0.66546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:55:56,147] Trial 16 finished with value: 0.6654572590190931 and parameters: {'n_d': 55, 'n_a': 33, 'n_steps': 3, 'gamma': 1.685819329497655, 'lambda_sparse': 9.072173261477716e-06, 'lr': 0.0018070083746098832, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 49 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 58 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_auc = 0.67996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 09:59:35,953] Trial 17 finished with value: 0.6799621452547189 and parameters: {'n_d': 58, 'n_a': 49, 'n_steps': 4, 'gamma': 1.4659709207561753, 'lambda_sparse': 0.00012575794514134124, 'lr': 0.004885742676160388, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 61 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 48 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 86 with best_epoch = 66 and best_val_auc = 0.64664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:04:56,044] Trial 18 finished with value: 0.6466396765763757 and parameters: {'n_d': 48, 'n_a': 61, 'n_steps': 3, 'gamma': 1.8107185778360457, 'lambda_sparse': 3.967992808019166e-05, 'lr': 0.0015195303559948493, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 28 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 39 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.53359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:06:24,273] Trial 19 finished with value: 0.5335926554932268 and parameters: {'n_d': 39, 'n_a': 28, 'n_steps': 4, 'gamma': 1.2087142256694299, 'lambda_sparse': 0.0002956774320181486, 'lr': 0.0001186915656307489, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 59 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 58 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_auc = 0.63692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:09:50,440] Trial 20 finished with value: 0.636923815895562 and parameters: {'n_d': 58, 'n_a': 59, 'n_steps': 5, 'gamma': 2.177902198798227, 'lambda_sparse': 8.868822776627862e-05, 'lr': 0.0028938675016151934, 'mask_type': 'entmax'}. Best is trial 12 with value: 0.6858347771887991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 51 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_val_auc = 0.68743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:16:50,917] Trial 21 finished with value: 0.6874301660004918 and parameters: {'n_d': 64, 'n_a': 51, 'n_steps': 4, 'gamma': 1.349774058053704, 'lambda_sparse': 5.78971363203213e-05, 'lr': 0.004680550316052563, 'mask_type': 'entmax'}. Best is trial 21 with value: 0.6874301660004918.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 50 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 59 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_auc = 0.66887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:20:31,338] Trial 22 finished with value: 0.668865329742279 and parameters: {'n_d': 59, 'n_a': 50, 'n_steps': 4, 'gamma': 1.455573761961228, 'lambda_sparse': 3.16797229998993e-05, 'lr': 0.003819791229955228, 'mask_type': 'entmax'}. Best is trial 21 with value: 0.6874301660004918.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 42 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_auc = 0.67614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:25:01,002] Trial 23 finished with value: 0.6761412176419124 and parameters: {'n_d': 64, 'n_a': 42, 'n_steps': 3, 'gamma': 1.1877100346536993, 'lambda_sparse': 9.138596248650149e-06, 'lr': 0.002080837512443387, 'mask_type': 'entmax'}. Best is trial 21 with value: 0.6874301660004918.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 48 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 50 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 115 with best_epoch = 95 and best_val_auc = 0.64048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:32:27,606] Trial 24 finished with value: 0.6404794171970334 and parameters: {'n_d': 50, 'n_a': 48, 'n_steps': 4, 'gamma': 1.4326690076143853, 'lambda_sparse': 6.972635010781019e-05, 'lr': 0.001038862222180127, 'mask_type': 'entmax'}. Best is trial 21 with value: 0.6874301660004918.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 59 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 92 with best_epoch = 72 and best_val_auc = 0.69138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:38:15,635] Trial 25 finished with value: 0.6913814694731787 and parameters: {'n_d': 59, 'n_a': 57, 'n_steps': 5, 'gamma': 1.7111039914098805, 'lambda_sparse': 0.00024137154945623953, 'lr': 0.0035177974693979767, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 142 with best_epoch = 122 and best_val_auc = 0.67141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:46:17,173] Trial 26 finished with value: 0.6714099463057314 and parameters: {'n_d': 60, 'n_a': 57, 'n_steps': 5, 'gamma': 1.7105285581605927, 'lambda_sparse': 0.0002907909568832351, 'lr': 0.003413363747905262, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 59 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 50 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 92 with best_epoch = 72 and best_val_auc = 0.6724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:51:22,387] Trial 27 finished with value: 0.6724023467654779 and parameters: {'n_d': 50, 'n_a': 59, 'n_steps': 5, 'gamma': 1.2792981500891853, 'lambda_sparse': 0.0002818227634273266, 'lr': 0.002154034242579788, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 52 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 42 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 73 and best_val_auc = 0.67283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 10:56:25,323] Trial 28 finished with value: 0.6728272119579825 and parameters: {'n_d': 42, 'n_a': 52, 'n_steps': 6, 'gamma': 1.0508831413752007, 'lambda_sparse': 1.9178900342334478e-05, 'lr': 0.0012440825934780472, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 59 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 38 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 98 with best_epoch = 78 and best_val_auc = 0.66353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:01:18,428] Trial 29 finished with value: 0.6635313559661251 and parameters: {'n_d': 38, 'n_a': 59, 'n_steps': 5, 'gamma': 1.8288352009467073, 'lambda_sparse': 0.00015735381798312893, 'lr': 0.002902113645866384, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 98 with best_epoch = 78 and best_val_auc = 0.67253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:06:10,943] Trial 30 finished with value: 0.6725324367133847 and parameters: {'n_d': 60, 'n_a': 64, 'n_steps': 3, 'gamma': 1.181078575384395, 'lambda_sparse': 0.00011079086445359092, 'lr': 0.002406048691409037, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 51 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 56 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 101 with best_epoch = 81 and best_val_auc = 0.68213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:11:13,354] Trial 31 finished with value: 0.6821299298372017 and parameters: {'n_d': 56, 'n_a': 51, 'n_steps': 4, 'gamma': 1.6107769180210172, 'lambda_sparse': 5.0626960353130955e-05, 'lr': 0.004964336946369967, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 46 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 52 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_auc = 0.66198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:14:09,495] Trial 32 finished with value: 0.6619754230067646 and parameters: {'n_d': 52, 'n_a': 46, 'n_steps': 4, 'gamma': 1.5265358618301703, 'lambda_sparse': 0.00018815850128125835, 'lr': 0.0038019949761093096, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 56 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 61 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 117 and best_val_auc = 0.66743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:20:59,810] Trial 33 finished with value: 0.6674323389314898 and parameters: {'n_d': 61, 'n_a': 56, 'n_steps': 5, 'gamma': 1.6663461854448247, 'lambda_sparse': 2.7657801951503034e-05, 'lr': 0.003775496614955459, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 40 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 42 and best_val_auc = 0.68239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:24:27,494] Trial 34 finished with value: 0.6823872506132812 and parameters: {'n_d': 57, 'n_a': 40, 'n_steps': 3, 'gamma': 1.3731131881029426, 'lambda_sparse': 6.049416291597958e-06, 'lr': 0.004876919215303812, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 47 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 62 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 89 and best_val_auc = 0.64588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:30:40,016] Trial 35 finished with value: 0.6458814380228615 and parameters: {'n_d': 62, 'n_a': 47, 'n_steps': 6, 'gamma': 1.543870894595551, 'lambda_sparse': 8.485627089426298e-05, 'lr': 0.0007129912388642234, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 43 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 112 with best_epoch = 92 and best_val_auc = 0.67373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:37:30,010] Trial 36 finished with value: 0.6737341247376758 and parameters: {'n_d': 64, 'n_a': 43, 'n_steps': 4, 'gamma': 1.9359256547880368, 'lambda_sparse': 0.0005430482143903764, 'lr': 0.0018536683466806778, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 56 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 77 with best_epoch = 57 and best_val_auc = 0.6614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:42:38,362] Trial 37 finished with value: 0.6613961653486126 and parameters: {'n_d': 56, 'n_a': 57, 'n_steps': 7, 'gamma': 1.7939161873912624, 'lambda_sparse': 4.025863786186014e-05, 'lr': 0.003189135134422202, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 36 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 45 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 55 and best_val_auc = 0.66838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:46:56,169] Trial 38 finished with value: 0.6683827103311433 and parameters: {'n_d': 45, 'n_a': 36, 'n_steps': 5, 'gamma': 1.2730859440538689, 'lambda_sparse': 0.0005043973588640182, 'lr': 0.002462872221858567, 'mask_type': 'entmax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 51 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 51 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 34 and best_val_auc = 0.68842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 11:50:53,661] Trial 39 finished with value: 0.6884159904848495 and parameters: {'n_d': 51, 'n_a': 51, 'n_steps': 6, 'gamma': 1.1130046690463282, 'lambda_sparse': 0.0009154426057575289, 'lr': 0.0039021481711319027, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 19 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 51 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 178 with best_epoch = 158 and best_val_auc = 0.67844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:09:26,219] Trial 40 finished with value: 0.6784430949399298 and parameters: {'n_d': 51, 'n_a': 19, 'n_steps': 6, 'gamma': 1.1342847630975037, 'lambda_sparse': 0.0009596193753775677, 'lr': 0.0006089079510678374, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 51 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_auc = 0.6856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:12:03,794] Trial 41 finished with value: 0.6855968984269124 and parameters: {'n_d': 60, 'n_a': 51, 'n_steps': 6, 'gamma': 1.0115419332008881, 'lambda_sparse': 0.0002112181205928748, 'lr': 0.004289145238749252, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 52 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 84 with best_epoch = 64 and best_val_auc = 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:16:54,973] Trial 42 finished with value: 0.6899953682260306 and parameters: {'n_d': 60, 'n_a': 52, 'n_steps': 6, 'gamma': 1.0171040587544928, 'lambda_sparse': 0.0007603883784493343, 'lr': 0.0035891865514553313, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 55 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 61 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 42 and best_val_auc = 0.68434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:20:19,421] Trial 43 finished with value: 0.6843434603354318 and parameters: {'n_d': 61, 'n_a': 55, 'n_steps': 7, 'gamma': 1.0880602719889694, 'lambda_sparse': 0.0007756215222284108, 'lr': 0.0035910822322213322, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 53 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 36 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 63 and best_val_auc = 0.61006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:25:03,371] Trial 44 finished with value: 0.610064673288388 and parameters: {'n_d': 36, 'n_a': 53, 'n_steps': 6, 'gamma': 1.2548969437809554, 'lambda_sparse': 0.0006447989659234164, 'lr': 0.0003714376022450471, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 26 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_auc = 0.68085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:27:56,031] Trial 45 finished with value: 0.6808467569004854 and parameters: {'n_d': 26, 'n_a': 60, 'n_steps': 6, 'gamma': 1.082839715153365, 'lambda_sparse': 0.00039688336066339195, 'lr': 0.0025762401014970218, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 56 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 54 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_auc = 0.65671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:31:28,897] Trial 46 finished with value: 0.6567066371605511 and parameters: {'n_d': 54, 'n_a': 56, 'n_steps': 7, 'gamma': 2.488077498301922, 'lambda_sparse': 0.0003544298137617609, 'lr': 0.0033335817339692772, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 62 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 48 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 88 with best_epoch = 68 and best_val_auc = 0.6718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:35:52,990] Trial 47 finished with value: 0.6717964992937975 and parameters: {'n_d': 48, 'n_a': 62, 'n_steps': 6, 'gamma': 1.138506256526948, 'lambda_sparse': 0.0007002407260733054, 'lr': 0.001605447614760261, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 45 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 101 with best_epoch = 81 and best_val_auc = 0.68924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:41:06,688] Trial 48 finished with value: 0.6892365578485695 and parameters: {'n_d': 64, 'n_a': 45, 'n_steps': 5, 'gamma': 1.0025942828427767, 'lambda_sparse': 0.0004295411263328813, 'lr': 0.002272200268295512, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 45 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 58 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_auc = 0.66886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 12:44:13,476] Trial 49 finished with value: 0.6688601833267573 and parameters: {'n_d': 58, 'n_a': 45, 'n_steps': 5, 'gamma': 1.367113271007407, 'lambda_sparse': 0.0004104048356351314, 'lr': 0.004180368268552419, 'mask_type': 'sparsemax'}. Best is trial 25 with value: 0.6913814694731787.\n",
      "\n",
      "Best trial: 25\n",
      "Best validation AUROC: 0.6914\n",
      "\n",
      "Best hyperparameters:\n",
      "  n_d: 59\n",
      "  n_a: 57\n",
      "  n_steps: 5\n",
      "  gamma: 1.7111039914098805\n",
      "  lambda_sparse: 0.00024137154945623953\n",
      "  lr: 0.0035177974693979767\n",
      "  mask_type: entmax\n"
     ]
    }
   ],
   "source": [
    "print(\"hyperparameter optimization under way\")\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"tabnet_readmission_10pct\")\n",
    "\n",
    "objective = make_objective(X_train_scaled, y_train, X_val_scaled, y_val, pretrainer)\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "print(f\"\\nBest trial: {study.best_trial.number}\")\n",
    "print(f\"Best validation AUROC: {study.best_value:.4f}\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "best_params = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a22b3e",
   "metadata": {},
   "source": [
    "Retraining using the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65e5f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 59 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.13584 | val_auc: 0.52226 |  0:00:02s\n",
      "epoch 1  | loss: 0.89311 | val_auc: 0.55845 |  0:00:04s\n",
      "epoch 2  | loss: 0.82872 | val_auc: 0.5585  |  0:00:06s\n",
      "epoch 3  | loss: 0.78105 | val_auc: 0.57096 |  0:00:08s\n",
      "epoch 4  | loss: 0.76253 | val_auc: 0.59085 |  0:00:10s\n",
      "epoch 5  | loss: 0.73844 | val_auc: 0.60154 |  0:00:13s\n",
      "epoch 6  | loss: 0.72085 | val_auc: 0.60592 |  0:00:15s\n",
      "epoch 7  | loss: 0.71155 | val_auc: 0.62129 |  0:00:17s\n",
      "epoch 8  | loss: 0.70354 | val_auc: 0.61798 |  0:00:19s\n",
      "epoch 9  | loss: 0.69408 | val_auc: 0.61406 |  0:00:21s\n",
      "epoch 10 | loss: 0.69128 | val_auc: 0.61359 |  0:00:23s\n",
      "epoch 11 | loss: 0.69554 | val_auc: 0.62069 |  0:00:25s\n",
      "epoch 12 | loss: 0.69249 | val_auc: 0.62463 |  0:00:28s\n",
      "epoch 13 | loss: 0.6868  | val_auc: 0.62261 |  0:00:30s\n",
      "epoch 14 | loss: 0.67813 | val_auc: 0.63902 |  0:00:32s\n",
      "epoch 15 | loss: 0.68438 | val_auc: 0.63961 |  0:00:34s\n",
      "epoch 16 | loss: 0.67382 | val_auc: 0.63584 |  0:00:36s\n",
      "epoch 17 | loss: 0.67438 | val_auc: 0.64296 |  0:00:39s\n",
      "epoch 18 | loss: 0.67413 | val_auc: 0.64527 |  0:00:41s\n",
      "epoch 19 | loss: 0.67034 | val_auc: 0.6523  |  0:00:44s\n",
      "epoch 20 | loss: 0.66594 | val_auc: 0.65159 |  0:00:46s\n",
      "epoch 21 | loss: 0.66919 | val_auc: 0.6446  |  0:00:49s\n",
      "epoch 22 | loss: 0.66419 | val_auc: 0.63682 |  0:00:52s\n",
      "epoch 23 | loss: 0.66368 | val_auc: 0.6478  |  0:00:56s\n",
      "epoch 24 | loss: 0.6611  | val_auc: 0.63948 |  0:00:59s\n",
      "epoch 25 | loss: 0.65594 | val_auc: 0.64774 |  0:01:02s\n",
      "epoch 26 | loss: 0.65398 | val_auc: 0.63571 |  0:01:04s\n",
      "epoch 27 | loss: 0.65243 | val_auc: 0.65422 |  0:01:07s\n",
      "epoch 28 | loss: 0.64645 | val_auc: 0.66842 |  0:01:10s\n",
      "epoch 29 | loss: 0.65085 | val_auc: 0.65124 |  0:01:13s\n",
      "epoch 30 | loss: 0.65346 | val_auc: 0.65995 |  0:01:16s\n",
      "epoch 31 | loss: 0.64828 | val_auc: 0.65582 |  0:01:18s\n",
      "epoch 32 | loss: 0.64636 | val_auc: 0.65579 |  0:01:21s\n",
      "epoch 33 | loss: 0.64202 | val_auc: 0.65803 |  0:01:23s\n",
      "epoch 34 | loss: 0.64311 | val_auc: 0.64931 |  0:01:26s\n",
      "epoch 35 | loss: 0.6455  | val_auc: 0.6581  |  0:01:28s\n",
      "epoch 36 | loss: 0.64162 | val_auc: 0.66075 |  0:01:31s\n",
      "epoch 37 | loss: 0.64292 | val_auc: 0.65041 |  0:01:33s\n",
      "epoch 38 | loss: 0.64259 | val_auc: 0.64441 |  0:01:35s\n",
      "epoch 39 | loss: 0.6351  | val_auc: 0.65119 |  0:01:38s\n",
      "epoch 40 | loss: 0.63662 | val_auc: 0.64577 |  0:01:40s\n",
      "epoch 41 | loss: 0.64033 | val_auc: 0.65025 |  0:01:43s\n",
      "epoch 42 | loss: 0.64019 | val_auc: 0.63976 |  0:01:45s\n",
      "epoch 43 | loss: 0.63593 | val_auc: 0.63735 |  0:01:48s\n",
      "epoch 44 | loss: 0.63701 | val_auc: 0.64315 |  0:01:50s\n",
      "epoch 45 | loss: 0.63479 | val_auc: 0.63799 |  0:01:53s\n",
      "epoch 46 | loss: 0.63144 | val_auc: 0.6346  |  0:01:55s\n",
      "epoch 47 | loss: 0.62708 | val_auc: 0.64389 |  0:01:58s\n",
      "epoch 48 | loss: 0.6294  | val_auc: 0.64909 |  0:02:00s\n",
      "epoch 49 | loss: 0.63006 | val_auc: 0.6485  |  0:02:03s\n",
      "epoch 50 | loss: 0.63035 | val_auc: 0.64631 |  0:02:05s\n",
      "epoch 51 | loss: 0.62639 | val_auc: 0.64296 |  0:02:07s\n",
      "epoch 52 | loss: 0.62974 | val_auc: 0.6519  |  0:02:10s\n",
      "epoch 53 | loss: 0.62623 | val_auc: 0.65385 |  0:02:12s\n",
      "epoch 54 | loss: 0.63112 | val_auc: 0.65176 |  0:02:15s\n",
      "epoch 55 | loss: 0.62725 | val_auc: 0.65675 |  0:02:17s\n",
      "epoch 56 | loss: 0.62349 | val_auc: 0.66172 |  0:02:20s\n",
      "epoch 57 | loss: 0.62348 | val_auc: 0.65666 |  0:02:22s\n",
      "epoch 58 | loss: 0.6266  | val_auc: 0.65931 |  0:02:25s\n",
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 28 and best_val_auc = 0.66842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_readmission_final.zip\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "final_model = TabNetClassifier(n_d=best_params[\"n_d\"], n_a=best_params[\"n_a\"], n_steps=best_params[\"n_steps\"], gamma=best_params[\"gamma\"], lambda_sparse=best_params[\"lambda_sparse\"], optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=best_params[\"lr\"]), mask_type=best_params.get(\"mask_type\", \"entmax\"), device_name=DEVICE, verbose=1)\n",
    "\n",
    "final_model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], eval_name=[\"val\"], eval_metric=[\"auc\"], max_epochs=MAX_FINETUNE_EPOCHS, patience=EARLY_STOPPING_PATIENCE, batch_size=BATCH_SIZE, virtual_batch_size=VIRTUAL_BATCH_SIZE, num_workers=0, drop_last=False, from_unsupervised=pretrainer, weights=1)\n",
    "final_model.save_model(\"tabnet_readmission_final\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0af95f",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cae2d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUROC: 0.6507\n",
      "95% CI: [0.6315, 0.6684]\n",
      "Classification Report (threshold=0.5)\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No Readmission       0.93      0.62      0.75      8164\n",
      "   Readmission       0.17      0.62      0.26       983\n",
      "\n",
      "      accuracy                           0.62      9147\n",
      "     macro avg       0.55      0.62      0.50      9147\n",
      "  weighted avg       0.85      0.62      0.69      9147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "y_test_pred_default = (y_test_proba >= 0.5).astype(int)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "auc_mean, auc_lower, auc_upper = bootstrap_auc_ci(y_test, y_test_proba, n_bootstraps=2000)\n",
    "\n",
    "print(f\"\\nAUROC: {auc_score:.4f}\")\n",
    "print(f\"95% CI: [{auc_lower:.4f}, {auc_upper:.4f}]\")\n",
    "\n",
    "# Classification report (default threshold)\n",
    "print(\"Classification Report (threshold=0.5)\")\n",
    "print(classification_report(y_test, y_test_pred_default, target_names=['No Readmission', 'Readmission']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66a1f5",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7635c5a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m feature_importance = final_model.feature_importances_\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create dataframe\u001b[39;00m\n\u001b[32m      5\u001b[39m importance_df = pd.DataFrame({\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mfeature_names\u001b[49m,\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m: feature_importance\n\u001b[32m      8\u001b[39m }).sort_values(\u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTop 20 most important features:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(importance_df.head(\u001b[32m20\u001b[39m).to_string(index=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[31mNameError\u001b[39m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "# Get feature importance from TabNet's attention mechanism\n",
    "feature_importance = final_model.feature_importances_\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9a72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

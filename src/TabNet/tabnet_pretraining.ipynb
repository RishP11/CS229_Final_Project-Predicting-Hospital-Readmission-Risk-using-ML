{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3de49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327ca4ed",
   "metadata": {},
   "source": [
    "Some useful global constants and setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac54c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "CORR_THRESHOLD = 0.90 \n",
    "TEST_SIZE = 0.30 # % of the total dataset \n",
    "VAL_SIZE = 0.30 # % of the training dataset    \n",
    "N_TRIALS = 50                     \n",
    "MAX_PRETRAIN_EPOCHS = 150\n",
    "MAX_FINETUNE_EPOCHS = 200\n",
    "EARLY_STOPPING_PATIENCE = 30\n",
    "BATCH_SIZE = 2048\n",
    "VIRTUAL_BATCH_SIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027cc805",
   "metadata": {},
   "source": [
    "Setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8958f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee8245",
   "metadata": {},
   "source": [
    "Simple Bootstrapping method to derive the confidence bounds on the AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f425c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc_ci(y_true, y_scores, n_bootstraps=2000, ci=0.95):\n",
    "    \"\"\" \n",
    "    Simple Bootstrapping method to get an confidence interval on the AUROC score.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    aucs = []\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = rng.integers(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        aucs.append(roc_auc_score(y_true[idx], y_scores[idx]))\n",
    "\n",
    "    lower = np.percentile(aucs, (1 - ci) / 2 * 100)\n",
    "    upper = np.percentile(aucs, (1 + ci) / 2 * 100)\n",
    "    return np.mean(aucs), lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933896b6",
   "metadata": {},
   "source": [
    "Loading the dataset, pre-processing, and analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f71e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>anion_gap_mean</th>\n",
       "      <th>anion_gap_sd</th>\n",
       "      <th>anion_gap_min</th>\n",
       "      <th>anion_gap_max</th>\n",
       "      <th>bicarbonate_mean</th>\n",
       "      <th>bicarbonate_sd</th>\n",
       "      <th>bicarbonate_min</th>\n",
       "      <th>bicarbonate_max</th>\n",
       "      <th>calcium_total_mean</th>\n",
       "      <th>calcium_total_sd</th>\n",
       "      <th>calcium_total_min</th>\n",
       "      <th>calcium_total_max</th>\n",
       "      <th>chloride_mean</th>\n",
       "      <th>chloride_sd</th>\n",
       "      <th>chloride_min</th>\n",
       "      <th>chloride_max</th>\n",
       "      <th>creatinine_mean</th>\n",
       "      <th>creatinine_sd</th>\n",
       "      <th>creatinine_min</th>\n",
       "      <th>creatinine_max</th>\n",
       "      <th>glucose_mean</th>\n",
       "      <th>glucose_sd</th>\n",
       "      <th>glucose_min</th>\n",
       "      <th>glucose_max</th>\n",
       "      <th>hematocrit_mean</th>\n",
       "      <th>hematocrit_sd</th>\n",
       "      <th>hematocrit_min</th>\n",
       "      <th>hematocrit_max</th>\n",
       "      <th>hemoglobin_mean</th>\n",
       "      <th>hemoglobin_sd</th>\n",
       "      <th>hemoglobin_min</th>\n",
       "      <th>hemoglobin_max</th>\n",
       "      <th>mchc_mean</th>\n",
       "      <th>mchc_sd</th>\n",
       "      <th>mchc_min</th>\n",
       "      <th>mchc_max</th>\n",
       "      <th>mch_mean</th>\n",
       "      <th>mch_sd</th>\n",
       "      <th>mch_min</th>\n",
       "      <th>...</th>\n",
       "      <th>pt_mean</th>\n",
       "      <th>pt_sd</th>\n",
       "      <th>pt_min</th>\n",
       "      <th>pt_max</th>\n",
       "      <th>phosphate_mean</th>\n",
       "      <th>phosphate_sd</th>\n",
       "      <th>phosphate_min</th>\n",
       "      <th>phosphate_max</th>\n",
       "      <th>platelet_count_mean</th>\n",
       "      <th>platelet_count_sd</th>\n",
       "      <th>platelet_count_min</th>\n",
       "      <th>platelet_count_max</th>\n",
       "      <th>potassium_mean</th>\n",
       "      <th>potassium_sd</th>\n",
       "      <th>potassium_min</th>\n",
       "      <th>potassium_max</th>\n",
       "      <th>rdw_mean</th>\n",
       "      <th>rdw_sd</th>\n",
       "      <th>rdw_min</th>\n",
       "      <th>rdw_max</th>\n",
       "      <th>red_blood_cells_mean</th>\n",
       "      <th>red_blood_cells_sd</th>\n",
       "      <th>red_blood_cells_min</th>\n",
       "      <th>red_blood_cells_max</th>\n",
       "      <th>sodium_mean</th>\n",
       "      <th>sodium_sd</th>\n",
       "      <th>sodium_min</th>\n",
       "      <th>sodium_max</th>\n",
       "      <th>urea_nitrogen_mean</th>\n",
       "      <th>urea_nitrogen_sd</th>\n",
       "      <th>urea_nitrogen_min</th>\n",
       "      <th>urea_nitrogen_max</th>\n",
       "      <th>white_blood_cells_mean</th>\n",
       "      <th>white_blood_cells_sd</th>\n",
       "      <th>white_blood_cells_min</th>\n",
       "      <th>white_blood_cells_max</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>icu_los_hours</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200003</td>\n",
       "      <td>13.375000</td>\n",
       "      <td>3.583195</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>3.105295</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.771429</td>\n",
       "      <td>0.292770</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.3</td>\n",
       "      <td>108.125000</td>\n",
       "      <td>2.356602</td>\n",
       "      <td>105.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.113389</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>26.596187</td>\n",
       "      <td>81.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>31.077778</td>\n",
       "      <td>1.943436</td>\n",
       "      <td>28.5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10.283333</td>\n",
       "      <td>0.421505</td>\n",
       "      <td>9.6</td>\n",
       "      <td>10.8</td>\n",
       "      <td>33.483333</td>\n",
       "      <td>0.711102</td>\n",
       "      <td>32.8</td>\n",
       "      <td>34.8</td>\n",
       "      <td>30.233333</td>\n",
       "      <td>0.524087</td>\n",
       "      <td>29.6</td>\n",
       "      <td>...</td>\n",
       "      <td>14.540000</td>\n",
       "      <td>2.440901</td>\n",
       "      <td>12.7</td>\n",
       "      <td>18.8</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>0.820170</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>118.857143</td>\n",
       "      <td>6.568322</td>\n",
       "      <td>109.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>3.587500</td>\n",
       "      <td>0.356320</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>14.583333</td>\n",
       "      <td>0.278687</td>\n",
       "      <td>14.1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>3.403333</td>\n",
       "      <td>0.141657</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.57</td>\n",
       "      <td>143.125000</td>\n",
       "      <td>1.246423</td>\n",
       "      <td>141.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>15.571429</td>\n",
       "      <td>4.577377</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.471429</td>\n",
       "      <td>13.176711</td>\n",
       "      <td>13.2</td>\n",
       "      <td>43.9</td>\n",
       "      <td>48</td>\n",
       "      <td>M</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200007</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.9</td>\n",
       "      <td>8.9</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>101.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>11.313709</td>\n",
       "      <td>217.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>37.750000</td>\n",
       "      <td>0.494975</td>\n",
       "      <td>37.4</td>\n",
       "      <td>38.1</td>\n",
       "      <td>13.050000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>12.8</td>\n",
       "      <td>13.3</td>\n",
       "      <td>34.600000</td>\n",
       "      <td>0.424264</td>\n",
       "      <td>34.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>0.424264</td>\n",
       "      <td>26.1</td>\n",
       "      <td>...</td>\n",
       "      <td>13.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.7</td>\n",
       "      <td>13.7</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>15.556349</td>\n",
       "      <td>225.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>13.1</td>\n",
       "      <td>13.3</td>\n",
       "      <td>4.945000</td>\n",
       "      <td>0.049497</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.98</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>135.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>1.272792</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200009</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>113.333333</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>112.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>24.748737</td>\n",
       "      <td>91.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>29.366667</td>\n",
       "      <td>1.888121</td>\n",
       "      <td>26.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>10.057143</td>\n",
       "      <td>0.704408</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>34.371429</td>\n",
       "      <td>0.309377</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.7</td>\n",
       "      <td>32.257143</td>\n",
       "      <td>0.723089</td>\n",
       "      <td>31.7</td>\n",
       "      <td>...</td>\n",
       "      <td>14.480000</td>\n",
       "      <td>1.269646</td>\n",
       "      <td>12.9</td>\n",
       "      <td>16.2</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>139.428571</td>\n",
       "      <td>59.642985</td>\n",
       "      <td>75.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.294392</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>15.214286</td>\n",
       "      <td>0.445079</td>\n",
       "      <td>14.3</td>\n",
       "      <td>15.6</td>\n",
       "      <td>3.117143</td>\n",
       "      <td>0.194398</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.32</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>141.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>3.214550</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.471429</td>\n",
       "      <td>1.471637</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.3</td>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.4</td>\n",
       "      <td>10.4</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.7</td>\n",
       "      <td>12.7</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200014</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>0.057735</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.8</td>\n",
       "      <td>111.333333</td>\n",
       "      <td>3.055050</td>\n",
       "      <td>108.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.057735</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>7.810250</td>\n",
       "      <td>101.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>33.050000</td>\n",
       "      <td>2.661453</td>\n",
       "      <td>29.8</td>\n",
       "      <td>36.3</td>\n",
       "      <td>11.033333</td>\n",
       "      <td>0.702377</td>\n",
       "      <td>10.3</td>\n",
       "      <td>11.7</td>\n",
       "      <td>33.433333</td>\n",
       "      <td>1.150362</td>\n",
       "      <td>32.3</td>\n",
       "      <td>34.6</td>\n",
       "      <td>30.033333</td>\n",
       "      <td>0.945163</td>\n",
       "      <td>29.3</td>\n",
       "      <td>...</td>\n",
       "      <td>13.066667</td>\n",
       "      <td>0.115470</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>8.544004</td>\n",
       "      <td>113.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>13.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>0.347707</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.01</td>\n",
       "      <td>141.333333</td>\n",
       "      <td>3.055050</td>\n",
       "      <td>138.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>2.203028</td>\n",
       "      <td>10.7</td>\n",
       "      <td>14.7</td>\n",
       "      <td>85</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30484</th>\n",
       "      <td>299992</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>2.856153</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.125000</td>\n",
       "      <td>2.609556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.307143</td>\n",
       "      <td>0.255597</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.7</td>\n",
       "      <td>107.730769</td>\n",
       "      <td>3.539013</td>\n",
       "      <td>102.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.098205</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>126.864865</td>\n",
       "      <td>26.268023</td>\n",
       "      <td>62.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>28.628000</td>\n",
       "      <td>2.299551</td>\n",
       "      <td>24.8</td>\n",
       "      <td>32.8</td>\n",
       "      <td>9.930435</td>\n",
       "      <td>0.803082</td>\n",
       "      <td>8.5</td>\n",
       "      <td>11.3</td>\n",
       "      <td>34.686957</td>\n",
       "      <td>0.779455</td>\n",
       "      <td>33.3</td>\n",
       "      <td>36.1</td>\n",
       "      <td>29.660870</td>\n",
       "      <td>0.575041</td>\n",
       "      <td>28.5</td>\n",
       "      <td>...</td>\n",
       "      <td>12.841667</td>\n",
       "      <td>0.553433</td>\n",
       "      <td>12.3</td>\n",
       "      <td>13.9</td>\n",
       "      <td>3.419048</td>\n",
       "      <td>0.910834</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>475.869565</td>\n",
       "      <td>272.759852</td>\n",
       "      <td>102.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>3.981250</td>\n",
       "      <td>0.314647</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.808696</td>\n",
       "      <td>0.432659</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>3.352609</td>\n",
       "      <td>0.290223</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.84</td>\n",
       "      <td>141.976744</td>\n",
       "      <td>3.180958</td>\n",
       "      <td>137.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.662524</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.134783</td>\n",
       "      <td>3.781727</td>\n",
       "      <td>8.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>299993</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>1.341641</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>2.073644</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.216025</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>99.800000</td>\n",
       "      <td>3.492850</td>\n",
       "      <td>98.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.044721</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>119.400000</td>\n",
       "      <td>23.776038</td>\n",
       "      <td>97.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>28.680000</td>\n",
       "      <td>1.042593</td>\n",
       "      <td>27.3</td>\n",
       "      <td>30.2</td>\n",
       "      <td>10.175000</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>9.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>36.025000</td>\n",
       "      <td>0.928709</td>\n",
       "      <td>35.2</td>\n",
       "      <td>37.1</td>\n",
       "      <td>32.875000</td>\n",
       "      <td>0.531507</td>\n",
       "      <td>32.3</td>\n",
       "      <td>...</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>0.565685</td>\n",
       "      <td>11.3</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1.920000</td>\n",
       "      <td>0.486826</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>49.598387</td>\n",
       "      <td>268.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>3.660000</td>\n",
       "      <td>0.409878</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.2</td>\n",
       "      <td>13.575000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3.095000</td>\n",
       "      <td>0.084261</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.15</td>\n",
       "      <td>135.400000</td>\n",
       "      <td>1.516575</td>\n",
       "      <td>133.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.605530</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>299994</td>\n",
       "      <td>16.157895</td>\n",
       "      <td>2.477973</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.631579</td>\n",
       "      <td>3.451417</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>7.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>109.315789</td>\n",
       "      <td>4.203869</td>\n",
       "      <td>101.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.336842</td>\n",
       "      <td>0.791069</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>146.526316</td>\n",
       "      <td>80.865024</td>\n",
       "      <td>63.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>29.848276</td>\n",
       "      <td>2.714988</td>\n",
       "      <td>24.9</td>\n",
       "      <td>38.1</td>\n",
       "      <td>10.923810</td>\n",
       "      <td>1.191178</td>\n",
       "      <td>9.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.114286</td>\n",
       "      <td>1.165026</td>\n",
       "      <td>33.4</td>\n",
       "      <td>37.7</td>\n",
       "      <td>30.647619</td>\n",
       "      <td>0.499619</td>\n",
       "      <td>29.9</td>\n",
       "      <td>...</td>\n",
       "      <td>15.328571</td>\n",
       "      <td>2.356086</td>\n",
       "      <td>13.1</td>\n",
       "      <td>24.1</td>\n",
       "      <td>5.914286</td>\n",
       "      <td>0.868117</td>\n",
       "      <td>4.6</td>\n",
       "      <td>7.9</td>\n",
       "      <td>156.636364</td>\n",
       "      <td>46.494798</td>\n",
       "      <td>109.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>4.771429</td>\n",
       "      <td>0.882124</td>\n",
       "      <td>3.4</td>\n",
       "      <td>6.7</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>0.626897</td>\n",
       "      <td>13.8</td>\n",
       "      <td>16.4</td>\n",
       "      <td>3.566667</td>\n",
       "      <td>0.405553</td>\n",
       "      <td>2.99</td>\n",
       "      <td>4.68</td>\n",
       "      <td>142.315789</td>\n",
       "      <td>4.397634</td>\n",
       "      <td>130.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>44.578947</td>\n",
       "      <td>12.102873</td>\n",
       "      <td>28.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>10.076190</td>\n",
       "      <td>2.642329</td>\n",
       "      <td>5.3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>74</td>\n",
       "      <td>F</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>299998</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>0.416333</td>\n",
       "      <td>8.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>107.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.057735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>32.269697</td>\n",
       "      <td>130.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>29.480000</td>\n",
       "      <td>2.426314</td>\n",
       "      <td>26.9</td>\n",
       "      <td>32.5</td>\n",
       "      <td>9.680000</td>\n",
       "      <td>0.690652</td>\n",
       "      <td>8.9</td>\n",
       "      <td>10.4</td>\n",
       "      <td>32.820000</td>\n",
       "      <td>0.544977</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>29.520000</td>\n",
       "      <td>0.389872</td>\n",
       "      <td>28.9</td>\n",
       "      <td>...</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>12.5</td>\n",
       "      <td>12.9</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.513160</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>190.800000</td>\n",
       "      <td>10.848963</td>\n",
       "      <td>173.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>0.208167</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>14.980000</td>\n",
       "      <td>0.164317</td>\n",
       "      <td>14.7</td>\n",
       "      <td>15.1</td>\n",
       "      <td>3.278000</td>\n",
       "      <td>0.268179</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.60</td>\n",
       "      <td>139.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>137.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>20.750000</td>\n",
       "      <td>0.957427</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1.210372</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>87</td>\n",
       "      <td>M</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>299999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.400000</td>\n",
       "      <td>1.852026</td>\n",
       "      <td>33.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.9</td>\n",
       "      <td>35.150000</td>\n",
       "      <td>0.212132</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>29.650000</td>\n",
       "      <td>0.212132</td>\n",
       "      <td>29.5</td>\n",
       "      <td>...</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>54.369109</td>\n",
       "      <td>184.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.950000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>12.9</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>0.113137</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.03</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>3.394113</td>\n",
       "      <td>15.9</td>\n",
       "      <td>20.7</td>\n",
       "      <td>49</td>\n",
       "      <td>M</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30489 rows Ã— 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       icustay_id  anion_gap_mean  anion_gap_sd  ...  gender  icu_los_hours  target\n",
       "0          200003       13.375000      3.583195  ...       M            141       0\n",
       "1          200007       15.500000      2.121320  ...       M             30       0\n",
       "2          200009        9.500000      2.121320  ...       F             51       0\n",
       "3          200012             NaN           NaN  ...       F             10       0\n",
       "4          200014       10.000000      1.732051  ...       M             41       0\n",
       "...           ...             ...           ...  ...     ...            ...     ...\n",
       "30484      299992       15.375000      2.856153  ...       M            499       0\n",
       "30485      299993        9.400000      1.341641  ...       M             67       0\n",
       "30486      299994       16.157895      2.477973  ...       F            152       1\n",
       "30487      299998       11.500000      1.732051  ...       M             46       1\n",
       "30488      299999             NaN           NaN  ...       M             31       0\n",
       "\n",
       "[30489 rows x 93 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort_data = pd.read_csv('../cohort_data_new.csv')\n",
    "cohort_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cff3a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_cols = [\n",
    "    'anion_gap_mean', 'anion_gap_min', 'anion_gap_max', 'anion_gap_sd',\n",
    "    'bicarbonate_mean', 'bicarbonate_min', 'bicarbonate_max', 'bicarbonate_sd',\n",
    "    'calcium_total_mean', 'calcium_total_min', 'calcium_total_max', 'calcium_total_sd',\n",
    "    'chloride_mean', 'chloride_min', 'chloride_max', 'chloride_sd',\n",
    "    'creatinine_mean', 'creatinine_min', 'creatinine_max', 'creatinine_sd',\n",
    "    'glucose_mean', 'glucose_min', 'glucose_max', 'glucose_sd',\n",
    "    'hematocrit_mean', 'hematocrit_min', 'hematocrit_max', 'hematocrit_sd',\n",
    "    'hemoglobin_mean', 'hemoglobin_min', 'hemoglobin_max', 'hemoglobin_sd',\n",
    "    'mchc_mean', 'mchc_min', 'mchc_max', 'mchc_sd',\n",
    "    'mcv_mean', 'mcv_min', 'mcv_max', 'mcv_sd',\n",
    "    'magnesium_mean', 'magnesium_min', 'magnesium_max', 'magnesium_sd',\n",
    "    'pt_mean', 'pt_min', 'pt_max', 'pt_sd',\n",
    "    'phosphate_mean', 'phosphate_min', 'phosphate_max', 'phosphate_sd',\n",
    "    'platelet_count_mean', 'platelet_count_min', 'platelet_count_max', 'platelet_count_sd',\n",
    "    'potassium_mean', 'potassium_min', 'potassium_max', 'potassium_sd',\n",
    "    'rdw_mean', 'rdw_min', 'rdw_max', 'rdw_sd',\n",
    "    'red_blood_cells_mean', 'red_blood_cells_min', 'red_blood_cells_max', 'red_blood_cells_sd',\n",
    "    'sodium_mean', 'sodium_min', 'sodium_max', 'sodium_sd',\n",
    "    'urea_nitrogen_mean', 'urea_nitrogen_min', 'urea_nitrogen_max', 'urea_nitrogen_sd',\n",
    "    'white_blood_cells_mean', 'white_blood_cells_min', 'white_blood_cells_max', 'white_blood_cells_sd',\n",
    "    'age', 'icu_los_hours'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66668fd",
   "metadata": {},
   "source": [
    "REmove the ICUstay_id and the gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1464d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [c for c in cohort_data.columns if 'icustay_id' in c.lower() or 'gender' in c.lower()]\n",
    "df = cohort_data.drop(columns=['icustay_id', 'gender'], errors='ignore')\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473f929",
   "metadata": {},
   "source": [
    "Trying out some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d754a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.select_dtypes(include=['number']).replace([np.inf, -np.inf], np.nan)\n",
    "# print(f\"initial feature matrix shape: {X.shape}\")\n",
    "\n",
    "# def create_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df_eng = df.copy()\n",
    "    \n",
    "#     # BUN/Creatinine ratio (kidney function indicator)\n",
    "#     if 'urea_nitrogen_mean' in df_eng.columns and 'creatinine_mean' in df_eng.columns:\n",
    "#         df_eng['bun_creatinine_ratio'] = (\n",
    "#             df_eng['urea_nitrogen_mean'] / (df_eng['creatinine_mean'] + 1e-6)\n",
    "#         )\n",
    "\n",
    "#     # Variability indices (physiological instability)\n",
    "#     variability_features = []\n",
    "#     for base_name in ['glucose', 'potassium', 'sodium', 'hemoglobin']:\n",
    "#         mean_col = f'{base_name}_mean'\n",
    "#         sd_col = f'{base_name}_sd'\n",
    "#         if mean_col in df_eng.columns and sd_col in df_eng.columns:\n",
    "#             cv_col = f'{base_name}_cv'\n",
    "#             df_eng[cv_col] = df_eng[sd_col] / (df_eng[mean_col] + 1e-6)\n",
    "#             variability_features.append(cv_col)\n",
    "    \n",
    "#     # Range features (max - min)\n",
    "#     for base_name in ['glucose', 'creatinine', 'potassium']:\n",
    "#         max_col = f'{base_name}_max'\n",
    "#         min_col = f'{base_name}_min'\n",
    "#         if max_col in df_eng.columns and min_col in df_eng.columns:\n",
    "#             range_col = f'{base_name}_range'\n",
    "#             df_eng[range_col] = df_eng[max_col] - df_eng[min_col]\n",
    "    \n",
    "#     return df_eng\n",
    "\n",
    "# X_engineered = create_engineered_features(X)\n",
    "# print(f\"final feature matrix shape: {X_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd98d7b",
   "metadata": {},
   "source": [
    "Dimensionality reduction by deleting the columns with high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1931de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = X_engineered.corr().abs()\n",
    "# upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "# to_drop = [col for col in upper.columns if any(upper[col] >= CORR_THRESHOLD)]\n",
    "\n",
    "# print(f\"dropping {len(to_drop)} features\")\n",
    "# X_reduced = X_engineered.drop(columns=to_drop, errors='ignore')\n",
    "# print(f\"final feature count: {X_reduced.shape[1]}\")\n",
    "# feature_names = X_reduced.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8e811",
   "metadata": {},
   "source": [
    "Creating the final train-val-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d3ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 14939 samples (10.74% readmission)\n",
      "Validation set: 6403 samples (10.74% readmission)\n",
      "Test set: 9147 samples (10.75% readmission)\n"
     ]
    }
   ],
   "source": [
    "# separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y.values, test_size=TEST_SIZE, random_state=SEED, stratify=y.values)\n",
    "\n",
    "# separate validation set \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp)\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({y_train.mean()*100:.2f}% readmission)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({y_val.mean()*100:.2f}% readmission)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({y_test.mean()*100:.2f}% readmission)\")\n",
    "\n",
    "# SimpleImputation using median strat and scaling;\n",
    "# Imputation - FIT on train only and avoidning data leakage:\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scaling - FIT on train only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439a817",
   "metadata": {},
   "source": [
    "Pretraining the TABNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4acb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 12.27619| val_0_unsup_loss_numpy: 3.424139976501465|  0:00:02s\n",
      "epoch 1  | loss: 11.32026| val_0_unsup_loss_numpy: 3.2746400833129883|  0:00:05s\n",
      "epoch 2  | loss: 10.2118 | val_0_unsup_loss_numpy: 3.4841198921203613|  0:00:08s\n",
      "epoch 3  | loss: 9.31887 | val_0_unsup_loss_numpy: 3.6340599060058594|  0:00:11s\n",
      "epoch 4  | loss: 8.67822 | val_0_unsup_loss_numpy: 3.795259952545166|  0:00:13s\n",
      "epoch 5  | loss: 8.03425 | val_0_unsup_loss_numpy: 4.024010181427002|  0:00:16s\n",
      "epoch 6  | loss: 7.4706  | val_0_unsup_loss_numpy: 3.8844199180603027|  0:00:19s\n",
      "epoch 7  | loss: 6.94101 | val_0_unsup_loss_numpy: 3.1694400310516357|  0:00:21s\n",
      "epoch 8  | loss: 6.44917 | val_0_unsup_loss_numpy: 3.524480104446411|  0:00:23s\n",
      "epoch 9  | loss: 6.05784 | val_0_unsup_loss_numpy: 3.4997899532318115|  0:00:26s\n",
      "epoch 10 | loss: 5.6332  | val_0_unsup_loss_numpy: 3.381969928741455|  0:00:28s\n",
      "epoch 11 | loss: 5.18665 | val_0_unsup_loss_numpy: 3.610290050506592|  0:00:31s\n",
      "epoch 12 | loss: 4.89567 | val_0_unsup_loss_numpy: 3.4475998878479004|  0:00:33s\n",
      "epoch 13 | loss: 4.58247 | val_0_unsup_loss_numpy: 2.9044899940490723|  0:00:36s\n",
      "epoch 14 | loss: 4.22007 | val_0_unsup_loss_numpy: 3.0717999935150146|  0:00:39s\n",
      "epoch 15 | loss: 3.92954 | val_0_unsup_loss_numpy: 3.106760025024414|  0:00:42s\n",
      "epoch 16 | loss: 3.6555  | val_0_unsup_loss_numpy: 2.663909912109375|  0:00:44s\n",
      "epoch 17 | loss: 3.46973 | val_0_unsup_loss_numpy: 2.5529398918151855|  0:00:47s\n",
      "epoch 18 | loss: 3.24396 | val_0_unsup_loss_numpy: 2.6150400638580322|  0:00:50s\n",
      "epoch 19 | loss: 3.03328 | val_0_unsup_loss_numpy: 2.5416901111602783|  0:00:53s\n",
      "epoch 20 | loss: 2.84399 | val_0_unsup_loss_numpy: 2.3744499683380127|  0:00:56s\n",
      "epoch 21 | loss: 2.64087 | val_0_unsup_loss_numpy: 2.124530076980591|  0:00:58s\n",
      "epoch 22 | loss: 2.46081 | val_0_unsup_loss_numpy: 2.3369600772857666|  0:01:01s\n",
      "epoch 23 | loss: 2.37726 | val_0_unsup_loss_numpy: 1.9951200485229492|  0:01:04s\n",
      "epoch 24 | loss: 2.23212 | val_0_unsup_loss_numpy: 2.0869300365448|  0:01:07s\n",
      "epoch 25 | loss: 2.10759 | val_0_unsup_loss_numpy: 1.9345899820327759|  0:01:10s\n",
      "epoch 26 | loss: 2.01521 | val_0_unsup_loss_numpy: 2.2662899494171143|  0:01:13s\n",
      "epoch 27 | loss: 1.91932 | val_0_unsup_loss_numpy: 4.4510498046875|  0:01:15s\n",
      "epoch 28 | loss: 1.84394 | val_0_unsup_loss_numpy: 1.8088899850845337|  0:01:18s\n",
      "epoch 29 | loss: 1.77261 | val_0_unsup_loss_numpy: 1.8156299591064453|  0:01:21s\n",
      "epoch 30 | loss: 1.7137  | val_0_unsup_loss_numpy: 1.7427799701690674|  0:01:24s\n",
      "epoch 31 | loss: 1.61906 | val_0_unsup_loss_numpy: 1.559630036354065|  0:01:27s\n",
      "epoch 32 | loss: 1.52849 | val_0_unsup_loss_numpy: 1.5567899942398071|  0:01:30s\n",
      "epoch 33 | loss: 1.48551 | val_0_unsup_loss_numpy: 1.5014699697494507|  0:01:33s\n",
      "epoch 34 | loss: 1.42571 | val_0_unsup_loss_numpy: 1.5087300539016724|  0:01:36s\n",
      "epoch 35 | loss: 1.37852 | val_0_unsup_loss_numpy: 1.490280032157898|  0:01:39s\n",
      "epoch 36 | loss: 1.3467  | val_0_unsup_loss_numpy: 1.4326800107955933|  0:01:42s\n",
      "epoch 37 | loss: 1.30113 | val_0_unsup_loss_numpy: 1.2947499752044678|  0:01:44s\n",
      "epoch 38 | loss: 1.27578 | val_0_unsup_loss_numpy: 1.2727199792861938|  0:01:47s\n",
      "epoch 39 | loss: 1.23939 | val_0_unsup_loss_numpy: 1.317929983139038|  0:01:50s\n",
      "epoch 40 | loss: 1.2049  | val_0_unsup_loss_numpy: 1.2273900508880615|  0:01:53s\n",
      "epoch 41 | loss: 1.17713 | val_0_unsup_loss_numpy: 1.233739972114563|  0:01:56s\n",
      "epoch 42 | loss: 1.15804 | val_0_unsup_loss_numpy: 1.1702500581741333|  0:01:59s\n",
      "epoch 43 | loss: 1.13375 | val_0_unsup_loss_numpy: 1.1451499462127686|  0:02:02s\n",
      "epoch 44 | loss: 1.10164 | val_0_unsup_loss_numpy: 1.1267800331115723|  0:02:05s\n",
      "epoch 45 | loss: 1.09626 | val_0_unsup_loss_numpy: 1.1162400245666504|  0:02:08s\n",
      "epoch 46 | loss: 1.06927 | val_0_unsup_loss_numpy: 1.1043800115585327|  0:02:11s\n",
      "epoch 47 | loss: 1.06044 | val_0_unsup_loss_numpy: 1.1124900579452515|  0:02:13s\n",
      "epoch 48 | loss: 1.0586  | val_0_unsup_loss_numpy: 1.1083099842071533|  0:02:16s\n",
      "epoch 49 | loss: 1.03939 | val_0_unsup_loss_numpy: 1.0905699729919434|  0:02:19s\n",
      "epoch 50 | loss: 1.03777 | val_0_unsup_loss_numpy: 1.0458400249481201|  0:02:22s\n",
      "epoch 51 | loss: 1.02463 | val_0_unsup_loss_numpy: 1.0342799425125122|  0:02:26s\n",
      "epoch 52 | loss: 1.01055 | val_0_unsup_loss_numpy: 1.0145800113677979|  0:02:28s\n",
      "epoch 53 | loss: 1.00534 | val_0_unsup_loss_numpy: 1.0090199708938599|  0:02:31s\n",
      "epoch 54 | loss: 0.99292 | val_0_unsup_loss_numpy: 0.9925699830055237|  0:02:34s\n",
      "epoch 55 | loss: 0.99362 | val_0_unsup_loss_numpy: 0.9996100068092346|  0:02:37s\n",
      "epoch 56 | loss: 0.98131 | val_0_unsup_loss_numpy: 0.9801300168037415|  0:02:41s\n",
      "epoch 57 | loss: 0.97647 | val_0_unsup_loss_numpy: 0.9631199836730957|  0:02:44s\n",
      "epoch 58 | loss: 0.96363 | val_0_unsup_loss_numpy: 0.9678500294685364|  0:02:48s\n",
      "epoch 59 | loss: 0.96047 | val_0_unsup_loss_numpy: 0.9458400011062622|  0:02:52s\n",
      "epoch 60 | loss: 0.96082 | val_0_unsup_loss_numpy: 0.9559299945831299|  0:02:55s\n",
      "epoch 61 | loss: 0.94746 | val_0_unsup_loss_numpy: 0.9413099884986877|  0:02:58s\n",
      "epoch 62 | loss: 0.94999 | val_0_unsup_loss_numpy: 0.9449899792671204|  0:03:00s\n",
      "epoch 63 | loss: 0.94261 | val_0_unsup_loss_numpy: 0.9250100255012512|  0:03:03s\n",
      "epoch 64 | loss: 0.93305 | val_0_unsup_loss_numpy: 0.9200599789619446|  0:03:06s\n",
      "epoch 65 | loss: 0.93987 | val_0_unsup_loss_numpy: 0.9210900068283081|  0:03:09s\n",
      "epoch 66 | loss: 0.91527 | val_0_unsup_loss_numpy: 0.9111199975013733|  0:03:12s\n",
      "epoch 67 | loss: 0.92089 | val_0_unsup_loss_numpy: 0.9016500115394592|  0:03:15s\n",
      "epoch 68 | loss: 0.92056 | val_0_unsup_loss_numpy: 0.9067000150680542|  0:03:18s\n",
      "epoch 69 | loss: 0.92921 | val_0_unsup_loss_numpy: 0.8956900238990784|  0:03:21s\n",
      "epoch 70 | loss: 0.92052 | val_0_unsup_loss_numpy: 0.8930299878120422|  0:03:24s\n",
      "epoch 71 | loss: 0.92652 | val_0_unsup_loss_numpy: 0.8855500221252441|  0:03:27s\n",
      "epoch 72 | loss: 0.91396 | val_0_unsup_loss_numpy: 0.885890007019043|  0:03:30s\n",
      "epoch 73 | loss: 0.9106  | val_0_unsup_loss_numpy: 0.8857899904251099|  0:03:33s\n",
      "epoch 74 | loss: 0.91374 | val_0_unsup_loss_numpy: 0.8685100078582764|  0:03:36s\n",
      "epoch 75 | loss: 0.90284 | val_0_unsup_loss_numpy: 0.8626199960708618|  0:03:39s\n",
      "epoch 76 | loss: 0.89791 | val_0_unsup_loss_numpy: 0.8777400255203247|  0:03:42s\n",
      "epoch 77 | loss: 0.89201 | val_0_unsup_loss_numpy: 0.8773999810218811|  0:03:45s\n",
      "epoch 78 | loss: 0.90021 | val_0_unsup_loss_numpy: 0.8649100065231323|  0:03:48s\n",
      "epoch 79 | loss: 0.89972 | val_0_unsup_loss_numpy: 0.8808000087738037|  0:03:51s\n",
      "epoch 80 | loss: 0.88853 | val_0_unsup_loss_numpy: 0.8499100208282471|  0:03:54s\n",
      "epoch 81 | loss: 0.88467 | val_0_unsup_loss_numpy: 0.8442999720573425|  0:03:57s\n",
      "epoch 82 | loss: 0.88566 | val_0_unsup_loss_numpy: 0.8410699963569641|  0:04:00s\n",
      "epoch 83 | loss: 0.88745 | val_0_unsup_loss_numpy: 0.8384400010108948|  0:04:03s\n",
      "epoch 84 | loss: 0.88102 | val_0_unsup_loss_numpy: 0.8362299799919128|  0:04:06s\n",
      "epoch 85 | loss: 0.87901 | val_0_unsup_loss_numpy: 0.8346499800682068|  0:04:09s\n",
      "epoch 86 | loss: 0.87151 | val_0_unsup_loss_numpy: 0.8298100233078003|  0:04:12s\n",
      "epoch 87 | loss: 0.86893 | val_0_unsup_loss_numpy: 0.8283500075340271|  0:04:15s\n",
      "epoch 88 | loss: 0.88073 | val_0_unsup_loss_numpy: 0.829800009727478|  0:04:18s\n",
      "epoch 89 | loss: 0.87081 | val_0_unsup_loss_numpy: 0.8274400234222412|  0:04:20s\n",
      "epoch 90 | loss: 0.86856 | val_0_unsup_loss_numpy: 0.8214700222015381|  0:04:23s\n",
      "epoch 91 | loss: 0.85846 | val_0_unsup_loss_numpy: 0.8194599747657776|  0:04:26s\n",
      "epoch 92 | loss: 0.86921 | val_0_unsup_loss_numpy: 0.8178600072860718|  0:04:29s\n",
      "epoch 93 | loss: 0.85881 | val_0_unsup_loss_numpy: 0.813759982585907|  0:04:32s\n",
      "epoch 94 | loss: 0.86258 | val_0_unsup_loss_numpy: 0.8148300051689148|  0:04:35s\n",
      "epoch 95 | loss: 0.85725 | val_0_unsup_loss_numpy: 0.8172299861907959|  0:04:38s\n",
      "epoch 96 | loss: 0.8657  | val_0_unsup_loss_numpy: 0.8103500008583069|  0:04:41s\n",
      "epoch 97 | loss: 0.85411 | val_0_unsup_loss_numpy: 0.8078799843788147|  0:04:43s\n",
      "epoch 98 | loss: 0.8602  | val_0_unsup_loss_numpy: 0.802049994468689|  0:04:46s\n",
      "epoch 99 | loss: 0.84654 | val_0_unsup_loss_numpy: 0.7991899847984314|  0:04:49s\n",
      "epoch 100| loss: 0.85659 | val_0_unsup_loss_numpy: 0.7984899878501892|  0:04:52s\n",
      "epoch 101| loss: 0.85932 | val_0_unsup_loss_numpy: 0.7951700091362|  0:04:55s\n",
      "epoch 102| loss: 0.843   | val_0_unsup_loss_numpy: 0.7953000068664551|  0:04:58s\n",
      "epoch 103| loss: 0.84927 | val_0_unsup_loss_numpy: 0.7937800288200378|  0:05:01s\n",
      "epoch 104| loss: 0.8496  | val_0_unsup_loss_numpy: 0.7904099822044373|  0:05:04s\n",
      "epoch 105| loss: 0.8488  | val_0_unsup_loss_numpy: 0.7898899912834167|  0:05:07s\n",
      "epoch 106| loss: 0.84237 | val_0_unsup_loss_numpy: 0.7839599847793579|  0:05:09s\n",
      "epoch 107| loss: 0.84195 | val_0_unsup_loss_numpy: 0.7823699712753296|  0:05:12s\n",
      "epoch 108| loss: 0.84262 | val_0_unsup_loss_numpy: 0.780019998550415|  0:05:15s\n",
      "epoch 109| loss: 0.84021 | val_0_unsup_loss_numpy: 0.7789199948310852|  0:05:18s\n",
      "epoch 110| loss: 0.83192 | val_0_unsup_loss_numpy: 0.7757700085639954|  0:05:21s\n",
      "epoch 111| loss: 0.83074 | val_0_unsup_loss_numpy: 0.7709400057792664|  0:05:24s\n",
      "epoch 112| loss: 0.8242  | val_0_unsup_loss_numpy: 0.771049976348877|  0:05:27s\n",
      "epoch 113| loss: 0.83025 | val_0_unsup_loss_numpy: 0.7692099809646606|  0:05:30s\n",
      "epoch 114| loss: 0.83644 | val_0_unsup_loss_numpy: 0.7791200280189514|  0:05:33s\n",
      "epoch 115| loss: 0.8245  | val_0_unsup_loss_numpy: 0.7674300074577332|  0:05:35s\n",
      "epoch 116| loss: 0.82701 | val_0_unsup_loss_numpy: 0.7646399736404419|  0:05:38s\n",
      "epoch 117| loss: 0.82329 | val_0_unsup_loss_numpy: 0.7647799849510193|  0:05:41s\n",
      "epoch 118| loss: 0.82493 | val_0_unsup_loss_numpy: 0.76214998960495|  0:05:44s\n",
      "epoch 119| loss: 0.82335 | val_0_unsup_loss_numpy: 0.7602499723434448|  0:05:48s\n",
      "epoch 120| loss: 0.82095 | val_0_unsup_loss_numpy: 0.7613000273704529|  0:05:52s\n",
      "epoch 121| loss: 0.8154  | val_0_unsup_loss_numpy: 0.7593799829483032|  0:05:55s\n",
      "epoch 122| loss: 0.82046 | val_0_unsup_loss_numpy: 0.760699987411499|  0:05:58s\n",
      "epoch 123| loss: 0.81884 | val_0_unsup_loss_numpy: 0.756600022315979|  0:06:01s\n",
      "epoch 124| loss: 0.81416 | val_0_unsup_loss_numpy: 0.7507100105285645|  0:06:05s\n",
      "epoch 125| loss: 0.80835 | val_0_unsup_loss_numpy: 0.748989999294281|  0:06:08s\n",
      "epoch 126| loss: 0.81734 | val_0_unsup_loss_numpy: 0.7520899772644043|  0:06:12s\n",
      "epoch 127| loss: 0.80528 | val_0_unsup_loss_numpy: 0.7487999796867371|  0:06:16s\n",
      "epoch 128| loss: 0.79964 | val_0_unsup_loss_numpy: 0.7496899962425232|  0:06:19s\n",
      "epoch 129| loss: 0.80951 | val_0_unsup_loss_numpy: 0.7486799955368042|  0:06:23s\n",
      "epoch 130| loss: 0.80094 | val_0_unsup_loss_numpy: 0.7414000034332275|  0:06:28s\n",
      "epoch 131| loss: 0.8142  | val_0_unsup_loss_numpy: 0.7404000163078308|  0:06:33s\n",
      "epoch 132| loss: 0.80538 | val_0_unsup_loss_numpy: 0.7425000071525574|  0:06:36s\n",
      "epoch 133| loss: 0.80123 | val_0_unsup_loss_numpy: 0.7382500171661377|  0:06:40s\n",
      "epoch 134| loss: 0.79913 | val_0_unsup_loss_numpy: 0.7396799921989441|  0:06:43s\n",
      "epoch 135| loss: 0.7919  | val_0_unsup_loss_numpy: 0.7384499907493591|  0:06:46s\n",
      "epoch 136| loss: 0.79538 | val_0_unsup_loss_numpy: 0.7341899871826172|  0:06:49s\n",
      "epoch 137| loss: 0.79277 | val_0_unsup_loss_numpy: 0.7342900037765503|  0:06:53s\n",
      "epoch 138| loss: 0.7924  | val_0_unsup_loss_numpy: 0.7313399910926819|  0:06:58s\n",
      "epoch 139| loss: 0.79186 | val_0_unsup_loss_numpy: 0.729610025882721|  0:07:02s\n",
      "epoch 140| loss: 0.79485 | val_0_unsup_loss_numpy: 0.7301300168037415|  0:07:05s\n",
      "epoch 141| loss: 0.79125 | val_0_unsup_loss_numpy: 0.7310699820518494|  0:07:09s\n",
      "epoch 142| loss: 0.79181 | val_0_unsup_loss_numpy: 0.7272400259971619|  0:07:13s\n",
      "epoch 143| loss: 0.78684 | val_0_unsup_loss_numpy: 0.7422999739646912|  0:07:17s\n",
      "epoch 144| loss: 0.79611 | val_0_unsup_loss_numpy: 0.7636100053787231|  0:07:21s\n",
      "epoch 145| loss: 0.78533 | val_0_unsup_loss_numpy: 0.7712399959564209|  0:07:25s\n",
      "epoch 146| loss: 0.77614 | val_0_unsup_loss_numpy: 0.7927200198173523|  0:07:29s\n",
      "epoch 147| loss: 0.78603 | val_0_unsup_loss_numpy: 0.8175899982452393|  0:07:33s\n",
      "epoch 148| loss: 0.78498 | val_0_unsup_loss_numpy: 0.7183700203895569|  0:07:37s\n",
      "epoch 149| loss: 0.7892  | val_0_unsup_loss_numpy: 0.7839300036430359|  0:07:41s\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_0_unsup_loss_numpy = 0.7183700203895569\n",
      "Pretraining complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "def run_pretraining(X_train: np.ndarray, X_val: np.ndarray, pretrain_params: dict):\n",
    "    \"\"\"Run unsupervised pretraining\"\"\"\n",
    "    pretrainer = TabNetPretrainer(**pretrain_params)\n",
    "    pretrainer.fit(\n",
    "        X_train=X_train,\n",
    "        eval_set=[X_val],\n",
    "        max_epochs=MAX_PRETRAIN_EPOCHS,\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    print(\"Pretraining complete!\")\n",
    "    return pretrainer\n",
    "\n",
    "pretrain_params = dict(\n",
    "    n_d=32, \n",
    "    n_a=32,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-3),\n",
    "    mask_type=\"entmax\",\n",
    "    device_name=DEVICE\n",
    ")\n",
    "\n",
    "pretrainer = run_pretraining(X_train_scaled, X_val_scaled, pretrain_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b5c7a",
   "metadata": {},
   "source": [
    "Hyperparam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86a7959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(X_train, y_train, X_val, y_val, pretrainer):\n",
    "    \"\"\"Create Optuna objective function\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Hyperparameters to tune\n",
    "        n_d = trial.suggest_int(\"n_d\", 16, 64)\n",
    "        n_a = trial.suggest_int(\"n_a\", 16, 64)\n",
    "        n_steps = trial.suggest_int(\"n_steps\", 3, 7)\n",
    "        gamma = trial.suggest_float(\"gamma\", 1.0, 2.5)\n",
    "        lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        mask_type = trial.suggest_categorical(\"mask_type\", [\"sparsemax\", \"entmax\"])\n",
    "        \n",
    "        clf = TabNetClassifier(n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma, lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,optimizer_params=dict(lr=lr), mask_type=mask_type, device_name=DEVICE, verbose=0)\n",
    "        \n",
    "        try:\n",
    "            clf.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_name=[\"val\"],\n",
    "                eval_metric=[\"auc\"],\n",
    "                max_epochs=MAX_FINETUNE_EPOCHS,\n",
    "                patience=20, \n",
    "                batch_size=BATCH_SIZE,\n",
    "                virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "                num_workers=0,\n",
    "                drop_last=False,\n",
    "                from_unsupervised=pretrainer,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        pred_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, pred_proba)\n",
    "        \n",
    "        return auc\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f1154f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:05:05,027] A new study created in memory with name: tabnet_readmission_pre-training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter optimization under way\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0423156f6d4d868ad879485e427da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 34 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_val_auc = 0.64311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:10:53,364] Trial 0 finished with value: 0.6431101090561354 and parameters: {'n_d': 63, 'n_a': 34, 'n_steps': 5, 'gamma': 2.375848747346354, 'lambda_sparse': 1.0150453098190141e-06, 'lr': 0.002904960572268428, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.6431101090561354.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 51 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 47 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 197 with best_epoch = 177 and best_val_auc = 0.60229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:21:35,721] Trial 1 finished with value: 0.6022894667236363 and parameters: {'n_d': 47, 'n_a': 51, 'n_steps': 5, 'gamma': 2.130304021726256, 'lambda_sparse': 3.5053839557096555e-06, 'lr': 0.00022236312080409708, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.6431101090561354.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 28 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 181 with best_epoch = 161 and best_val_auc = 0.62782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:30:49,672] Trial 2 finished with value: 0.627819996337667 and parameters: {'n_d': 63, 'n_a': 28, 'n_steps': 3, 'gamma': 1.1100820844802823, 'lambda_sparse': 1.8325402726658204e-06, 'lr': 0.00027967714076842907, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.6431101090561354.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 55 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 45 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 89 and best_val_auc = 0.58552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:35:22,987] Trial 3 finished with value: 0.5855162363425502 and parameters: {'n_d': 45, 'n_a': 55, 'n_steps': 3, 'gamma': 2.468044856500515, 'lambda_sparse': 0.0004221530882989301, 'lr': 0.0002307724936117588, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.6431101090561354.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 53 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 38 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 86 with best_epoch = 66 and best_val_auc = 0.66519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:39:43,428] Trial 4 finished with value: 0.6651919418502921 and parameters: {'n_d': 38, 'n_a': 53, 'n_steps': 7, 'gamma': 1.7092058362024638, 'lambda_sparse': 1.0453229347334104e-05, 'lr': 0.003024072683588243, 'mask_type': 'entmax'}. Best is trial 4 with value: 0.6651919418502921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 43 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 105 and best_val_auc = 0.64895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:45:15,143] Trial 5 finished with value: 0.6489486052615516 and parameters: {'n_d': 21, 'n_a': 43, 'n_steps': 7, 'gamma': 1.6211106232833121, 'lambda_sparse': 2.0833184039093435e-05, 'lr': 0.0011588158426885967, 'mask_type': 'sparsemax'}. Best is trial 4 with value: 0.6651919418502921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 58 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 41 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 200 with best_epoch = 186 and best_val_auc = 0.62454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:54:01,117] Trial 6 finished with value: 0.6245367403202507 and parameters: {'n_d': 41, 'n_a': 58, 'n_steps': 5, 'gamma': 1.2901245591897352, 'lambda_sparse': 1.0588537657993355e-06, 'lr': 0.00024584440572235545, 'mask_type': 'entmax'}. Best is trial 4 with value: 0.6651919418502921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 29 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 58 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_auc = 0.61577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:57:31,312] Trial 7 finished with value: 0.6157688864473336 and parameters: {'n_d': 58, 'n_a': 29, 'n_steps': 6, 'gamma': 1.6694791124748665, 'lambda_sparse': 0.00035935903695026166, 'lr': 0.0013372042475800915, 'mask_type': 'entmax'}. Best is trial 4 with value: 0.6651919418502921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 34 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_val_auc = 0.48265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 12:58:39,405] Trial 8 finished with value: 0.48264982502187226 and parameters: {'n_d': 57, 'n_a': 34, 'n_steps': 7, 'gamma': 1.9507505561037557, 'lambda_sparse': 0.0004025592687581807, 'lr': 0.00010855901745571555, 'mask_type': 'entmax'}. Best is trial 4 with value: 0.6651919418502921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 39 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 72 with best_epoch = 52 and best_val_auc = 0.66298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:02:21,030] Trial 9 finished with value: 0.662976230442125 and parameters: {'n_d': 18, 'n_a': 39, 'n_steps': 7, 'gamma': 1.2601033601622027, 'lambda_sparse': 5.111425541859793e-06, 'lr': 0.002647042812830149, 'mask_type': 'entmax'}. Best is trial 4 with value: 0.6651919418502921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 30 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 90 with best_epoch = 70 and best_val_auc = 0.67249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:06:50,338] Trial 10 finished with value: 0.6724902846446521 and parameters: {'n_d': 30, 'n_a': 18, 'n_steps': 6, 'gamma': 1.5023570844547356, 'lambda_sparse': 6.99535756778796e-05, 'lr': 0.004537998964557947, 'mask_type': 'entmax'}. Best is trial 10 with value: 0.6724902846446521.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 30 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_val_auc = 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:11:35,695] Trial 11 finished with value: 0.6760023093043602 and parameters: {'n_d': 30, 'n_a': 18, 'n_steps': 6, 'gamma': 1.5130510484586674, 'lambda_sparse': 6.447455127160425e-05, 'lr': 0.004899487513462078, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 31 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 62 and best_val_auc = 0.67149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:15:48,380] Trial 12 finished with value: 0.6714942063928055 and parameters: {'n_d': 31, 'n_a': 16, 'n_steps': 6, 'gamma': 1.4343138581141348, 'lambda_sparse': 9.440425749614164e-05, 'lr': 0.004907449261008185, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 28 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 62 and best_val_auc = 0.67242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:21:44,367] Trial 13 finished with value: 0.672418818287249 and parameters: {'n_d': 28, 'n_a': 16, 'n_steps': 6, 'gamma': 1.4729438023327848, 'lambda_sparse': 7.977702901309571e-05, 'lr': 0.004678384981754271, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 23 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 29 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 60 with best_epoch = 40 and best_val_auc = 0.62234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:27:03,570] Trial 14 finished with value: 0.6223361614681886 and parameters: {'n_d': 29, 'n_a': 23, 'n_steps': 4, 'gamma': 1.9835249364440657, 'lambda_sparse': 0.00010785144944351071, 'lr': 0.0014657348817296715, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 34 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 161 with best_epoch = 141 and best_val_auc = 0.66253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:36:07,794] Trial 15 finished with value: 0.6625287391401656 and parameters: {'n_d': 34, 'n_a': 21, 'n_steps': 6, 'gamma': 1.044688768371709, 'lambda_sparse': 3.6023971795546226e-05, 'lr': 0.0006713516505281632, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 23 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 24 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 167 with best_epoch = 147 and best_val_auc = 0.62844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:45:15,022] Trial 16 finished with value: 0.6284433559177196 and parameters: {'n_d': 24, 'n_a': 23, 'n_steps': 4, 'gamma': 1.5134055722071513, 'lambda_sparse': 4.633673587291443e-05, 'lr': 0.00065743648159628, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 47 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 36 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 121 with best_epoch = 101 and best_val_auc = 0.64729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:51:17,458] Trial 17 finished with value: 0.6472875846914485 and parameters: {'n_d': 36, 'n_a': 47, 'n_steps': 5, 'gamma': 1.890161869583632, 'lambda_sparse': 0.0001708161504103177, 'lr': 0.0020783332402391245, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 27 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 25 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_auc = 0.65013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:53:53,190] Trial 18 finished with value: 0.6501285631447231 and parameters: {'n_d': 25, 'n_a': 27, 'n_steps': 6, 'gamma': 1.3765344816067695, 'lambda_sparse': 0.0008217652651285677, 'lr': 0.004918785352071182, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 20 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 49 and best_val_auc = 0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 13:57:23,908] Trial 19 finished with value: 0.6259956967588353 and parameters: {'n_d': 16, 'n_a': 20, 'n_steps': 4, 'gamma': 1.821535339931367, 'lambda_sparse': 1.6344788208578936e-05, 'lr': 0.0019402734527306357, 'mask_type': 'entmax'}. Best is trial 11 with value: 0.6760023093043602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 44 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 117 and best_val_auc = 0.68017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:04:09,828] Trial 20 finished with value: 0.6801725365724632 and parameters: {'n_d': 44, 'n_a': 63, 'n_steps': 6, 'gamma': 1.1672990420036071, 'lambda_sparse': 4.974577194504219e-05, 'lr': 0.0009105150667606697, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 62 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 46 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 167 with best_epoch = 147 and best_val_auc = 0.63698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:12:26,594] Trial 21 finished with value: 0.6369786262182343 and parameters: {'n_d': 46, 'n_a': 62, 'n_steps': 6, 'gamma': 1.156630594894149, 'lambda_sparse': 5.064393787435411e-05, 'lr': 0.0004457169194625112, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 41 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 42 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 85 and best_val_auc = 0.67137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:18:06,748] Trial 22 finished with value: 0.6713664062341044 and parameters: {'n_d': 42, 'n_a': 41, 'n_steps': 6, 'gamma': 1.5790947496040921, 'lambda_sparse': 0.00018096658175165966, 'lr': 0.003485365597865224, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 34 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 50 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 151 with best_epoch = 131 and best_val_auc = 0.6532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:25:53,730] Trial 23 finished with value: 0.6532016165130521 and parameters: {'n_d': 50, 'n_a': 34, 'n_steps': 5, 'gamma': 1.2432762653848164, 'lambda_sparse': 2.3857169165019986e-05, 'lr': 0.0009544123704343237, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 33 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 100 with best_epoch = 80 and best_val_auc = 0.66393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:31:53,471] Trial 24 finished with value: 0.6639322519278112 and parameters: {'n_d': 33, 'n_a': 63, 'n_steps': 7, 'gamma': 1.3551632227476924, 'lambda_sparse': 1.0680871877666568e-05, 'lr': 0.0020064147796507517, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 19 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 52 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 155 with best_epoch = 135 and best_val_auc = 0.65873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:39:31,258] Trial 25 finished with value: 0.6587283057641051 and parameters: {'n_d': 52, 'n_a': 19, 'n_steps': 6, 'gamma': 1.0079148300924476, 'lambda_sparse': 0.00016563362538100107, 'lr': 0.00045677643452259644, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 47 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 38 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 62 and best_val_auc = 0.67241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:43:27,091] Trial 26 finished with value: 0.6724106797696799 and parameters: {'n_d': 38, 'n_a': 47, 'n_steps': 5, 'gamma': 1.190487990869774, 'lambda_sparse': 6.376844280544343e-05, 'lr': 0.0034759824512139766, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 26 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 26 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_auc = 0.49099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:44:40,359] Trial 27 finished with value: 0.49099269568048176 and parameters: {'n_d': 26, 'n_a': 26, 'n_steps': 6, 'gamma': 1.5216285908112657, 'lambda_sparse': 3.174987125238579e-05, 'lr': 0.00013427995491277955, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 37 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 129 with best_epoch = 109 and best_val_auc = 0.65044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:50:49,870] Trial 28 finished with value: 0.6504399886060754 and parameters: {'n_d': 21, 'n_a': 37, 'n_steps': 7, 'gamma': 1.3799881527688103, 'lambda_sparse': 1.3639180287728937e-05, 'lr': 0.000949089552622901, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 31 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 81 with best_epoch = 61 and best_val_auc = 0.65283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:54:39,307] Trial 29 finished with value: 0.6528301694846284 and parameters: {'n_d': 31, 'n_a': 32, 'n_steps': 5, 'gamma': 2.2028780599495277, 'lambda_sparse': 6.711053482552086e-06, 'lr': 0.0035814708444553527, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 47 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 35 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 81 with best_epoch = 61 and best_val_auc = 0.65766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:59:07,507] Trial 30 finished with value: 0.6576565647317342 and parameters: {'n_d': 35, 'n_a': 47, 'n_steps': 6, 'gamma': 1.7286929762348697, 'lambda_sparse': 0.00011947248732540794, 'lr': 0.0024772109717869045, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 28 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 89 and best_val_auc = 0.66801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:04:58,045] Trial 31 finished with value: 0.6680110480375999 and parameters: {'n_d': 28, 'n_a': 16, 'n_steps': 6, 'gamma': 1.4847742923529383, 'lambda_sparse': 7.32229055414557e-05, 'lr': 0.0043003618075568165, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 23 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 100 with best_epoch = 80 and best_val_auc = 0.67734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:09:57,168] Trial 32 finished with value: 0.677343130073857 and parameters: {'n_d': 23, 'n_a': 18, 'n_steps': 6, 'gamma': 1.5889715546151069, 'lambda_sparse': 0.0002575173703580523, 'lr': 0.004072689905554465, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 24 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 22 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 48 and best_val_auc = 0.63726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:13:42,495] Trial 33 finished with value: 0.6372611853750839 and parameters: {'n_d': 22, 'n_a': 24, 'n_steps': 5, 'gamma': 1.5929916204486103, 'lambda_sparse': 0.000311094353913765, 'lr': 0.0016066271968668616, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 19 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 19 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 175 with best_epoch = 155 and best_val_auc = 0.67368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:22:22,473] Trial 34 finished with value: 0.6736777452237076 and parameters: {'n_d': 19, 'n_a': 19, 'n_steps': 7, 'gamma': 1.803400967454952, 'lambda_sparse': 0.00026843061941714676, 'lr': 0.002613428691608741, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 30 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 19 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 76 with best_epoch = 56 and best_val_auc = 0.64381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:26:14,694] Trial 35 finished with value: 0.6438098944027345 and parameters: {'n_d': 19, 'n_a': 30, 'n_steps': 7, 'gamma': 2.139144591940925, 'lambda_sparse': 0.00024808804253455936, 'lr': 0.0024786370923863296, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 22 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_auc = 0.63602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:28:53,523] Trial 36 finished with value: 0.6360165008443712 and parameters: {'n_d': 16, 'n_a': 22, 'n_steps': 7, 'gamma': 1.789768436690365, 'lambda_sparse': 0.0006708369943366882, 'lr': 0.003430187545686478, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 23 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_auc = 0.63564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:32:07,280] Trial 37 finished with value: 0.6356376782844005 and parameters: {'n_d': 23, 'n_a': 57, 'n_steps': 7, 'gamma': 2.005477530043302, 'lambda_sparse': 0.0006410737458331425, 'lr': 0.002956470596090286, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 25 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 43 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 138 with best_epoch = 118 and best_val_auc = 0.6276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:38:59,089] Trial 38 finished with value: 0.6276035626360659 and parameters: {'n_d': 43, 'n_a': 25, 'n_steps': 7, 'gamma': 1.8448414295105249, 'lambda_sparse': 0.00022980080474631153, 'lr': 0.000397443052923884, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 19 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 20 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 62 and best_val_auc = 0.62001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:43:04,847] Trial 39 finished with value: 0.6200060021567072 and parameters: {'n_d': 20, 'n_a': 19, 'n_steps': 3, 'gamma': 2.3064968876639256, 'lambda_sparse': 0.0004423373769920696, 'lr': 0.001080092372888657, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 49 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 154 and best_val_auc = 0.6681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:51:53,652] Trial 40 finished with value: 0.6681015890455554 and parameters: {'n_d': 49, 'n_a': 60, 'n_steps': 7, 'gamma': 1.6697018177675873, 'lambda_sparse': 0.00014399961120675325, 'lr': 0.0017055349568530027, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 26 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 85 and best_val_auc = 0.6608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 15:57:08,219] Trial 41 finished with value: 0.6607977781847036 and parameters: {'n_d': 26, 'n_a': 18, 'n_steps': 6, 'gamma': 1.6263901901818143, 'lambda_sparse': 4.6162599755344614e-05, 'lr': 0.004033858774589813, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 53 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 40 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 121 with best_epoch = 101 and best_val_auc = 0.67045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 16:03:08,336] Trial 42 finished with value: 0.6704480253921749 and parameters: {'n_d': 40, 'n_a': 53, 'n_steps': 6, 'gamma': 1.5577852314033878, 'lambda_sparse': 2.3383374981409076e-05, 'lr': 0.00394854941461318, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 31 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 118 with best_epoch = 98 and best_val_auc = 0.66701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 16:08:57,799] Trial 43 finished with value: 0.6670064497751734 and parameters: {'n_d': 31, 'n_a': 18, 'n_steps': 6, 'gamma': 1.7278004601577228, 'lambda_sparse': 5.926308428424023e-05, 'lr': 0.0026278103211863765, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 39 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_val_auc = 0.68017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 16:13:37,405] Trial 44 finished with value: 0.6801666870129606 and parameters: {'n_d': 39, 'n_a': 21, 'n_steps': 6, 'gamma': 1.2898204628432126, 'lambda_sparse': 9.975809292013352e-05, 'lr': 0.0030790532930772252, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 29 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 44 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 100 with best_epoch = 80 and best_val_auc = 0.66841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 16:18:34,841] Trial 45 finished with value: 0.668408055097764 and parameters: {'n_d': 44, 'n_a': 29, 'n_steps': 5, 'gamma': 1.0892228413929268, 'lambda_sparse': 0.00027890264844216344, 'lr': 0.003012557114454062, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 22 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 39 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 89 with best_epoch = 69 and best_val_auc = 0.66609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 16:23:01,576] Trial 46 finished with value: 0.6660912480416692 and parameters: {'n_d': 39, 'n_a': 22, 'n_steps': 7, 'gamma': 1.3104739185321694, 'lambda_sparse': 9.907937778026752e-05, 'lr': 0.0022353282832630827, 'mask_type': 'sparsemax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_auc = 0.61786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 16:26:09,622] Trial 47 finished with value: 0.6178640460640094 and parameters: {'n_d': 57, 'n_a': 21, 'n_steps': 6, 'gamma': 1.4276311867188332, 'lambda_sparse': 0.00048274210198664307, 'lr': 0.0013430785557114672, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_val_auc = 0.49341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 16:27:15,998] Trial 48 finished with value: 0.49341250584955954 and parameters: {'n_d': 18, 'n_a': 16, 'n_steps': 7, 'gamma': 1.201613622418267, 'lambda_sparse': 0.00012214995340311996, 'lr': 0.0001712769131933992, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 25 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 37 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 134 with best_epoch = 114 and best_val_auc = 0.64384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 16:34:11,682] Trial 49 finished with value: 0.643836598913508 and parameters: {'n_d': 37, 'n_a': 25, 'n_steps': 6, 'gamma': 1.282202998682981, 'lambda_sparse': 0.00019930388432188654, 'lr': 0.0007534391008868536, 'mask_type': 'entmax'}. Best is trial 20 with value: 0.6801725365724632.\n",
      "\n",
      "Best trial: 20\n",
      "Best validation AUROC: 0.6802\n",
      "\n",
      "Best hyperparameters:\n",
      "  n_d: 44\n",
      "  n_a: 63\n",
      "  n_steps: 6\n",
      "  gamma: 1.1672990420036071\n",
      "  lambda_sparse: 4.974577194504219e-05\n",
      "  lr: 0.0009105150667606697\n",
      "  mask_type: entmax\n"
     ]
    }
   ],
   "source": [
    "print(\"hyperparameter optimization under way\")\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"tabnet_readmission_pre-training\")\n",
    "\n",
    "objective = make_objective(X_train_scaled, y_train, X_val_scaled, y_val, pretrainer)\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "print(f\"\\nBest trial: {study.best_trial.number}\")\n",
    "print(f\"Best validation AUROC: {study.best_value:.4f}\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "best_params = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a22b3e",
   "metadata": {},
   "source": [
    "Retraining using the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65e5f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 44 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.57474 | val_auc: 0.4977  |  0:00:03s\n",
      "epoch 1  | loss: 1.27593 | val_auc: 0.50531 |  0:00:05s\n",
      "epoch 2  | loss: 1.14839 | val_auc: 0.52333 |  0:00:08s\n",
      "epoch 3  | loss: 1.03285 | val_auc: 0.54146 |  0:00:11s\n",
      "epoch 4  | loss: 1.00124 | val_auc: 0.56548 |  0:00:13s\n",
      "epoch 5  | loss: 0.93183 | val_auc: 0.56732 |  0:00:16s\n",
      "epoch 6  | loss: 0.90592 | val_auc: 0.57447 |  0:00:18s\n",
      "epoch 7  | loss: 0.8674  | val_auc: 0.5813  |  0:00:21s\n",
      "epoch 8  | loss: 0.85808 | val_auc: 0.57672 |  0:00:23s\n",
      "epoch 9  | loss: 0.8287  | val_auc: 0.58395 |  0:00:26s\n",
      "epoch 10 | loss: 0.806   | val_auc: 0.59414 |  0:00:29s\n",
      "epoch 11 | loss: 0.78763 | val_auc: 0.59783 |  0:00:32s\n",
      "epoch 12 | loss: 0.77595 | val_auc: 0.59934 |  0:00:35s\n",
      "epoch 13 | loss: 0.76053 | val_auc: 0.60812 |  0:00:39s\n",
      "epoch 14 | loss: 0.75562 | val_auc: 0.60585 |  0:00:42s\n",
      "epoch 15 | loss: 0.74391 | val_auc: 0.60896 |  0:00:45s\n",
      "epoch 16 | loss: 0.73027 | val_auc: 0.61333 |  0:00:48s\n",
      "epoch 17 | loss: 0.72863 | val_auc: 0.61124 |  0:00:51s\n",
      "epoch 18 | loss: 0.71678 | val_auc: 0.60381 |  0:00:53s\n",
      "epoch 19 | loss: 0.71184 | val_auc: 0.61238 |  0:00:56s\n",
      "epoch 20 | loss: 0.70874 | val_auc: 0.61217 |  0:00:59s\n",
      "epoch 21 | loss: 0.7054  | val_auc: 0.61087 |  0:01:01s\n",
      "epoch 22 | loss: 0.71538 | val_auc: 0.61712 |  0:01:03s\n",
      "epoch 23 | loss: 0.69563 | val_auc: 0.62116 |  0:01:06s\n",
      "epoch 24 | loss: 0.70333 | val_auc: 0.6173  |  0:01:09s\n",
      "epoch 25 | loss: 0.69407 | val_auc: 0.61802 |  0:01:12s\n",
      "epoch 26 | loss: 0.69322 | val_auc: 0.61988 |  0:01:17s\n",
      "epoch 27 | loss: 0.68374 | val_auc: 0.61836 |  0:01:20s\n",
      "epoch 28 | loss: 0.68162 | val_auc: 0.62618 |  0:01:24s\n",
      "epoch 29 | loss: 0.6876  | val_auc: 0.61775 |  0:01:27s\n",
      "epoch 30 | loss: 0.67894 | val_auc: 0.61906 |  0:01:30s\n",
      "epoch 31 | loss: 0.67059 | val_auc: 0.62393 |  0:01:35s\n",
      "epoch 32 | loss: 0.66753 | val_auc: 0.62963 |  0:01:39s\n",
      "epoch 33 | loss: 0.6776  | val_auc: 0.63187 |  0:01:44s\n",
      "epoch 34 | loss: 0.67302 | val_auc: 0.63698 |  0:01:48s\n",
      "epoch 35 | loss: 0.67499 | val_auc: 0.63619 |  0:01:51s\n",
      "epoch 36 | loss: 0.67083 | val_auc: 0.63218 |  0:01:55s\n",
      "epoch 37 | loss: 0.67308 | val_auc: 0.62997 |  0:01:59s\n",
      "epoch 38 | loss: 0.66013 | val_auc: 0.63705 |  0:02:03s\n",
      "epoch 39 | loss: 0.6511  | val_auc: 0.63525 |  0:02:06s\n",
      "epoch 40 | loss: 0.66445 | val_auc: 0.62811 |  0:02:10s\n",
      "epoch 41 | loss: 0.64901 | val_auc: 0.63175 |  0:02:13s\n",
      "epoch 42 | loss: 0.65955 | val_auc: 0.63095 |  0:02:16s\n",
      "epoch 43 | loss: 0.65456 | val_auc: 0.62608 |  0:02:19s\n",
      "epoch 44 | loss: 0.65061 | val_auc: 0.63049 |  0:02:22s\n",
      "epoch 45 | loss: 0.65041 | val_auc: 0.62774 |  0:02:26s\n",
      "epoch 46 | loss: 0.65744 | val_auc: 0.62901 |  0:02:31s\n",
      "epoch 47 | loss: 0.65307 | val_auc: 0.63694 |  0:02:36s\n",
      "epoch 48 | loss: 0.6494  | val_auc: 0.63208 |  0:02:41s\n",
      "epoch 49 | loss: 0.646   | val_auc: 0.63283 |  0:02:47s\n",
      "epoch 50 | loss: 0.64722 | val_auc: 0.63601 |  0:02:52s\n",
      "epoch 51 | loss: 0.63784 | val_auc: 0.63952 |  0:02:56s\n",
      "epoch 52 | loss: 0.63735 | val_auc: 0.63625 |  0:03:00s\n",
      "epoch 53 | loss: 0.63862 | val_auc: 0.63527 |  0:03:04s\n",
      "epoch 54 | loss: 0.64287 | val_auc: 0.63884 |  0:03:07s\n",
      "epoch 55 | loss: 0.64241 | val_auc: 0.64203 |  0:03:11s\n",
      "epoch 56 | loss: 0.64567 | val_auc: 0.64131 |  0:03:15s\n",
      "epoch 57 | loss: 0.63932 | val_auc: 0.65239 |  0:03:18s\n",
      "epoch 58 | loss: 0.64147 | val_auc: 0.64275 |  0:03:22s\n",
      "epoch 59 | loss: 0.64393 | val_auc: 0.6461  |  0:03:25s\n",
      "epoch 60 | loss: 0.63711 | val_auc: 0.64422 |  0:03:29s\n",
      "epoch 61 | loss: 0.62936 | val_auc: 0.64534 |  0:03:33s\n",
      "epoch 62 | loss: 0.63321 | val_auc: 0.64214 |  0:03:37s\n",
      "epoch 63 | loss: 0.63512 | val_auc: 0.64512 |  0:03:40s\n",
      "epoch 64 | loss: 0.63937 | val_auc: 0.64628 |  0:03:44s\n",
      "epoch 65 | loss: 0.63769 | val_auc: 0.64652 |  0:03:47s\n",
      "epoch 66 | loss: 0.63049 | val_auc: 0.65007 |  0:03:50s\n",
      "epoch 67 | loss: 0.62751 | val_auc: 0.64957 |  0:03:54s\n",
      "epoch 68 | loss: 0.63452 | val_auc: 0.64691 |  0:03:59s\n",
      "epoch 69 | loss: 0.63168 | val_auc: 0.65009 |  0:04:04s\n",
      "epoch 70 | loss: 0.63344 | val_auc: 0.64502 |  0:04:08s\n",
      "epoch 71 | loss: 0.62996 | val_auc: 0.64641 |  0:04:11s\n",
      "epoch 72 | loss: 0.63314 | val_auc: 0.64522 |  0:04:14s\n",
      "epoch 73 | loss: 0.62446 | val_auc: 0.65075 |  0:04:17s\n",
      "epoch 74 | loss: 0.61931 | val_auc: 0.64902 |  0:04:20s\n",
      "epoch 75 | loss: 0.62072 | val_auc: 0.65255 |  0:04:23s\n",
      "epoch 76 | loss: 0.62193 | val_auc: 0.65035 |  0:04:27s\n",
      "epoch 77 | loss: 0.61352 | val_auc: 0.65453 |  0:04:30s\n",
      "epoch 78 | loss: 0.61943 | val_auc: 0.65343 |  0:04:34s\n",
      "epoch 79 | loss: 0.61814 | val_auc: 0.65434 |  0:04:37s\n",
      "epoch 80 | loss: 0.62005 | val_auc: 0.65455 |  0:04:41s\n",
      "epoch 81 | loss: 0.61366 | val_auc: 0.65551 |  0:04:44s\n",
      "epoch 82 | loss: 0.62044 | val_auc: 0.64941 |  0:04:47s\n",
      "epoch 83 | loss: 0.61653 | val_auc: 0.65186 |  0:04:50s\n",
      "epoch 84 | loss: 0.61318 | val_auc: 0.65686 |  0:04:54s\n",
      "epoch 85 | loss: 0.6126  | val_auc: 0.65415 |  0:04:57s\n",
      "epoch 86 | loss: 0.61117 | val_auc: 0.65696 |  0:05:00s\n",
      "epoch 87 | loss: 0.61012 | val_auc: 0.6544  |  0:05:04s\n",
      "epoch 88 | loss: 0.60513 | val_auc: 0.65328 |  0:05:07s\n",
      "epoch 89 | loss: 0.60748 | val_auc: 0.65587 |  0:05:10s\n",
      "epoch 90 | loss: 0.60538 | val_auc: 0.65795 |  0:05:13s\n",
      "epoch 91 | loss: 0.6016  | val_auc: 0.6581  |  0:05:16s\n",
      "epoch 92 | loss: 0.60447 | val_auc: 0.65856 |  0:05:19s\n",
      "epoch 93 | loss: 0.59884 | val_auc: 0.64985 |  0:05:22s\n",
      "epoch 94 | loss: 0.59842 | val_auc: 0.65761 |  0:05:26s\n",
      "epoch 95 | loss: 0.60081 | val_auc: 0.65847 |  0:05:29s\n",
      "epoch 96 | loss: 0.59698 | val_auc: 0.66116 |  0:05:32s\n",
      "epoch 97 | loss: 0.60306 | val_auc: 0.66287 |  0:05:37s\n",
      "epoch 98 | loss: 0.59944 | val_auc: 0.66234 |  0:05:41s\n",
      "epoch 99 | loss: 0.59784 | val_auc: 0.65839 |  0:05:44s\n",
      "epoch 100| loss: 0.5969  | val_auc: 0.65709 |  0:05:48s\n",
      "epoch 101| loss: 0.59439 | val_auc: 0.65534 |  0:05:51s\n",
      "epoch 102| loss: 0.59598 | val_auc: 0.65533 |  0:05:55s\n",
      "epoch 103| loss: 0.59509 | val_auc: 0.65872 |  0:05:59s\n",
      "epoch 104| loss: 0.59762 | val_auc: 0.65694 |  0:06:02s\n",
      "epoch 105| loss: 0.59397 | val_auc: 0.65507 |  0:06:05s\n",
      "epoch 106| loss: 0.59433 | val_auc: 0.65411 |  0:06:09s\n",
      "epoch 107| loss: 0.59225 | val_auc: 0.65153 |  0:06:12s\n",
      "epoch 108| loss: 0.59137 | val_auc: 0.65658 |  0:06:15s\n",
      "epoch 109| loss: 0.58678 | val_auc: 0.65234 |  0:06:18s\n",
      "epoch 110| loss: 0.58812 | val_auc: 0.65419 |  0:06:21s\n",
      "epoch 111| loss: 0.58357 | val_auc: 0.65547 |  0:06:25s\n",
      "epoch 112| loss: 0.58344 | val_auc: 0.65505 |  0:06:29s\n",
      "epoch 113| loss: 0.58181 | val_auc: 0.66476 |  0:06:33s\n",
      "epoch 114| loss: 0.59082 | val_auc: 0.66257 |  0:06:36s\n",
      "epoch 115| loss: 0.59159 | val_auc: 0.66657 |  0:06:40s\n",
      "epoch 116| loss: 0.57943 | val_auc: 0.66544 |  0:06:43s\n",
      "epoch 117| loss: 0.58868 | val_auc: 0.66824 |  0:06:46s\n",
      "epoch 118| loss: 0.58503 | val_auc: 0.66331 |  0:06:50s\n",
      "epoch 119| loss: 0.58536 | val_auc: 0.66225 |  0:06:53s\n",
      "epoch 120| loss: 0.58104 | val_auc: 0.66188 |  0:06:57s\n",
      "epoch 121| loss: 0.57646 | val_auc: 0.66364 |  0:07:00s\n",
      "epoch 122| loss: 0.5779  | val_auc: 0.66445 |  0:07:04s\n",
      "epoch 123| loss: 0.57005 | val_auc: 0.66127 |  0:07:07s\n",
      "epoch 124| loss: 0.57569 | val_auc: 0.66743 |  0:07:11s\n",
      "epoch 125| loss: 0.58632 | val_auc: 0.67063 |  0:07:14s\n",
      "epoch 126| loss: 0.57732 | val_auc: 0.66834 |  0:07:17s\n",
      "epoch 127| loss: 0.57388 | val_auc: 0.66994 |  0:07:20s\n",
      "epoch 128| loss: 0.56578 | val_auc: 0.66838 |  0:07:23s\n",
      "epoch 129| loss: 0.57458 | val_auc: 0.66364 |  0:07:26s\n",
      "epoch 130| loss: 0.56833 | val_auc: 0.66224 |  0:07:30s\n",
      "epoch 131| loss: 0.56739 | val_auc: 0.65721 |  0:07:33s\n",
      "epoch 132| loss: 0.57982 | val_auc: 0.6617  |  0:07:36s\n",
      "epoch 133| loss: 0.5725  | val_auc: 0.66365 |  0:07:39s\n",
      "epoch 134| loss: 0.56579 | val_auc: 0.65699 |  0:07:42s\n",
      "epoch 135| loss: 0.5665  | val_auc: 0.65994 |  0:07:45s\n",
      "epoch 136| loss: 0.57165 | val_auc: 0.66917 |  0:07:48s\n",
      "epoch 137| loss: 0.56646 | val_auc: 0.66626 |  0:07:52s\n",
      "epoch 138| loss: 0.56753 | val_auc: 0.66195 |  0:07:55s\n",
      "epoch 139| loss: 0.55907 | val_auc: 0.66731 |  0:07:58s\n",
      "epoch 140| loss: 0.55793 | val_auc: 0.66434 |  0:08:01s\n",
      "epoch 141| loss: 0.56434 | val_auc: 0.66698 |  0:08:05s\n",
      "epoch 142| loss: 0.55783 | val_auc: 0.65935 |  0:08:08s\n",
      "epoch 143| loss: 0.5543  | val_auc: 0.66219 |  0:08:12s\n",
      "epoch 144| loss: 0.5565  | val_auc: 0.66285 |  0:08:15s\n",
      "epoch 145| loss: 0.55706 | val_auc: 0.6622  |  0:08:18s\n",
      "epoch 146| loss: 0.56143 | val_auc: 0.66242 |  0:08:22s\n",
      "epoch 147| loss: 0.55724 | val_auc: 0.65786 |  0:08:25s\n",
      "epoch 148| loss: 0.55468 | val_auc: 0.6582  |  0:08:28s\n",
      "epoch 149| loss: 0.55152 | val_auc: 0.65826 |  0:08:32s\n",
      "epoch 150| loss: 0.54627 | val_auc: 0.65597 |  0:08:35s\n",
      "epoch 151| loss: 0.55755 | val_auc: 0.65318 |  0:08:38s\n",
      "epoch 152| loss: 0.55557 | val_auc: 0.66132 |  0:08:41s\n",
      "epoch 153| loss: 0.54997 | val_auc: 0.66546 |  0:08:44s\n",
      "epoch 154| loss: 0.54286 | val_auc: 0.66387 |  0:08:48s\n",
      "epoch 155| loss: 0.53934 | val_auc: 0.66056 |  0:08:51s\n",
      "\n",
      "Early stopping occurred at epoch 155 with best_epoch = 125 and best_val_auc = 0.67063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_readmission_final.zip\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "final_model = TabNetClassifier(n_d=best_params[\"n_d\"], n_a=best_params[\"n_a\"], n_steps=best_params[\"n_steps\"], gamma=best_params[\"gamma\"], lambda_sparse=best_params[\"lambda_sparse\"], optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=best_params[\"lr\"]), mask_type=best_params.get(\"mask_type\", \"entmax\"), device_name=DEVICE, verbose=1)\n",
    "\n",
    "final_model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], eval_name=[\"val\"], eval_metric=[\"auc\"], max_epochs=MAX_FINETUNE_EPOCHS, patience=EARLY_STOPPING_PATIENCE, batch_size=BATCH_SIZE, virtual_batch_size=VIRTUAL_BATCH_SIZE, num_workers=0, drop_last=False, from_unsupervised=pretrainer, weights=1)\n",
    "final_model.save_model(\"tabnet_readmission_final\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0af95f",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ecc6c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error (Vanilla LR): 0.299\n",
      "Test error (Vanilla LR): 0.335\n"
     ]
    }
   ],
   "source": [
    "# calculate training error\n",
    "y_train_pred = final_model.predict(X_train_scaled)\n",
    "train_error = np.mean(y_train_pred != y_train)\n",
    "print(f\"Training error (Vanilla LR): {train_error:.3f}\")\n",
    "\n",
    "# calculate test error\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "y_pred_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "test_error = np.mean(y_pred != y_test)\n",
    "print(f\"Test error (Vanilla LR): {test_error:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cae2d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.67      0.78      8164\n",
      "           1       0.18      0.60      0.28       983\n",
      "\n",
      "    accuracy                           0.67      9147\n",
      "   macro avg       0.56      0.64      0.53      9147\n",
      "weighted avg       0.85      0.67      0.73      9147\n",
      "\n",
      "ROC-AUC: 0.6804\n",
      "95% CI = [0.6621, 0.6983]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = final_model.predict(X_test_scaled)\n",
    "y_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "np.savez(f'../results/tabnet_pretrained_fpr_tpr_thresholds.npz', fpr, tpr, thresholds)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "_, lower, upper = bootstrap_auc_ci(y_test, y_proba)\n",
    "print(f\"95% CI = [{lower:.4f}, {upper:.4f}]\")\n",
    "\n",
    "importances = final_model.feature_importances_\n",
    "feat_imp = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "feat_imp = feat_imp.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y='feature', x='importance', data=feat_imp, color='green')\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce140232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

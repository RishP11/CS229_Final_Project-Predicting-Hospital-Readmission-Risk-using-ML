{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3de49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327ca4ed",
   "metadata": {},
   "source": [
    "Some useful global constants and setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac54c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "CORR_THRESHOLD = 0.85 # correlation threshold for dimensionality reduction\n",
    "TEST_SIZE = 0.30 # train-test split\n",
    "VAL_SIZE = 0.20  # train-val split   \n",
    "N_TRIALS = 50                     \n",
    "MAX_PRETRAIN_EPOCHS = 150\n",
    "MAX_FINETUNE_EPOCHS = 200\n",
    "EARLY_STOPPING_PATIENCE = 30\n",
    "BATCH_SIZE = 2048\n",
    "VIRTUAL_BATCH_SIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FOCAL_ALPHA = 0.75                 # Higher alpha for rare positive class\n",
    "FOCAL_GAMMA = 2.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8958f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee8245",
   "metadata": {},
   "source": [
    "Soem useful helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f425c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc_ci(y_true, y_scores, n_bootstraps=2000, ci=0.95):\n",
    "    \"\"\" \n",
    "    Simple Bootstrapping method to get an confidence interval on the AUROC score.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    aucs = []\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = rng.integers(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        aucs.append(roc_auc_score(y_true[idx], y_scores[idx]))\n",
    "\n",
    "    lower = np.percentile(aucs, (1 - ci) / 2 * 100)\n",
    "    upper = np.percentile(aucs, (1 + ci) / 2 * 100)\n",
    "    return np.mean(aucs), lower, upper\n",
    "\n",
    "def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                         title: str = \"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Readmission', 'Readmission'],yticklabels=['No Readmission', 'Readmission'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true: np.ndarray, y_scores: np.ndarray):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(y_true: np.ndarray, y_scores: np.ndarray):\n",
    "    \"\"\"Plot Precision-Recall curve (better for imbalanced data)\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    ap_score = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {ap_score:.4f})')\n",
    "    plt.axhline(y=y_true.mean(), color='red', linestyle='--', label=f'Baseline ({y_true.mean():.3f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration_curve(y_true: np.ndarray, y_scores: np.ndarray, n_bins: int = 10):\n",
    "    \"\"\"Plot calibration curve\"\"\"\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_true, y_scores, n_bins=n_bins, strategy='uniform'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Model', color='blue')\n",
    "    plt.plot([0, 1], [0, 1], '--', label='Perfect calibration', color='gray')\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933896b6",
   "metadata": {},
   "source": [
    "## Loading the dataset, pre-processing, and analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f71e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>anion_gap_mean</th>\n",
       "      <th>anion_gap_sd</th>\n",
       "      <th>anion_gap_min</th>\n",
       "      <th>anion_gap_max</th>\n",
       "      <th>bicarbonate_mean</th>\n",
       "      <th>bicarbonate_sd</th>\n",
       "      <th>bicarbonate_min</th>\n",
       "      <th>bicarbonate_max</th>\n",
       "      <th>calcium_total_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>urea_nitrogen_min</th>\n",
       "      <th>urea_nitrogen_max</th>\n",
       "      <th>white_blood_cells_mean</th>\n",
       "      <th>white_blood_cells_sd</th>\n",
       "      <th>white_blood_cells_min</th>\n",
       "      <th>white_blood_cells_max</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>icu_los_hours</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200003</td>\n",
       "      <td>13.375000</td>\n",
       "      <td>3.583195</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>3.105295</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.771429</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.471429</td>\n",
       "      <td>13.176711</td>\n",
       "      <td>13.2</td>\n",
       "      <td>43.9</td>\n",
       "      <td>48</td>\n",
       "      <td>M</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200007</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>1.272792</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200009</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.471429</td>\n",
       "      <td>1.471637</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.3</td>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200014</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>2.203028</td>\n",
       "      <td>10.7</td>\n",
       "      <td>14.7</td>\n",
       "      <td>85</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30484</th>\n",
       "      <td>299992</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>2.856153</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.125000</td>\n",
       "      <td>2.609556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.307143</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.134783</td>\n",
       "      <td>3.781727</td>\n",
       "      <td>8.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>299993</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>1.341641</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>2.073644</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.605530</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>299994</td>\n",
       "      <td>16.157895</td>\n",
       "      <td>2.477973</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.631579</td>\n",
       "      <td>3.451417</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>10.076190</td>\n",
       "      <td>2.642329</td>\n",
       "      <td>5.3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>74</td>\n",
       "      <td>F</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>299998</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1.210372</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>87</td>\n",
       "      <td>M</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>299999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>3.394113</td>\n",
       "      <td>15.9</td>\n",
       "      <td>20.7</td>\n",
       "      <td>49</td>\n",
       "      <td>M</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30489 rows Ã— 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       icustay_id  anion_gap_mean  anion_gap_sd  anion_gap_min  anion_gap_max  \\\n",
       "0          200003       13.375000      3.583195            9.0           21.0   \n",
       "1          200007       15.500000      2.121320           14.0           17.0   \n",
       "2          200009        9.500000      2.121320            8.0           11.0   \n",
       "3          200012             NaN           NaN            NaN            NaN   \n",
       "4          200014       10.000000      1.732051            9.0           12.0   \n",
       "...           ...             ...           ...            ...            ...   \n",
       "30484      299992       15.375000      2.856153           11.0           25.0   \n",
       "30485      299993        9.400000      1.341641            8.0           11.0   \n",
       "30486      299994       16.157895      2.477973           13.0           24.0   \n",
       "30487      299998       11.500000      1.732051           10.0           14.0   \n",
       "30488      299999             NaN           NaN            NaN            NaN   \n",
       "\n",
       "       bicarbonate_mean  bicarbonate_sd  bicarbonate_min  bicarbonate_max  \\\n",
       "0             25.250000        3.105295             18.0             28.0   \n",
       "1             23.000000        1.414214             22.0             24.0   \n",
       "2             23.333333        2.081666             21.0             25.0   \n",
       "3                   NaN             NaN              NaN              NaN   \n",
       "4             24.000000        1.000000             23.0             25.0   \n",
       "...                 ...             ...              ...              ...   \n",
       "30484         23.125000        2.609556             15.0             26.0   \n",
       "30485         29.600000        2.073644             26.0             31.0   \n",
       "30486         21.631579        3.451417             17.0             31.0   \n",
       "30487         23.500000        1.290994             22.0             25.0   \n",
       "30488         24.000000             NaN             24.0             24.0   \n",
       "\n",
       "       calcium_total_mean  ...  urea_nitrogen_min  urea_nitrogen_max  \\\n",
       "0                7.771429  ...               10.0               21.0   \n",
       "1                8.900000  ...                8.0               10.0   \n",
       "2                8.000000  ...               15.0               21.0   \n",
       "3                     NaN  ...                NaN                NaN   \n",
       "4                7.733333  ...               21.0               24.0   \n",
       "...                   ...  ...                ...                ...   \n",
       "30484            8.307143  ...                8.0               23.0   \n",
       "30485            8.000000  ...               12.0               15.0   \n",
       "30486            8.100000  ...               28.0               63.0   \n",
       "30487            8.800000  ...               20.0               22.0   \n",
       "30488                 NaN  ...               11.0               13.0   \n",
       "\n",
       "       white_blood_cells_mean  white_blood_cells_sd  white_blood_cells_min  \\\n",
       "0                   26.471429             13.176711                   13.2   \n",
       "1                   10.300000              1.272792                    9.4   \n",
       "2                   12.471429              1.471637                   10.5   \n",
       "3                    4.900000                   NaN                    4.9   \n",
       "4                   13.233333              2.203028                   10.7   \n",
       "...                       ...                   ...                    ...   \n",
       "30484               14.134783              3.781727                    8.1   \n",
       "30485               12.600000              0.605530                   12.0   \n",
       "30486               10.076190              2.642329                    5.3   \n",
       "30487                9.900000              1.210372                    7.9   \n",
       "30488               18.300000              3.394113                   15.9   \n",
       "\n",
       "       white_blood_cells_max  age  gender  icu_los_hours  target  \n",
       "0                       43.9   48       M            141       0  \n",
       "1                       11.2   44       M             30       0  \n",
       "2                       14.3   47       F             51       0  \n",
       "3                        4.9   33       F             10       0  \n",
       "4                       14.7   85       M             41       0  \n",
       "...                      ...  ...     ...            ...     ...  \n",
       "30484                   22.1   41       M            499       0  \n",
       "30485                   13.3   26       M             67       0  \n",
       "30486                   14.5   74       F            152       1  \n",
       "30487                   11.0   87       M             46       1  \n",
       "30488                   20.7   49       M             31       0  \n",
       "\n",
       "[30489 rows x 93 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort_data = pd.read_csv('../cohort_data_new.csv')\n",
    "cohort_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e82ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (30489, 93)\n",
      "Readmission rate: 10.74%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {cohort_data.shape}\")\n",
    "print(f\"Readmission rate: {cohort_data['target'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cff3a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_cols = [\n",
    "    'anion_gap_mean', 'anion_gap_min', 'anion_gap_max', 'anion_gap_sd',\n",
    "    'bicarbonate_mean', 'bicarbonate_min', 'bicarbonate_max', 'bicarbonate_sd',\n",
    "    'calcium_total_mean', 'calcium_total_min', 'calcium_total_max', 'calcium_total_sd',\n",
    "    'chloride_mean', 'chloride_min', 'chloride_max', 'chloride_sd',\n",
    "    'creatinine_mean', 'creatinine_min', 'creatinine_max', 'creatinine_sd',\n",
    "    'glucose_mean', 'glucose_min', 'glucose_max', 'glucose_sd',\n",
    "    'hematocrit_mean', 'hematocrit_min', 'hematocrit_max', 'hematocrit_sd',\n",
    "    'hemoglobin_mean', 'hemoglobin_min', 'hemoglobin_max', 'hemoglobin_sd',\n",
    "    'mchc_mean', 'mchc_min', 'mchc_max', 'mchc_sd',\n",
    "    'mcv_mean', 'mcv_min', 'mcv_max', 'mcv_sd',\n",
    "    'magnesium_mean', 'magnesium_min', 'magnesium_max', 'magnesium_sd',\n",
    "    'pt_mean', 'pt_min', 'pt_max', 'pt_sd',\n",
    "    'phosphate_mean', 'phosphate_min', 'phosphate_max', 'phosphate_sd',\n",
    "    'platelet_count_mean', 'platelet_count_min', 'platelet_count_max', 'platelet_count_sd',\n",
    "    'potassium_mean', 'potassium_min', 'potassium_max', 'potassium_sd',\n",
    "    'rdw_mean', 'rdw_min', 'rdw_max', 'rdw_sd',\n",
    "    'red_blood_cells_mean', 'red_blood_cells_min', 'red_blood_cells_max', 'red_blood_cells_sd',\n",
    "    'sodium_mean', 'sodium_min', 'sodium_max', 'sodium_sd',\n",
    "    'urea_nitrogen_mean', 'urea_nitrogen_min', 'urea_nitrogen_max', 'urea_nitrogen_sd',\n",
    "    'white_blood_cells_mean', 'white_blood_cells_min', 'white_blood_cells_max', 'white_blood_cells_sd',\n",
    "    'age', 'icu_los_hours'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66668fd",
   "metadata": {},
   "source": [
    "REmove the ICUstay_id and the gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1464d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [c for c in cohort_data.columns if 'icustay_id' in c.lower() or 'gender' in c.lower()]\n",
    "df = cohort_data.drop(columns=['icustay_id', 'gender'], errors='ignore')\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473f929",
   "metadata": {},
   "source": [
    "Trying out some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d754a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial feature matrix shape: (30489, 90)\n",
      "final feature matrix shape: (30489, 98)\n"
     ]
    }
   ],
   "source": [
    "X = X.select_dtypes(include=['number']).replace([np.inf, -np.inf], np.nan)\n",
    "print(f\"initial feature matrix shape: {X.shape}\")\n",
    "\n",
    "def create_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # BUN/Creatinine ratio (kidney function indicator)\n",
    "    if 'urea_nitrogen_mean' in df_eng.columns and 'creatinine_mean' in df_eng.columns:\n",
    "        df_eng['bun_creatinine_ratio'] = (\n",
    "            df_eng['urea_nitrogen_mean'] / (df_eng['creatinine_mean'] + 1e-6)\n",
    "        )\n",
    "\n",
    "    # Variability indices (physiological instability)\n",
    "    variability_features = []\n",
    "    for base_name in ['glucose', 'potassium', 'sodium', 'hemoglobin']:\n",
    "        mean_col = f'{base_name}_mean'\n",
    "        sd_col = f'{base_name}_sd'\n",
    "        if mean_col in df_eng.columns and sd_col in df_eng.columns:\n",
    "            cv_col = f'{base_name}_cv'\n",
    "            df_eng[cv_col] = df_eng[sd_col] / (df_eng[mean_col] + 1e-6)\n",
    "            variability_features.append(cv_col)\n",
    "    \n",
    "    # Range features (max - min)\n",
    "    for base_name in ['glucose', 'creatinine', 'potassium']:\n",
    "        max_col = f'{base_name}_max'\n",
    "        min_col = f'{base_name}_min'\n",
    "        if max_col in df_eng.columns and min_col in df_eng.columns:\n",
    "            range_col = f'{base_name}_range'\n",
    "            df_eng[range_col] = df_eng[max_col] - df_eng[min_col]\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "X_engineered = create_engineered_features(X)\n",
    "print(f\"final feature matrix shape: {X_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd98d7b",
   "metadata": {},
   "source": [
    "Dimensionality reduction by deleting the columns with high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1931de36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 42 features\n",
      "final feature count: 56\n"
     ]
    }
   ],
   "source": [
    "corr = X_engineered.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] >= CORR_THRESHOLD)]\n",
    "\n",
    "print(f\"dropping {len(to_drop)} features\")\n",
    "X_reduced = X_engineered.drop(columns=to_drop, errors='ignore')\n",
    "print(f\"final feature count: {X_reduced.shape[1]}\")\n",
    "feature_names = X_reduced.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8e811",
   "metadata": {},
   "source": [
    "Creating the final train-val-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59d3ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 17073 samples (10.74% readmission)\n",
      "Validation set: 4269 samples (10.75% readmission)\n",
      "Test set: 9147 samples (10.75% readmission)\n"
     ]
    }
   ],
   "source": [
    "# separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_reduced, y.values, test_size=TEST_SIZE, random_state=SEED, stratify=y.values)\n",
    "\n",
    "# separate validation set \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=SEED, stratify=y_temp)\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({y_train.mean()*100:.2f}% readmission)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({y_val.mean()*100:.2f}% readmission)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({y_test.mean()*100:.2f}% readmission)\")\n",
    "\n",
    "# SimpleImputation using median strat and scaling;\n",
    "# Imputation - FIT on train only and avoidning data leakage:\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scaling - FIT on train only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439a817",
   "metadata": {},
   "source": [
    "### Pretraining the TABNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff4acb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2873458.54586| val_0_unsup_loss_numpy: 249431.34375|  0:00:02s\n",
      "epoch 1  | loss: 2178197.42618| val_0_unsup_loss_numpy: 453751.53125|  0:00:05s\n",
      "epoch 2  | loss: 1646358.82802| val_0_unsup_loss_numpy: 418497.21875|  0:00:08s\n",
      "epoch 3  | loss: 1313544.28527| val_0_unsup_loss_numpy: 350956.40625|  0:00:11s\n",
      "epoch 4  | loss: 1134472.02326| val_0_unsup_loss_numpy: 384032.40625|  0:00:14s\n",
      "epoch 5  | loss: 910770.47804| val_0_unsup_loss_numpy: 499828.5|  0:00:16s\n",
      "epoch 6  | loss: 797942.89425| val_0_unsup_loss_numpy: 175733.984375|  0:00:19s\n",
      "epoch 7  | loss: 701835.1237| val_0_unsup_loss_numpy: 278253.875|  0:00:22s\n",
      "epoch 8  | loss: 547988.5787| val_0_unsup_loss_numpy: 356743.375|  0:00:24s\n",
      "epoch 9  | loss: 528224.34876| val_0_unsup_loss_numpy: 171060.046875|  0:00:27s\n",
      "epoch 10 | loss: 476515.14827| val_0_unsup_loss_numpy: 186770.4375|  0:00:30s\n",
      "epoch 11 | loss: 458406.31128| val_0_unsup_loss_numpy: 143591.578125|  0:00:33s\n",
      "epoch 12 | loss: 389109.70262| val_0_unsup_loss_numpy: 169919.8125|  0:00:35s\n",
      "epoch 13 | loss: 349704.35015| val_0_unsup_loss_numpy: 110131.125|  0:00:38s\n",
      "epoch 14 | loss: 336956.81613| val_0_unsup_loss_numpy: 353928.53125|  0:00:41s\n",
      "epoch 15 | loss: 301144.98358| val_0_unsup_loss_numpy: 253220.515625|  0:00:45s\n",
      "epoch 16 | loss: 302327.28153| val_0_unsup_loss_numpy: 136117.953125|  0:00:48s\n",
      "epoch 17 | loss: 245004.72403| val_0_unsup_loss_numpy: 118528.8828125|  0:00:51s\n",
      "epoch 18 | loss: 219114.66449| val_0_unsup_loss_numpy: 105319.9140625|  0:00:54s\n",
      "epoch 19 | loss: 167382.51023| val_0_unsup_loss_numpy: 389648.09375|  0:00:57s\n",
      "epoch 20 | loss: 190662.41636| val_0_unsup_loss_numpy: 74495.5546875|  0:01:00s\n",
      "epoch 21 | loss: 187917.79275| val_0_unsup_loss_numpy: 401403.0|  0:01:03s\n",
      "epoch 22 | loss: 169644.94473| val_0_unsup_loss_numpy: 634922.1875|  0:01:06s\n",
      "epoch 23 | loss: 151765.63693| val_0_unsup_loss_numpy: 209509.0|  0:01:10s\n",
      "epoch 24 | loss: 134629.70836| val_0_unsup_loss_numpy: 180044.078125|  0:01:13s\n",
      "epoch 25 | loss: 130622.03632| val_0_unsup_loss_numpy: 270970.375|  0:01:16s\n",
      "epoch 26 | loss: 134883.86171| val_0_unsup_loss_numpy: 226745.40625|  0:01:19s\n",
      "epoch 27 | loss: 111045.08484| val_0_unsup_loss_numpy: 261795.796875|  0:01:23s\n",
      "epoch 28 | loss: 97138.1655| val_0_unsup_loss_numpy: 104722.1171875|  0:01:26s\n",
      "epoch 29 | loss: 94731.20465| val_0_unsup_loss_numpy: 55773.5234375|  0:01:29s\n",
      "epoch 30 | loss: 93563.54779| val_0_unsup_loss_numpy: 109122.0703125|  0:01:32s\n",
      "epoch 31 | loss: 95262.2096| val_0_unsup_loss_numpy: 70348.8984375|  0:01:35s\n",
      "epoch 32 | loss: 71217.13709| val_0_unsup_loss_numpy: 61659.1328125|  0:01:38s\n",
      "epoch 33 | loss: 69820.28982| val_0_unsup_loss_numpy: 88466.9609375|  0:01:41s\n",
      "epoch 34 | loss: 60764.17859| val_0_unsup_loss_numpy: 24330.28515625|  0:01:44s\n",
      "epoch 35 | loss: 61459.93785| val_0_unsup_loss_numpy: 59280.671875|  0:01:47s\n",
      "epoch 36 | loss: 62941.08551| val_0_unsup_loss_numpy: 44419.74609375|  0:01:50s\n",
      "epoch 37 | loss: 57096.0112| val_0_unsup_loss_numpy: 68715.4609375|  0:01:54s\n",
      "epoch 38 | loss: 53710.16868| val_0_unsup_loss_numpy: 49971.8046875|  0:01:57s\n",
      "epoch 39 | loss: 48762.20333| val_0_unsup_loss_numpy: 22349.11328125|  0:02:00s\n",
      "epoch 40 | loss: 46594.53973| val_0_unsup_loss_numpy: 36613.30078125|  0:02:03s\n",
      "epoch 41 | loss: 43910.8465| val_0_unsup_loss_numpy: 21342.349609375|  0:02:06s\n",
      "epoch 42 | loss: 37635.97501| val_0_unsup_loss_numpy: 17820.37109375|  0:02:10s\n",
      "epoch 43 | loss: 40386.24522| val_0_unsup_loss_numpy: 13953.478515625|  0:02:13s\n",
      "epoch 44 | loss: 39263.51693| val_0_unsup_loss_numpy: 16024.9013671875|  0:02:16s\n",
      "epoch 45 | loss: 38789.98831| val_0_unsup_loss_numpy: 21952.771484375|  0:02:19s\n",
      "epoch 46 | loss: 35728.2262| val_0_unsup_loss_numpy: 15057.52734375|  0:02:22s\n",
      "epoch 47 | loss: 28639.33403| val_0_unsup_loss_numpy: 11711.869140625|  0:02:25s\n",
      "epoch 48 | loss: 29355.3293| val_0_unsup_loss_numpy: 10772.58984375|  0:02:28s\n",
      "epoch 49 | loss: 32394.93565| val_0_unsup_loss_numpy: 8357.5244140625|  0:02:31s\n",
      "epoch 50 | loss: 27164.40924| val_0_unsup_loss_numpy: 7790.97705078125|  0:02:34s\n",
      "epoch 51 | loss: 22892.06105| val_0_unsup_loss_numpy: 14084.392578125|  0:02:37s\n",
      "epoch 52 | loss: 22552.0784| val_0_unsup_loss_numpy: 8848.376953125|  0:02:41s\n",
      "epoch 53 | loss: 22063.35468| val_0_unsup_loss_numpy: 9626.0205078125|  0:02:44s\n",
      "epoch 54 | loss: 21711.37469| val_0_unsup_loss_numpy: 5645.544921875|  0:02:47s\n",
      "epoch 55 | loss: 19016.79049| val_0_unsup_loss_numpy: 9502.37890625|  0:02:50s\n",
      "epoch 56 | loss: 18072.16296| val_0_unsup_loss_numpy: 7499.8544921875|  0:02:53s\n",
      "epoch 57 | loss: 15351.16886| val_0_unsup_loss_numpy: 6859.40625|  0:02:56s\n",
      "epoch 58 | loss: 14216.38587| val_0_unsup_loss_numpy: 14703.935546875|  0:03:00s\n",
      "epoch 59 | loss: 12732.50296| val_0_unsup_loss_numpy: 9999.3779296875|  0:03:03s\n",
      "epoch 60 | loss: 12975.54653| val_0_unsup_loss_numpy: 8055.03857421875|  0:03:06s\n",
      "epoch 61 | loss: 11629.40114| val_0_unsup_loss_numpy: 4575.24658203125|  0:03:09s\n",
      "epoch 62 | loss: 10957.6699| val_0_unsup_loss_numpy: 7514.63916015625|  0:03:12s\n",
      "epoch 63 | loss: 10258.19256| val_0_unsup_loss_numpy: 3434.028076171875|  0:03:16s\n",
      "epoch 64 | loss: 8210.94697| val_0_unsup_loss_numpy: 3539.818359375|  0:03:19s\n",
      "epoch 65 | loss: 8016.92553| val_0_unsup_loss_numpy: 2957.95361328125|  0:03:22s\n",
      "epoch 66 | loss: 8685.21287| val_0_unsup_loss_numpy: 6757.27880859375|  0:03:25s\n",
      "epoch 67 | loss: 7056.91066| val_0_unsup_loss_numpy: 4237.52490234375|  0:03:28s\n",
      "epoch 68 | loss: 7109.38191| val_0_unsup_loss_numpy: 5055.6884765625|  0:03:31s\n",
      "epoch 69 | loss: 6498.52974| val_0_unsup_loss_numpy: 7118.369140625|  0:03:35s\n",
      "epoch 70 | loss: 6499.91546| val_0_unsup_loss_numpy: 3563.12353515625|  0:03:38s\n",
      "epoch 71 | loss: 5828.94859| val_0_unsup_loss_numpy: 3635.222412109375|  0:03:41s\n",
      "epoch 72 | loss: 5689.5214| val_0_unsup_loss_numpy: 3397.668701171875|  0:03:44s\n",
      "epoch 73 | loss: 4397.04419| val_0_unsup_loss_numpy: 10165.359375|  0:03:47s\n",
      "epoch 74 | loss: 3891.7126| val_0_unsup_loss_numpy: 1994.455322265625|  0:03:50s\n",
      "epoch 75 | loss: 3936.13795| val_0_unsup_loss_numpy: 995.8043823242188|  0:03:53s\n",
      "epoch 76 | loss: 2757.53878| val_0_unsup_loss_numpy: 2226.953857421875|  0:03:56s\n",
      "epoch 77 | loss: 2809.07251| val_0_unsup_loss_numpy: 1640.208984375|  0:03:59s\n",
      "epoch 78 | loss: 2631.02241| val_0_unsup_loss_numpy: 1671.6566162109375|  0:04:03s\n",
      "epoch 79 | loss: 2404.34606| val_0_unsup_loss_numpy: 855.9216918945312|  0:04:06s\n",
      "epoch 80 | loss: 2514.83628| val_0_unsup_loss_numpy: 2240.922607421875|  0:04:09s\n",
      "epoch 81 | loss: 1903.18944| val_0_unsup_loss_numpy: 1660.485107421875|  0:04:14s\n",
      "epoch 82 | loss: 1393.21728| val_0_unsup_loss_numpy: 1470.0302734375|  0:04:19s\n",
      "epoch 83 | loss: 1383.94733| val_0_unsup_loss_numpy: 797.1280517578125|  0:04:24s\n",
      "epoch 84 | loss: 1340.7638| val_0_unsup_loss_numpy: 1725.573486328125|  0:04:28s\n",
      "epoch 85 | loss: 1253.46692| val_0_unsup_loss_numpy: 952.4300537109375|  0:04:34s\n",
      "epoch 86 | loss: 1371.10844| val_0_unsup_loss_numpy: 680.0868530273438|  0:04:38s\n",
      "epoch 87 | loss: 1040.2918| val_0_unsup_loss_numpy: 311.87017822265625|  0:04:43s\n",
      "epoch 88 | loss: 1044.5208| val_0_unsup_loss_numpy: 554.5767211914062|  0:04:48s\n",
      "epoch 89 | loss: 671.75335| val_0_unsup_loss_numpy: 1006.4979858398438|  0:04:52s\n",
      "epoch 90 | loss: 608.00213| val_0_unsup_loss_numpy: 381.07080078125|  0:04:58s\n",
      "epoch 91 | loss: 598.31001| val_0_unsup_loss_numpy: 211.33273315429688|  0:05:01s\n",
      "epoch 92 | loss: 477.91082| val_0_unsup_loss_numpy: 179.48500061035156|  0:05:05s\n",
      "epoch 93 | loss: 390.4985| val_0_unsup_loss_numpy: 191.09715270996094|  0:05:08s\n",
      "epoch 94 | loss: 362.77548| val_0_unsup_loss_numpy: 121.62715148925781|  0:05:11s\n",
      "epoch 95 | loss: 334.38453| val_0_unsup_loss_numpy: 121.42748260498047|  0:05:15s\n",
      "epoch 96 | loss: 204.96085| val_0_unsup_loss_numpy: 83.49382019042969|  0:05:18s\n",
      "epoch 97 | loss: 191.98166| val_0_unsup_loss_numpy: 209.46133422851562|  0:05:21s\n",
      "epoch 98 | loss: 159.93514| val_0_unsup_loss_numpy: 63.49980163574219|  0:05:25s\n",
      "epoch 99 | loss: 116.79607| val_0_unsup_loss_numpy: 70.51873016357422|  0:05:28s\n",
      "epoch 100| loss: 85.27913| val_0_unsup_loss_numpy: 64.39408874511719|  0:05:31s\n",
      "epoch 101| loss: 69.23118| val_0_unsup_loss_numpy: 39.18785095214844|  0:05:34s\n",
      "epoch 102| loss: 51.25441| val_0_unsup_loss_numpy: 39.21636962890625|  0:05:37s\n",
      "epoch 103| loss: 36.48098| val_0_unsup_loss_numpy: 64.5058822631836|  0:05:40s\n",
      "epoch 104| loss: 24.97371| val_0_unsup_loss_numpy: 14.259659767150879|  0:05:43s\n",
      "epoch 105| loss: 16.58844| val_0_unsup_loss_numpy: 96.2569580078125|  0:05:46s\n",
      "epoch 106| loss: 17.595  | val_0_unsup_loss_numpy: 19.162979125976562|  0:05:50s\n",
      "epoch 107| loss: 21.36536| val_0_unsup_loss_numpy: 24.71702003479004|  0:05:53s\n",
      "epoch 108| loss: 13.40378| val_0_unsup_loss_numpy: 18.59716033935547|  0:05:56s\n",
      "epoch 109| loss: 9.93905 | val_0_unsup_loss_numpy: 33.24842071533203|  0:05:59s\n",
      "epoch 110| loss: 25.04887| val_0_unsup_loss_numpy: 29.03175926208496|  0:06:02s\n",
      "epoch 111| loss: 19.44558| val_0_unsup_loss_numpy: 7.500030040740967|  0:06:05s\n",
      "epoch 112| loss: 6.94132 | val_0_unsup_loss_numpy: 5.199379920959473|  0:06:08s\n",
      "epoch 113| loss: 8.91512 | val_0_unsup_loss_numpy: 6.520659923553467|  0:06:11s\n",
      "epoch 114| loss: 7.38396 | val_0_unsup_loss_numpy: 4.87932014465332|  0:06:14s\n",
      "epoch 115| loss: 7.049   | val_0_unsup_loss_numpy: 4.667300224304199|  0:06:17s\n",
      "epoch 116| loss: 6.51036 | val_0_unsup_loss_numpy: 4.283609867095947|  0:06:20s\n",
      "epoch 117| loss: 4.41899 | val_0_unsup_loss_numpy: 4.0742998123168945|  0:06:23s\n",
      "epoch 118| loss: 5.03409 | val_0_unsup_loss_numpy: 6.682370185852051|  0:06:27s\n",
      "epoch 119| loss: 5.80534 | val_0_unsup_loss_numpy: 3.2969400882720947|  0:06:30s\n",
      "epoch 120| loss: 4.19047 | val_0_unsup_loss_numpy: 4.925179958343506|  0:06:33s\n",
      "epoch 121| loss: 4.84856 | val_0_unsup_loss_numpy: 6.487500190734863|  0:06:36s\n",
      "epoch 122| loss: 5.15766 | val_0_unsup_loss_numpy: 4.757460117340088|  0:06:39s\n",
      "epoch 123| loss: 4.0831  | val_0_unsup_loss_numpy: 6.558509826660156|  0:06:42s\n",
      "epoch 124| loss: 3.92285 | val_0_unsup_loss_numpy: 3.4589500427246094|  0:06:45s\n",
      "epoch 125| loss: 3.43725 | val_0_unsup_loss_numpy: 3.559920072555542|  0:06:48s\n",
      "epoch 126| loss: 3.30825 | val_0_unsup_loss_numpy: 5.480329990386963|  0:06:51s\n",
      "epoch 127| loss: 3.67305 | val_0_unsup_loss_numpy: 4.850599765777588|  0:06:54s\n",
      "epoch 128| loss: 4.11451 | val_0_unsup_loss_numpy: 3.1484200954437256|  0:06:57s\n",
      "epoch 129| loss: 3.52342 | val_0_unsup_loss_numpy: 3.44281005859375|  0:07:01s\n",
      "epoch 130| loss: 3.27682 | val_0_unsup_loss_numpy: 2.77987003326416|  0:07:04s\n",
      "epoch 131| loss: 4.15004 | val_0_unsup_loss_numpy: 2.6383399963378906|  0:07:07s\n",
      "epoch 132| loss: 3.32861 | val_0_unsup_loss_numpy: 2.4435300827026367|  0:07:10s\n",
      "epoch 133| loss: 3.698   | val_0_unsup_loss_numpy: 3.111409902572632|  0:07:14s\n",
      "epoch 134| loss: 3.06971 | val_0_unsup_loss_numpy: 2.5990099906921387|  0:07:17s\n",
      "epoch 135| loss: 4.22561 | val_0_unsup_loss_numpy: 4.148290157318115|  0:07:20s\n",
      "epoch 136| loss: 3.25974 | val_0_unsup_loss_numpy: 2.8710999488830566|  0:07:23s\n",
      "epoch 137| loss: 4.17982 | val_0_unsup_loss_numpy: 5.883480072021484|  0:07:26s\n",
      "epoch 138| loss: 4.47593 | val_0_unsup_loss_numpy: 2.7045700550079346|  0:07:29s\n",
      "epoch 139| loss: 2.9362  | val_0_unsup_loss_numpy: 2.697499990463257|  0:07:32s\n",
      "epoch 140| loss: 2.68842 | val_0_unsup_loss_numpy: 2.6064200401306152|  0:07:35s\n",
      "epoch 141| loss: 2.92866 | val_0_unsup_loss_numpy: 2.549570083618164|  0:07:38s\n",
      "epoch 142| loss: 2.61737 | val_0_unsup_loss_numpy: 3.103600025177002|  0:07:41s\n",
      "epoch 143| loss: 2.86288 | val_0_unsup_loss_numpy: 2.747119903564453|  0:07:44s\n",
      "epoch 144| loss: 2.60811 | val_0_unsup_loss_numpy: 2.472759962081909|  0:07:47s\n",
      "epoch 145| loss: 2.72898 | val_0_unsup_loss_numpy: 4.475430011749268|  0:07:51s\n",
      "epoch 146| loss: 5.25395 | val_0_unsup_loss_numpy: 3.027129888534546|  0:07:54s\n",
      "epoch 147| loss: 3.88456 | val_0_unsup_loss_numpy: 2.070620059967041|  0:07:57s\n",
      "epoch 148| loss: 2.60823 | val_0_unsup_loss_numpy: 2.845720052719116|  0:08:00s\n",
      "epoch 149| loss: 2.906   | val_0_unsup_loss_numpy: 2.265549898147583|  0:08:03s\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_0_unsup_loss_numpy = 2.070620059967041\n",
      "Pretraining complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "def run_pretraining(X_train: np.ndarray, X_val: np.ndarray, pretrain_params: dict):\n",
    "    \"\"\"Run unsupervised pretraining\"\"\"\n",
    "    pretrainer = TabNetPretrainer(**pretrain_params)\n",
    "    pretrainer.fit(\n",
    "        X_train=X_train,\n",
    "        eval_set=[X_val],\n",
    "        max_epochs=MAX_PRETRAIN_EPOCHS,\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    print(\"Pretraining complete!\")\n",
    "    return pretrainer\n",
    "\n",
    "pretrain_params = dict(\n",
    "    n_d=32, \n",
    "    n_a=32,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-3),\n",
    "    mask_type=\"entmax\",\n",
    "    device_name=DEVICE\n",
    ")\n",
    "\n",
    "pretrainer = run_pretraining(X_train_scaled, X_val_scaled, pretrain_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b5c7a",
   "metadata": {},
   "source": [
    "Hyperparam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86a7959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(X_train, y_train, X_val, y_val, class_weights_tensor, pretrainer):\n",
    "    \"\"\"Create Optuna objective function\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Hyperparameters to tune\n",
    "        n_d = trial.suggest_int(\"n_d\", 16, 64)\n",
    "        n_a = trial.suggest_int(\"n_a\", 16, 64)\n",
    "        n_steps = trial.suggest_int(\"n_steps\", 3, 7)\n",
    "        gamma = trial.suggest_float(\"gamma\", 1.0, 2.5)\n",
    "        lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        mask_type = trial.suggest_categorical(\"mask_type\", [\"sparsemax\", \"entmax\"])\n",
    "        \n",
    "        clf = TabNetClassifier(n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma, lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,optimizer_params=dict(lr=lr), mask_type=mask_type, device_name=DEVICE, verbose=0)\n",
    "        \n",
    "        try:\n",
    "            clf.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_name=[\"val\"],\n",
    "                eval_metric=[\"auc\"],\n",
    "                max_epochs=MAX_FINETUNE_EPOCHS,\n",
    "                patience=20, \n",
    "                batch_size=BATCH_SIZE,\n",
    "                virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "                num_workers=0,\n",
    "                drop_last=False,\n",
    "                from_unsupervised=pretrainer,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        pred_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, pred_proba)\n",
    "        \n",
    "        return auc\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f1154f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:35:05,300] A new study created in memory with name: tabnet_readmission_10pct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter optimization under way\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3569a6112fb74a43975cfcf32cd15b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 42 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_auc = 0.55876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:36:05,597] Trial 0 finished with value: 0.5587609146895853 and parameters: {'n_d': 16, 'n_a': 42, 'n_steps': 3, 'gamma': 2.0279761670348284, 'lambda_sparse': 0.00023585544043090657, 'lr': 0.0005995208416834294, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.5587609146895853.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 27 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_val_auc = 0.55598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:37:06,992] Trial 1 finished with value: 0.5559792771001664 and parameters: {'n_d': 27, 'n_a': 21, 'n_steps': 4, 'gamma': 2.4362947429033426, 'lambda_sparse': 2.1576224252756286e-06, 'lr': 0.00014885526310768444, 'mask_type': 'entmax'}. Best is trial 0 with value: 0.5587609146895853.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 17 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 50 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.55157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:38:03,513] Trial 2 finished with value: 0.5515722299418455 and parameters: {'n_d': 50, 'n_a': 17, 'n_steps': 7, 'gamma': 1.6818613284896964, 'lambda_sparse': 2.071243394344492e-06, 'lr': 0.0019745299263699027, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.5587609146895853.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 42 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 19 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_auc = 0.56718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:39:39,219] Trial 3 finished with value: 0.567176161803304 and parameters: {'n_d': 19, 'n_a': 42, 'n_steps': 7, 'gamma': 1.5709721776426693, 'lambda_sparse': 2.140208127619011e-05, 'lr': 0.00024370476626022695, 'mask_type': 'sparsemax'}. Best is trial 3 with value: 0.567176161803304.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 49 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 24 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.55714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:40:48,106] Trial 4 finished with value: 0.5571406515362051 and parameters: {'n_d': 24, 'n_a': 49, 'n_steps': 7, 'gamma': 1.9786304592963204, 'lambda_sparse': 8.266405914122419e-05, 'lr': 0.0015557032472153364, 'mask_type': 'entmax'}. Best is trial 3 with value: 0.567176161803304.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 41 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 45 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.57306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:41:52,718] Trial 5 finished with value: 0.5730610879522413 and parameters: {'n_d': 45, 'n_a': 41, 'n_steps': 4, 'gamma': 1.2501140854293475, 'lambda_sparse': 0.0002938146458747543, 'lr': 0.0001831861495350541, 'mask_type': 'entmax'}. Best is trial 5 with value: 0.5730610879522413.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 24 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 58 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_auc = 0.57813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:43:04,921] Trial 6 finished with value: 0.5781260185614052 and parameters: {'n_d': 58, 'n_a': 24, 'n_steps': 5, 'gamma': 1.4096510069916823, 'lambda_sparse': 3.969473009794932e-06, 'lr': 0.00013612241447842713, 'mask_type': 'sparsemax'}. Best is trial 6 with value: 0.5781260185614052.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 48 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_auc = 0.56918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:44:25,827] Trial 7 finished with value: 0.5691849793285644 and parameters: {'n_d': 48, 'n_a': 16, 'n_steps': 7, 'gamma': 1.7060753686316334, 'lambda_sparse': 3.204512131800496e-05, 'lr': 0.00023362139296410526, 'mask_type': 'sparsemax'}. Best is trial 6 with value: 0.5781260185614052.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 47 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.57042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:45:32,574] Trial 8 finished with value: 0.5704181176699318 and parameters: {'n_d': 64, 'n_a': 47, 'n_steps': 4, 'gamma': 1.488164595382199, 'lambda_sparse': 1.3009746949084117e-06, 'lr': 0.000600288387434428, 'mask_type': 'entmax'}. Best is trial 6 with value: 0.5781260185614052.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 59 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_auc = 0.57014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:46:53,165] Trial 9 finished with value: 0.5701376380240051 and parameters: {'n_d': 18, 'n_a': 59, 'n_steps': 6, 'gamma': 1.7849146144126498, 'lambda_sparse': 9.352867688957997e-05, 'lr': 0.00030018372508486946, 'mask_type': 'entmax'}. Best is trial 6 with value: 0.5781260185614052.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 29 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_auc = 0.65634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:49:26,132] Trial 10 finished with value: 0.6563406698345714 and parameters: {'n_d': 63, 'n_a': 29, 'n_steps': 5, 'gamma': 1.1177037425123468, 'lambda_sparse': 7.439923686686814e-06, 'lr': 0.004478360820807508, 'mask_type': 'sparsemax'}. Best is trial 10 with value: 0.6563406698345714.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 29 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 63 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_val_auc = 0.69462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 14:54:00,132] Trial 11 finished with value: 0.6946191366602051 and parameters: {'n_d': 63, 'n_a': 29, 'n_steps': 5, 'gamma': 1.0368448017651986, 'lambda_sparse': 7.962313494947115e-06, 'lr': 0.004503380969275998, 'mask_type': 'sparsemax'}. Best is trial 11 with value: 0.6946191366602051.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 30 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 84 with best_epoch = 64 and best_val_auc = 0.69116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:00:30,564] Trial 12 finished with value: 0.6911630327254845 and parameters: {'n_d': 64, 'n_a': 30, 'n_steps': 5, 'gamma': 1.042767635337442, 'lambda_sparse': 9.308953617762418e-06, 'lr': 0.004801163413084997, 'mask_type': 'sparsemax'}. Best is trial 11 with value: 0.6946191366602051.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 31 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 55 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 86 with best_epoch = 66 and best_val_auc = 0.68232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:06:17,739] Trial 13 finished with value: 0.6823177740037397 and parameters: {'n_d': 55, 'n_a': 31, 'n_steps': 5, 'gamma': 1.0238502352569834, 'lambda_sparse': 9.86114278551921e-06, 'lr': 0.0035119091345912606, 'mask_type': 'sparsemax'}. Best is trial 11 with value: 0.6946191366602051.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 33 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 37 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 135 with best_epoch = 115 and best_val_auc = 0.67634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:14:38,310] Trial 14 finished with value: 0.6763436433190949 and parameters: {'n_d': 37, 'n_a': 33, 'n_steps': 6, 'gamma': 1.2581888201660896, 'lambda_sparse': 1.809310610189356e-05, 'lr': 0.0023094446407705507, 'mask_type': 'sparsemax'}. Best is trial 11 with value: 0.6946191366602051.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 35 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 56 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.56531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:15:39,367] Trial 15 finished with value: 0.5653074411450203 and parameters: {'n_d': 56, 'n_a': 35, 'n_steps': 6, 'gamma': 1.000267584828258, 'lambda_sparse': 5.231007449961608e-06, 'lr': 0.0010637854893068462, 'mask_type': 'sparsemax'}. Best is trial 11 with value: 0.6946191366602051.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 26 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 37 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 89 with best_epoch = 69 and best_val_auc = 0.67161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:21:09,442] Trial 16 finished with value: 0.6716100846871265 and parameters: {'n_d': 37, 'n_a': 26, 'n_steps': 3, 'gamma': 1.2545857965130243, 'lambda_sparse': 5.8927958086102926e-05, 'lr': 0.003165664763134903, 'mask_type': 'sparsemax'}. Best is trial 11 with value: 0.6946191366602051.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 36 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 92 with best_epoch = 72 and best_val_auc = 0.62943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:28:08,538] Trial 17 finished with value: 0.6294334940158625 and parameters: {'n_d': 60, 'n_a': 36, 'n_steps': 4, 'gamma': 2.493670594753345, 'lambda_sparse': 1.3805595230608313e-05, 'lr': 0.0049052763757230665, 'mask_type': 'sparsemax'}. Best is trial 11 with value: 0.6946191366602051.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 56 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 52 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_val_auc = 0.55817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:29:27,397] Trial 18 finished with value: 0.558174795144071 and parameters: {'n_d': 52, 'n_a': 56, 'n_steps': 6, 'gamma': 1.324276462590059, 'lambda_sparse': 0.0007902938726958225, 'lr': 0.001169721945218466, 'mask_type': 'sparsemax'}. Best is trial 11 with value: 0.6946191366602051.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 22 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 44 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 117 with best_epoch = 97 and best_val_auc = 0.69897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:36:28,741] Trial 19 finished with value: 0.6989738619273898 and parameters: {'n_d': 44, 'n_a': 22, 'n_steps': 5, 'gamma': 1.1481212253039474, 'lambda_sparse': 3.9715216216283716e-05, 'lr': 0.002616746364609544, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 42 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.55093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:37:36,148] Trial 20 finished with value: 0.550933788505195 and parameters: {'n_d': 42, 'n_a': 21, 'n_steps': 4, 'gamma': 2.22080246155102, 'lambda_sparse': 3.987630801446338e-05, 'lr': 0.0025689181416155907, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 27 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 33 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 54 and best_val_auc = 0.69043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:42:13,358] Trial 21 finished with value: 0.6904345290172061 and parameters: {'n_d': 33, 'n_a': 27, 'n_steps': 5, 'gamma': 1.1472581259463779, 'lambda_sparse': 4.541535755278825e-06, 'lr': 0.0038733305274092095, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 61 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 49 and best_val_auc = 0.68627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 15:45:51,524] Trial 22 finished with value: 0.6862739379799748 and parameters: {'n_d': 61, 'n_a': 21, 'n_steps': 5, 'gamma': 1.1302094176316995, 'lambda_sparse': 1.1070200654745234e-05, 'lr': 0.004936519617910341, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 36 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 53 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 200 with best_epoch = 184 and best_val_auc = 0.68766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:01:22,879] Trial 23 finished with value: 0.6876594674031759 and parameters: {'n_d': 53, 'n_a': 36, 'n_steps': 6, 'gamma': 1.3772927296772242, 'lambda_sparse': 2.497874440161843e-05, 'lr': 0.0028079706948511795, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 30 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 136 with best_epoch = 116 and best_val_auc = 0.69736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:14:40,445] Trial 24 finished with value: 0.6973558860697969 and parameters: {'n_d': 64, 'n_a': 30, 'n_steps': 5, 'gamma': 1.093844423187339, 'lambda_sparse': 0.00012061187296350777, 'lr': 0.0015794205169544634, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 24 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 45 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:16:32,008] Trial 25 finished with value: 0.5633992646344044 and parameters: {'n_d': 45, 'n_a': 24, 'n_steps': 5, 'gamma': 1.1726027008265874, 'lambda_sparse': 0.00015330830144466773, 'lr': 0.0014895582906838001, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 58 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_val_auc = 0.56771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:18:22,914] Trial 26 finished with value: 0.5677128185774164 and parameters: {'n_d': 58, 'n_a': 64, 'n_steps': 5, 'gamma': 1.4705229631328038, 'lambda_sparse': 0.0005535329808370042, 'lr': 0.0010042790056172131, 'mask_type': 'entmax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 38 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.56413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:20:28,277] Trial 27 finished with value: 0.5641346302300447 and parameters: {'n_d': 32, 'n_a': 38, 'n_steps': 4, 'gamma': 1.2096665019862973, 'lambda_sparse': 4.311694788554745e-05, 'lr': 0.001918604302579347, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 27 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 46 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.5665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:22:13,179] Trial 28 finished with value: 0.5665048404897101 and parameters: {'n_d': 46, 'n_a': 27, 'n_steps': 6, 'gamma': 1.5704415623434569, 'lambda_sparse': 8.080724414556316e-05, 'lr': 0.0008069327988811989, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 45 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 41 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_auc = 0.57556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:24:04,371] Trial 29 finished with value: 0.575559100864026 and parameters: {'n_d': 41, 'n_a': 45, 'n_steps': 3, 'gamma': 1.8619922408940606, 'lambda_sparse': 0.0002022971092479664, 'lr': 0.00043918667447453047, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 19 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 52 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.55698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:25:34,836] Trial 30 finished with value: 0.5569762521514876 and parameters: {'n_d': 52, 'n_a': 19, 'n_steps': 5, 'gamma': 1.315291928010348, 'lambda_sparse': 0.0003548972450958969, 'lr': 0.0014794101100865516, 'mask_type': 'sparsemax'}. Best is trial 19 with value: 0.6989738619273898.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 90 with best_epoch = 70 and best_val_auc = 0.71572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:32:37,448] Trial 31 finished with value: 0.7157154375310929 and parameters: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.0469848880092827, 'lambda_sparse': 0.00014926118239511857, 'lr': 0.0034650554044617, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 31 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 61 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 88 and best_val_auc = 0.69672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:40:07,139] Trial 32 finished with value: 0.6967188741930135 and parameters: {'n_d': 61, 'n_a': 31, 'n_steps': 5, 'gamma': 1.0785227481620048, 'lambda_sparse': 0.00013198745319180054, 'lr': 0.0032972605453969526, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 33 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 55 and best_val_auc = 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:45:14,298] Trial 33 finished with value: 0.6678011653772036 and parameters: {'n_d': 60, 'n_a': 33, 'n_steps': 4, 'gamma': 1.1061757105490653, 'lambda_sparse': 0.00014015863696785092, 'lr': 0.002266029839881153, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 33 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 56 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 116 with best_epoch = 96 and best_val_auc = 0.68453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:53:26,549] Trial 34 finished with value: 0.6845270158223686 and parameters: {'n_d': 56, 'n_a': 33, 'n_steps': 5, 'gamma': 1.0863715708512913, 'lambda_sparse': 0.00012049905040663968, 'lr': 0.0019010538837787342, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 24 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 25 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 88 with best_epoch = 68 and best_val_auc = 0.66694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 16:59:15,597] Trial 35 finished with value: 0.6669371393935235 and parameters: {'n_d': 25, 'n_a': 24, 'n_steps': 6, 'gamma': 1.1807099207784513, 'lambda_sparse': 0.0004239838328553843, 'lr': 0.003158654410155728, 'mask_type': 'entmax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 39 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 60 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.56081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:00:42,686] Trial 36 finished with value: 0.560810617627045 and parameters: {'n_d': 60, 'n_a': 39, 'n_steps': 4, 'gamma': 2.2757471876739346, 'lambda_sparse': 0.0002246164325857424, 'lr': 0.001762169917455482, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 43 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 49 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 146 with best_epoch = 126 and best_val_auc = 0.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:10:24,060] Trial 37 finished with value: 0.6915044116217499 and parameters: {'n_d': 49, 'n_a': 43, 'n_steps': 6, 'gamma': 1.3392617447480988, 'lambda_sparse': 5.67675780992679e-05, 'lr': 0.0026791977630796454, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 23 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 58 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_val_auc = 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:16:27,187] Trial 38 finished with value: 0.6470010693107806 and parameters: {'n_d': 58, 'n_a': 23, 'n_steps': 5, 'gamma': 1.613187633106881, 'lambda_sparse': 6.823510631412909e-05, 'lr': 0.003657746953536084, 'mask_type': 'entmax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 18 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 31 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:17:29,895] Trial 39 finished with value: 0.5640008234264835 and parameters: {'n_d': 31, 'n_a': 18, 'n_steps': 5, 'gamma': 1.4572422281575204, 'lambda_sparse': 0.00011326240398828456, 'lr': 0.0012735716576099986, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 31 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 54 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 85 and best_val_auc = 0.67746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:23:03,890] Trial 40 finished with value: 0.6774589859274127 and parameters: {'n_d': 54, 'n_a': 31, 'n_steps': 4, 'gamma': 1.2476717439567995, 'lambda_sparse': 0.00017898388729402037, 'lr': 0.00245824902525999, 'mask_type': 'entmax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 28 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 62 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_auc = 0.66181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:25:48,179] Trial 41 finished with value: 0.6618101658861271 and parameters: {'n_d': 62, 'n_a': 28, 'n_steps': 5, 'gamma': 1.0753577719450917, 'lambda_sparse': 3.292667998842974e-05, 'lr': 0.0039520326138579595, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 26 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 110 with best_epoch = 90 and best_val_auc = 0.69838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:32:22,134] Trial 42 finished with value: 0.6983808804945133 and parameters: {'n_d': 64, 'n_a': 26, 'n_steps': 5, 'gamma': 1.026460123728977, 'lambda_sparse': 0.0002908696486795256, 'lr': 0.003064334547572313, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 25 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 57 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 128 with best_epoch = 108 and best_val_auc = 0.68588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:40:09,089] Trial 43 finished with value: 0.6858822385763872 and parameters: {'n_d': 57, 'n_a': 25, 'n_steps': 5, 'gamma': 1.1747113463240682, 'lambda_sparse': 0.0002794777969203652, 'lr': 0.0029548514115480293, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 22 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 77 with best_epoch = 57 and best_val_auc = 0.6657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:43:42,895] Trial 44 finished with value: 0.6657014278443953 and parameters: {'n_d': 64, 'n_a': 22, 'n_steps': 5, 'gamma': 1.0133878965103365, 'lambda_sparse': 0.0008685544935126301, 'lr': 0.002130148029457512, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 20 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 61 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_auc = 0.57778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:44:55,620] Trial 45 finished with value: 0.577780636897512 and parameters: {'n_d': 61, 'n_a': 20, 'n_steps': 5, 'gamma': 1.0902178449135784, 'lambda_sparse': 0.0005093753699000485, 'lr': 0.00010666012506959906, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 16 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 6 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 102 with best_epoch = 82 and best_val_auc = 0.69306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:50:04,123] Trial 46 finished with value: 0.6930632037008445 and parameters: {'n_d': 64, 'n_a': 16, 'n_steps': 6, 'gamma': 1.284292232689549, 'lambda_sparse': 0.00028264600708784236, 'lr': 0.003416495339806995, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 31 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 59 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 4 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_auc = 0.56229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:51:21,923] Trial 47 finished with value: 0.5622922134733158 and parameters: {'n_d': 59, 'n_a': 31, 'n_steps': 4, 'gamma': 2.0302280675190953, 'lambda_sparse': 9.281271835711246e-05, 'lr': 0.0016223840145939829, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 29 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 62 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 120 with best_epoch = 100 and best_val_auc = 0.7042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 17:56:55,950] Trial 48 finished with value: 0.7042040496571916 and parameters: {'n_d': 62, 'n_a': 29, 'n_steps': 5, 'gamma': 1.2123156808321565, 'lambda_sparse': 0.0001735648393404781, 'lr': 0.0038589495345165556, 'mask_type': 'sparsemax'}. Best is trial 31 with value: 0.7157154375310929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 26 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 21 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 7 to 5\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 73 and best_val_auc = 0.69981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-29 18:01:07,699] Trial 49 finished with value: 0.6998050080341265 and parameters: {'n_d': 21, 'n_a': 26, 'n_steps': 7, 'gamma': 1.2127188790782348, 'lambda_sparse': 0.0006079857488742656, 'lr': 0.004107585810310206, 'mask_type': 'entmax'}. Best is trial 31 with value: 0.7157154375310929.\n",
      "\n",
      "Best trial: 31\n",
      "Best validation AUROC: 0.7157\n",
      "\n",
      "Best hyperparameters:\n",
      "  n_d: 64\n",
      "  n_a: 32\n",
      "  n_steps: 5\n",
      "  gamma: 1.0469848880092827\n",
      "  lambda_sparse: 0.00014926118239511857\n",
      "  lr: 0.0034650554044617\n",
      "  mask_type: sparsemax\n"
     ]
    }
   ],
   "source": [
    "print(\"hyperparameter optimization under way\")\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"tabnet_readmission_10pct\")\n",
    "\n",
    "objective = make_objective(X_train_scaled, y_train, X_val_scaled, y_val, class_weights_tensor, pretrainer)\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "print(f\"\\nBest trial: {study.best_trial.number}\")\n",
    "print(f\"Best validation AUROC: {study.best_value:.4f}\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "best_params = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a22b3e",
   "metadata": {},
   "source": [
    "Retraining using the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65e5f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 64 to 32\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.69476 | val_auc: 0.56769 |  0:00:02s\n",
      "epoch 1  | loss: 1.25609 | val_auc: 0.52337 |  0:00:04s\n",
      "epoch 2  | loss: 1.09138 | val_auc: 0.53692 |  0:00:06s\n",
      "epoch 3  | loss: 0.92045 | val_auc: 0.54232 |  0:00:08s\n",
      "epoch 4  | loss: 0.82316 | val_auc: 0.5397  |  0:00:10s\n",
      "epoch 5  | loss: 0.80664 | val_auc: 0.54415 |  0:00:12s\n",
      "epoch 6  | loss: 0.78022 | val_auc: 0.56022 |  0:00:14s\n",
      "epoch 7  | loss: 0.74605 | val_auc: 0.56819 |  0:00:16s\n",
      "epoch 8  | loss: 0.74568 | val_auc: 0.57904 |  0:00:19s\n",
      "epoch 9  | loss: 0.72927 | val_auc: 0.60036 |  0:00:21s\n",
      "epoch 10 | loss: 0.71823 | val_auc: 0.60178 |  0:00:23s\n",
      "epoch 11 | loss: 0.6999  | val_auc: 0.61139 |  0:00:26s\n",
      "epoch 12 | loss: 0.69202 | val_auc: 0.61279 |  0:00:28s\n",
      "epoch 13 | loss: 0.6824  | val_auc: 0.61792 |  0:00:31s\n",
      "epoch 14 | loss: 0.68281 | val_auc: 0.63204 |  0:00:34s\n",
      "epoch 15 | loss: 0.6707  | val_auc: 0.64659 |  0:00:37s\n",
      "epoch 16 | loss: 0.66472 | val_auc: 0.64739 |  0:00:39s\n",
      "epoch 17 | loss: 0.66352 | val_auc: 0.64934 |  0:00:42s\n",
      "epoch 18 | loss: 0.65578 | val_auc: 0.6431  |  0:00:44s\n",
      "epoch 19 | loss: 0.65537 | val_auc: 0.65123 |  0:00:47s\n",
      "epoch 20 | loss: 0.64797 | val_auc: 0.64676 |  0:00:51s\n",
      "epoch 21 | loss: 0.6464  | val_auc: 0.66038 |  0:00:53s\n",
      "epoch 22 | loss: 0.64271 | val_auc: 0.66242 |  0:00:56s\n",
      "epoch 23 | loss: 0.64058 | val_auc: 0.6534  |  0:00:58s\n",
      "epoch 24 | loss: 0.63156 | val_auc: 0.66098 |  0:01:00s\n",
      "epoch 25 | loss: 0.63429 | val_auc: 0.67016 |  0:01:03s\n",
      "epoch 26 | loss: 0.62806 | val_auc: 0.66501 |  0:01:05s\n",
      "epoch 27 | loss: 0.62703 | val_auc: 0.66504 |  0:01:08s\n",
      "epoch 28 | loss: 0.62056 | val_auc: 0.67206 |  0:01:10s\n",
      "epoch 29 | loss: 0.62081 | val_auc: 0.66981 |  0:01:13s\n",
      "epoch 30 | loss: 0.62161 | val_auc: 0.67042 |  0:01:15s\n",
      "epoch 31 | loss: 0.61782 | val_auc: 0.67372 |  0:01:19s\n",
      "epoch 32 | loss: 0.61309 | val_auc: 0.6772  |  0:01:22s\n",
      "epoch 33 | loss: 0.61107 | val_auc: 0.68595 |  0:01:25s\n",
      "epoch 34 | loss: 0.608   | val_auc: 0.68811 |  0:01:29s\n",
      "epoch 35 | loss: 0.60351 | val_auc: 0.68265 |  0:01:33s\n",
      "epoch 36 | loss: 0.5986  | val_auc: 0.68139 |  0:01:36s\n",
      "epoch 37 | loss: 0.60341 | val_auc: 0.68377 |  0:01:40s\n",
      "epoch 38 | loss: 0.59519 | val_auc: 0.68537 |  0:01:43s\n",
      "epoch 39 | loss: 0.59955 | val_auc: 0.67609 |  0:01:46s\n",
      "epoch 40 | loss: 0.59196 | val_auc: 0.68028 |  0:01:49s\n",
      "epoch 41 | loss: 0.59145 | val_auc: 0.67958 |  0:01:53s\n",
      "epoch 42 | loss: 0.5962  | val_auc: 0.67775 |  0:01:56s\n",
      "epoch 43 | loss: 0.58795 | val_auc: 0.67962 |  0:01:59s\n",
      "epoch 44 | loss: 0.59029 | val_auc: 0.67844 |  0:02:02s\n",
      "epoch 45 | loss: 0.59144 | val_auc: 0.68085 |  0:02:06s\n",
      "epoch 46 | loss: 0.5905  | val_auc: 0.6787  |  0:02:09s\n",
      "epoch 47 | loss: 0.58573 | val_auc: 0.66698 |  0:02:12s\n",
      "epoch 48 | loss: 0.58454 | val_auc: 0.6732  |  0:02:15s\n",
      "epoch 49 | loss: 0.57773 | val_auc: 0.67902 |  0:02:18s\n",
      "epoch 50 | loss: 0.5773  | val_auc: 0.67912 |  0:02:22s\n",
      "epoch 51 | loss: 0.57448 | val_auc: 0.68007 |  0:02:26s\n",
      "epoch 52 | loss: 0.57471 | val_auc: 0.67869 |  0:02:29s\n",
      "epoch 53 | loss: 0.57183 | val_auc: 0.67617 |  0:02:33s\n",
      "epoch 54 | loss: 0.56256 | val_auc: 0.67317 |  0:02:36s\n",
      "epoch 55 | loss: 0.56783 | val_auc: 0.67574 |  0:02:39s\n",
      "epoch 56 | loss: 0.55876 | val_auc: 0.67328 |  0:02:43s\n",
      "epoch 57 | loss: 0.56827 | val_auc: 0.66601 |  0:02:47s\n",
      "epoch 58 | loss: 0.56101 | val_auc: 0.65885 |  0:02:51s\n",
      "epoch 59 | loss: 0.55883 | val_auc: 0.66754 |  0:02:55s\n",
      "epoch 60 | loss: 0.55577 | val_auc: 0.66716 |  0:02:59s\n",
      "epoch 61 | loss: 0.55946 | val_auc: 0.66116 |  0:03:03s\n",
      "epoch 62 | loss: 0.55511 | val_auc: 0.67316 |  0:03:07s\n",
      "epoch 63 | loss: 0.55116 | val_auc: 0.68128 |  0:03:11s\n",
      "epoch 64 | loss: 0.55097 | val_auc: 0.67461 |  0:03:15s\n",
      "\n",
      "Early stopping occurred at epoch 64 with best_epoch = 34 and best_val_auc = 0.68811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_readmission_final.zip\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "final_model = TabNetClassifier(n_d=best_params[\"n_d\"], n_a=best_params[\"n_a\"], n_steps=best_params[\"n_steps\"], gamma=best_params[\"gamma\"], lambda_sparse=best_params[\"lambda_sparse\"], optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=best_params[\"lr\"]), mask_type=best_params.get(\"mask_type\", \"entmax\"), device_name=DEVICE, verbose=1)\n",
    "\n",
    "final_model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], eval_name=[\"val\"], eval_metric=[\"auc\"], max_epochs=MAX_FINETUNE_EPOCHS, patience=EARLY_STOPPING_PATIENCE, batch_size=BATCH_SIZE, virtual_batch_size=VIRTUAL_BATCH_SIZE, num_workers=0, drop_last=False, from_unsupervised=pretrainer, weights=1)\n",
    "final_model.save_model(\"tabnet_readmission_final\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0af95f",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cae2d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUROC: 0.6910\n",
      "95% CI: [0.6735, 0.7073]\n",
      "Classification Report (threshold=0.5)\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No Readmission       0.94      0.67      0.78      8164\n",
      "   Readmission       0.19      0.62      0.29       983\n",
      "\n",
      "      accuracy                           0.67      9147\n",
      "     macro avg       0.56      0.65      0.53      9147\n",
      "  weighted avg       0.86      0.67      0.73      9147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "y_test_pred_default = (y_test_proba >= 0.5).astype(int)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "auc_mean, auc_lower, auc_upper = bootstrap_auc_ci(y_test, y_test_proba, n_bootstraps=2000)\n",
    "\n",
    "print(f\"\\nAUROC: {auc_score:.4f}\")\n",
    "print(f\"95% CI: [{auc_lower:.4f}, {auc_upper:.4f}]\")\n",
    "\n",
    "# Classification report (default threshold)\n",
    "print(\"Classification Report (threshold=0.5)\")\n",
    "print(classification_report(y_test, y_test_pred_default, target_names=['No Readmission', 'Readmission']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66a1f5",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7635c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 most important features:\n",
      "            feature  importance\n",
      "                age    0.047833\n",
      " urea_nitrogen_mean    0.045689\n",
      "          mchc_mean    0.037300\n",
      "      phosphate_max    0.037054\n",
      "      icu_los_hours    0.035873\n",
      " calcium_total_mean    0.035227\n",
      "    hematocrit_mean    0.032977\n",
      "platelet_count_mean    0.032176\n",
      "      chloride_mean    0.030202\n",
      "      potassium_min    0.030158\n",
      "     potassium_mean    0.029994\n",
      "       glucose_mean    0.026153\n",
      "           mch_mean    0.025484\n",
      "           rdw_mean    0.024030\n",
      "   urea_nitrogen_sd    0.020903\n",
      "     bicarbonate_sd    0.020013\n",
      "            ptt_min    0.019935\n",
      "       magnesium_sd    0.019637\n",
      "  calcium_total_min    0.019096\n",
      "             mch_sd    0.018270\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance from TabNet's attention mechanism\n",
    "feature_importance = final_model.feature_importances_\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9a72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2416a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='svg'\n",
    "plt.rcParams.update({\n",
    "    'text.usetex':False,\n",
    "    'font.family':'monospace'\n",
    "})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77067961",
   "metadata": {},
   "source": [
    "Simple Bootstrapping method to get an confidence interval on the AUROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c1591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc_ci(y_true, y_scores, n_bootstraps=2000, ci=0.95):\n",
    "    rng = np.random.default_rng(42)\n",
    "    aucs = []\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = rng.integers(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        aucs.append(roc_auc_score(y_true[idx], y_scores[idx]))\n",
    "\n",
    "    lower = np.percentile(aucs, (1 - ci) / 2 * 100)\n",
    "    upper = np.percentile(aucs, (1 + ci) / 2 * 100)\n",
    "    return np.mean(aucs), lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997f81a",
   "metadata": {},
   "source": [
    "Loading the dataset, pre-processing, and analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8fefe0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>anion_gap_mean</th>\n",
       "      <th>anion_gap_sd</th>\n",
       "      <th>anion_gap_min</th>\n",
       "      <th>anion_gap_max</th>\n",
       "      <th>bicarbonate_mean</th>\n",
       "      <th>bicarbonate_sd</th>\n",
       "      <th>bicarbonate_min</th>\n",
       "      <th>bicarbonate_max</th>\n",
       "      <th>calcium_total_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>urea_nitrogen_min</th>\n",
       "      <th>urea_nitrogen_max</th>\n",
       "      <th>white_blood_cells_mean</th>\n",
       "      <th>white_blood_cells_sd</th>\n",
       "      <th>white_blood_cells_min</th>\n",
       "      <th>white_blood_cells_max</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>icu_los_hours</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200003</td>\n",
       "      <td>13.375000</td>\n",
       "      <td>3.583195</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>3.105295</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.771429</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.471429</td>\n",
       "      <td>13.176711</td>\n",
       "      <td>13.2</td>\n",
       "      <td>43.9</td>\n",
       "      <td>48</td>\n",
       "      <td>M</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200007</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>1.272792</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200009</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.471429</td>\n",
       "      <td>1.471637</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.3</td>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200014</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>2.203028</td>\n",
       "      <td>10.7</td>\n",
       "      <td>14.7</td>\n",
       "      <td>85</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30484</th>\n",
       "      <td>299992</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>2.856153</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.125000</td>\n",
       "      <td>2.609556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.307143</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.134783</td>\n",
       "      <td>3.781727</td>\n",
       "      <td>8.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>299993</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>1.341641</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>2.073644</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.605530</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>299994</td>\n",
       "      <td>16.157895</td>\n",
       "      <td>2.477973</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.631579</td>\n",
       "      <td>3.451417</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>10.076190</td>\n",
       "      <td>2.642329</td>\n",
       "      <td>5.3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>74</td>\n",
       "      <td>F</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>299998</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1.210372</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>87</td>\n",
       "      <td>M</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>299999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>3.394113</td>\n",
       "      <td>15.9</td>\n",
       "      <td>20.7</td>\n",
       "      <td>49</td>\n",
       "      <td>M</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30489 rows Ã— 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       icustay_id  anion_gap_mean  anion_gap_sd  anion_gap_min  anion_gap_max  \\\n",
       "0          200003       13.375000      3.583195            9.0           21.0   \n",
       "1          200007       15.500000      2.121320           14.0           17.0   \n",
       "2          200009        9.500000      2.121320            8.0           11.0   \n",
       "3          200012             NaN           NaN            NaN            NaN   \n",
       "4          200014       10.000000      1.732051            9.0           12.0   \n",
       "...           ...             ...           ...            ...            ...   \n",
       "30484      299992       15.375000      2.856153           11.0           25.0   \n",
       "30485      299993        9.400000      1.341641            8.0           11.0   \n",
       "30486      299994       16.157895      2.477973           13.0           24.0   \n",
       "30487      299998       11.500000      1.732051           10.0           14.0   \n",
       "30488      299999             NaN           NaN            NaN            NaN   \n",
       "\n",
       "       bicarbonate_mean  bicarbonate_sd  bicarbonate_min  bicarbonate_max  \\\n",
       "0             25.250000        3.105295             18.0             28.0   \n",
       "1             23.000000        1.414214             22.0             24.0   \n",
       "2             23.333333        2.081666             21.0             25.0   \n",
       "3                   NaN             NaN              NaN              NaN   \n",
       "4             24.000000        1.000000             23.0             25.0   \n",
       "...                 ...             ...              ...              ...   \n",
       "30484         23.125000        2.609556             15.0             26.0   \n",
       "30485         29.600000        2.073644             26.0             31.0   \n",
       "30486         21.631579        3.451417             17.0             31.0   \n",
       "30487         23.500000        1.290994             22.0             25.0   \n",
       "30488         24.000000             NaN             24.0             24.0   \n",
       "\n",
       "       calcium_total_mean  ...  urea_nitrogen_min  urea_nitrogen_max  \\\n",
       "0                7.771429  ...               10.0               21.0   \n",
       "1                8.900000  ...                8.0               10.0   \n",
       "2                8.000000  ...               15.0               21.0   \n",
       "3                     NaN  ...                NaN                NaN   \n",
       "4                7.733333  ...               21.0               24.0   \n",
       "...                   ...  ...                ...                ...   \n",
       "30484            8.307143  ...                8.0               23.0   \n",
       "30485            8.000000  ...               12.0               15.0   \n",
       "30486            8.100000  ...               28.0               63.0   \n",
       "30487            8.800000  ...               20.0               22.0   \n",
       "30488                 NaN  ...               11.0               13.0   \n",
       "\n",
       "       white_blood_cells_mean  white_blood_cells_sd  white_blood_cells_min  \\\n",
       "0                   26.471429             13.176711                   13.2   \n",
       "1                   10.300000              1.272792                    9.4   \n",
       "2                   12.471429              1.471637                   10.5   \n",
       "3                    4.900000                   NaN                    4.9   \n",
       "4                   13.233333              2.203028                   10.7   \n",
       "...                       ...                   ...                    ...   \n",
       "30484               14.134783              3.781727                    8.1   \n",
       "30485               12.600000              0.605530                   12.0   \n",
       "30486               10.076190              2.642329                    5.3   \n",
       "30487                9.900000              1.210372                    7.9   \n",
       "30488               18.300000              3.394113                   15.9   \n",
       "\n",
       "       white_blood_cells_max  age  gender  icu_los_hours  target  \n",
       "0                       43.9   48       M            141       0  \n",
       "1                       11.2   44       M             30       0  \n",
       "2                       14.3   47       F             51       0  \n",
       "3                        4.9   33       F             10       0  \n",
       "4                       14.7   85       M             41       0  \n",
       "...                      ...  ...     ...            ...     ...  \n",
       "30484                   22.1   41       M            499       0  \n",
       "30485                   13.3   26       M             67       0  \n",
       "30486                   14.5   74       F            152       1  \n",
       "30487                   11.0   87       M             46       1  \n",
       "30488                   20.7   49       M             31       0  \n",
       "\n",
       "[30489 rows x 93 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort_data = pd.read_csv('../cohort_data_new.csv')\n",
    "cohort_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e877cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_cols = [\n",
    "    'anion_gap_mean', 'anion_gap_min', 'anion_gap_max', 'anion_gap_sd',\n",
    "    'bicarbonate_mean', 'bicarbonate_min', 'bicarbonate_max', 'bicarbonate_sd',\n",
    "    'calcium_total_mean', 'calcium_total_min', 'calcium_total_max', 'calcium_total_sd',\n",
    "    'chloride_mean', 'chloride_min', 'chloride_max', 'chloride_sd',\n",
    "    'creatinine_mean', 'creatinine_min', 'creatinine_max', 'creatinine_sd',\n",
    "    'glucose_mean', 'glucose_min', 'glucose_max', 'glucose_sd',\n",
    "    'hematocrit_mean', 'hematocrit_min', 'hematocrit_max', 'hematocrit_sd',\n",
    "    'hemoglobin_mean', 'hemoglobin_min', 'hemoglobin_max', 'hemoglobin_sd',\n",
    "    'mchc_mean', 'mchc_min', 'mchc_max', 'mchc_sd',\n",
    "    'mcv_mean', 'mcv_min', 'mcv_max', 'mcv_sd',\n",
    "    'magnesium_mean', 'magnesium_min', 'magnesium_max', 'magnesium_sd',\n",
    "    'pt_mean', 'pt_min', 'pt_max', 'pt_sd',\n",
    "    'phosphate_mean', 'phosphate_min', 'phosphate_max', 'phosphate_sd',\n",
    "    'platelet_count_mean', 'platelet_count_min', 'platelet_count_max', 'platelet_count_sd',\n",
    "    'potassium_mean', 'potassium_min', 'potassium_max', 'potassium_sd',\n",
    "    'rdw_mean', 'rdw_min', 'rdw_max', 'rdw_sd',\n",
    "    'red_blood_cells_mean', 'red_blood_cells_min', 'red_blood_cells_max', 'red_blood_cells_sd',\n",
    "    'sodium_mean', 'sodium_min', 'sodium_max', 'sodium_sd',\n",
    "    'urea_nitrogen_mean', 'urea_nitrogen_min', 'urea_nitrogen_max', 'urea_nitrogen_sd',\n",
    "    'white_blood_cells_mean', 'white_blood_cells_min', 'white_blood_cells_max', 'white_blood_cells_sd',\n",
    "    'age', 'icu_los_hours'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c00ad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix shape: (30489, 90)\n"
     ]
    }
   ],
   "source": [
    "# REmove the ICUstay_id and the gender\n",
    "drop_cols = [c for c in cohort_data.columns if 'icustay_id' in c.lower() or 'gender' in c.lower()]\n",
    "df = cohort_data.drop(columns=['icustay_id', 'gender'], errors='ignore')\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[lab_cols] = imputer.fit_transform(df[lab_cols])\n",
    "\n",
    "# Keep only numeric\n",
    "X = X.select_dtypes(include=['number']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cfa74",
   "metadata": {},
   "source": [
    "Simple Imputation to handle the missing/ NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09c1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.30, random_state=7, stratify=y, shuffle=True)\n",
    "\n",
    "# train-dev\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.30, random_state=7, stratify=y_train_full, shuffle=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_val = imputer.transform(X_val)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e56c2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% readmission in train: 10.74369101010777\n",
      "% readmission in train: 10.74496329845385\n",
      "% readmission in train: 10.746692904777523\n"
     ]
    }
   ],
   "source": [
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f'% readmission in train: {np.mean(y_train)*100}')\n",
    "print(f'% readmission in train: {np.mean(y_val)*100}')\n",
    "print(f'% readmission in train: {np.mean(y_test)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d4051",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2e25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 16, 64),\n",
    "        \"n_a\": trial.suggest_int(\"n_a\", 16, 64),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 7),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.5),\n",
    "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True),\n",
    "        \"optimizer_fn\": torch.optim.Adam,\n",
    "        \"optimizer_params\": dict(lr=trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)),\n",
    "        \"mask_type\": \"sparsemax\",\n",
    "    }\n",
    "\n",
    "    model = TabNetClassifier(**params)\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "        eval_metric=[\"auc\"],\n",
    "        max_epochs=150,\n",
    "        patience=15,\n",
    "        batch_size=2048,\n",
    "        virtual_batch_size=256,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # Predict on validation test split\n",
    "    val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, val_pred_proba)\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc30c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 22:34:46,585] A new study created in memory with name: no-name-58071c95-7920-42c9-8741-cfe276f72f9a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af982e304484c0085c0dec8c49dbcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.98465 | val_0_auc: 0.49295 |  0:00:05s\n",
      "epoch 1  | loss: 0.81801 | val_0_auc: 0.46961 |  0:00:11s\n",
      "epoch 2  | loss: 0.74955 | val_0_auc: 0.47303 |  0:00:16s\n",
      "epoch 3  | loss: 0.66614 | val_0_auc: 0.48388 |  0:00:22s\n",
      "epoch 4  | loss: 0.65724 | val_0_auc: 0.47805 |  0:00:27s\n",
      "epoch 5  | loss: 0.64337 | val_0_auc: 0.48259 |  0:00:32s\n",
      "epoch 6  | loss: 0.64311 | val_0_auc: 0.47977 |  0:00:37s\n",
      "epoch 7  | loss: 0.6204  | val_0_auc: 0.48112 |  0:00:42s\n",
      "epoch 8  | loss: 0.62722 | val_0_auc: 0.47725 |  0:00:47s\n",
      "epoch 9  | loss: 0.59478 | val_0_auc: 0.49146 |  0:00:53s\n",
      "epoch 10 | loss: 0.59118 | val_0_auc: 0.50596 |  0:01:00s\n",
      "epoch 11 | loss: 0.57635 | val_0_auc: 0.4939  |  0:01:05s\n",
      "epoch 12 | loss: 0.56144 | val_0_auc: 0.48674 |  0:01:09s\n",
      "epoch 13 | loss: 0.55783 | val_0_auc: 0.49309 |  0:01:14s\n",
      "epoch 14 | loss: 0.54965 | val_0_auc: 0.49645 |  0:01:18s\n",
      "epoch 15 | loss: 0.54661 | val_0_auc: 0.48931 |  0:01:25s\n",
      "epoch 16 | loss: 0.52871 | val_0_auc: 0.5013  |  0:01:30s\n",
      "epoch 17 | loss: 0.53967 | val_0_auc: 0.5111  |  0:01:35s\n",
      "epoch 18 | loss: 0.53321 | val_0_auc: 0.49194 |  0:01:40s\n",
      "epoch 19 | loss: 0.50546 | val_0_auc: 0.50512 |  0:01:44s\n",
      "epoch 20 | loss: 0.51007 | val_0_auc: 0.5018  |  0:01:48s\n",
      "epoch 21 | loss: 0.503   | val_0_auc: 0.49282 |  0:01:52s\n",
      "epoch 22 | loss: 0.49919 | val_0_auc: 0.49384 |  0:01:57s\n",
      "epoch 23 | loss: 0.49293 | val_0_auc: 0.48831 |  0:02:02s\n",
      "epoch 24 | loss: 0.48001 | val_0_auc: 0.4911  |  0:02:06s\n",
      "epoch 25 | loss: 0.48475 | val_0_auc: 0.50219 |  0:02:10s\n",
      "epoch 26 | loss: 0.47222 | val_0_auc: 0.49716 |  0:02:15s\n",
      "epoch 27 | loss: 0.47562 | val_0_auc: 0.49333 |  0:02:19s\n",
      "epoch 28 | loss: 0.45962 | val_0_auc: 0.49977 |  0:02:23s\n",
      "epoch 29 | loss: 0.45845 | val_0_auc: 0.49627 |  0:02:28s\n",
      "epoch 30 | loss: 0.46056 | val_0_auc: 0.50138 |  0:02:33s\n",
      "epoch 31 | loss: 0.45028 | val_0_auc: 0.50097 |  0:02:41s\n",
      "epoch 32 | loss: 0.44311 | val_0_auc: 0.49927 |  0:02:48s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 17 and best_val_0_auc = 0.5111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 22:37:39,947] Trial 0 finished with value: 0.5111004293068018 and parameters: {'n_d': 50, 'n_a': 41, 'n_steps': 7, 'gamma': 1.346691658043647, 'lambda_sparse': 0.0006513256153340275, 'lr': 0.0005793796984794536}. Best is trial 0 with value: 0.5111004293068018.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.62987 | val_0_auc: 0.47022 |  0:00:04s\n",
      "epoch 1  | loss: 0.5917  | val_0_auc: 0.47682 |  0:00:10s\n",
      "epoch 2  | loss: 0.56648 | val_0_auc: 0.48718 |  0:00:15s\n",
      "epoch 3  | loss: 0.52733 | val_0_auc: 0.48413 |  0:00:20s\n",
      "epoch 4  | loss: 0.51411 | val_0_auc: 0.47968 |  0:00:25s\n",
      "epoch 5  | loss: 0.5005  | val_0_auc: 0.47625 |  0:00:29s\n",
      "epoch 6  | loss: 0.47738 | val_0_auc: 0.49737 |  0:00:35s\n",
      "epoch 7  | loss: 0.45378 | val_0_auc: 0.49804 |  0:00:41s\n",
      "epoch 8  | loss: 0.44995 | val_0_auc: 0.5071  |  0:00:46s\n",
      "epoch 9  | loss: 0.43644 | val_0_auc: 0.51513 |  0:00:50s\n",
      "epoch 10 | loss: 0.44005 | val_0_auc: 0.51482 |  0:00:56s\n",
      "epoch 11 | loss: 0.41807 | val_0_auc: 0.52884 |  0:01:00s\n",
      "epoch 12 | loss: 0.42002 | val_0_auc: 0.51012 |  0:01:04s\n",
      "epoch 13 | loss: 0.40538 | val_0_auc: 0.51167 |  0:01:09s\n",
      "epoch 14 | loss: 0.40834 | val_0_auc: 0.5139  |  0:01:13s\n",
      "epoch 15 | loss: 0.39968 | val_0_auc: 0.50589 |  0:01:17s\n",
      "epoch 16 | loss: 0.4007  | val_0_auc: 0.51492 |  0:01:21s\n",
      "epoch 17 | loss: 0.3891  | val_0_auc: 0.51636 |  0:01:25s\n",
      "epoch 18 | loss: 0.39554 | val_0_auc: 0.52145 |  0:01:29s\n",
      "epoch 19 | loss: 0.39034 | val_0_auc: 0.53313 |  0:01:33s\n",
      "epoch 20 | loss: 0.38771 | val_0_auc: 0.53997 |  0:01:38s\n",
      "epoch 21 | loss: 0.37908 | val_0_auc: 0.52414 |  0:01:42s\n",
      "epoch 22 | loss: 0.37998 | val_0_auc: 0.51331 |  0:01:48s\n",
      "epoch 23 | loss: 0.37873 | val_0_auc: 0.53187 |  0:01:53s\n",
      "epoch 24 | loss: 0.37429 | val_0_auc: 0.51322 |  0:01:58s\n",
      "epoch 25 | loss: 0.36596 | val_0_auc: 0.51956 |  0:02:01s\n",
      "epoch 26 | loss: 0.37495 | val_0_auc: 0.52806 |  0:02:05s\n",
      "epoch 27 | loss: 0.3702  | val_0_auc: 0.51276 |  0:02:09s\n",
      "epoch 28 | loss: 0.36892 | val_0_auc: 0.53411 |  0:02:14s\n",
      "epoch 29 | loss: 0.36628 | val_0_auc: 0.53296 |  0:02:19s\n",
      "epoch 30 | loss: 0.3639  | val_0_auc: 0.52778 |  0:02:24s\n",
      "epoch 31 | loss: 0.35776 | val_0_auc: 0.53366 |  0:02:30s\n",
      "epoch 32 | loss: 0.3585  | val_0_auc: 0.54029 |  0:02:36s\n",
      "epoch 33 | loss: 0.36073 | val_0_auc: 0.53938 |  0:02:41s\n",
      "epoch 34 | loss: 0.35936 | val_0_auc: 0.5502  |  0:02:46s\n",
      "epoch 35 | loss: 0.35568 | val_0_auc: 0.56794 |  0:02:49s\n",
      "epoch 36 | loss: 0.35239 | val_0_auc: 0.55929 |  0:02:54s\n",
      "epoch 37 | loss: 0.35867 | val_0_auc: 0.54984 |  0:02:57s\n",
      "epoch 38 | loss: 0.3532  | val_0_auc: 0.5455  |  0:03:02s\n",
      "epoch 39 | loss: 0.35601 | val_0_auc: 0.54516 |  0:03:06s\n",
      "epoch 40 | loss: 0.35321 | val_0_auc: 0.54709 |  0:03:10s\n",
      "epoch 41 | loss: 0.35289 | val_0_auc: 0.54238 |  0:03:14s\n",
      "epoch 42 | loss: 0.35599 | val_0_auc: 0.54988 |  0:03:18s\n",
      "epoch 43 | loss: 0.34816 | val_0_auc: 0.54937 |  0:03:22s\n",
      "epoch 44 | loss: 0.34639 | val_0_auc: 0.56635 |  0:03:26s\n",
      "epoch 45 | loss: 0.34889 | val_0_auc: 0.56601 |  0:03:29s\n",
      "epoch 46 | loss: 0.34637 | val_0_auc: 0.56757 |  0:03:34s\n",
      "epoch 47 | loss: 0.34796 | val_0_auc: 0.55678 |  0:03:39s\n",
      "epoch 48 | loss: 0.35127 | val_0_auc: 0.56161 |  0:03:42s\n",
      "epoch 49 | loss: 0.34599 | val_0_auc: 0.5669  |  0:03:46s\n",
      "epoch 50 | loss: 0.34678 | val_0_auc: 0.57094 |  0:03:51s\n",
      "epoch 51 | loss: 0.34457 | val_0_auc: 0.57231 |  0:03:54s\n",
      "epoch 52 | loss: 0.34301 | val_0_auc: 0.5725  |  0:03:58s\n",
      "epoch 53 | loss: 0.34776 | val_0_auc: 0.56711 |  0:04:01s\n",
      "epoch 54 | loss: 0.34555 | val_0_auc: 0.58148 |  0:04:05s\n",
      "epoch 55 | loss: 0.34484 | val_0_auc: 0.57431 |  0:04:10s\n",
      "epoch 56 | loss: 0.345   | val_0_auc: 0.57333 |  0:04:17s\n",
      "epoch 57 | loss: 0.34418 | val_0_auc: 0.58276 |  0:04:22s\n",
      "epoch 58 | loss: 0.34352 | val_0_auc: 0.57174 |  0:04:26s\n",
      "epoch 59 | loss: 0.33763 | val_0_auc: 0.5862  |  0:04:30s\n",
      "epoch 60 | loss: 0.34365 | val_0_auc: 0.57273 |  0:04:35s\n",
      "epoch 61 | loss: 0.34257 | val_0_auc: 0.58438 |  0:04:39s\n",
      "epoch 62 | loss: 0.3417  | val_0_auc: 0.57447 |  0:04:43s\n",
      "epoch 63 | loss: 0.33905 | val_0_auc: 0.57936 |  0:04:47s\n",
      "epoch 64 | loss: 0.33912 | val_0_auc: 0.57508 |  0:04:51s\n",
      "epoch 65 | loss: 0.34224 | val_0_auc: 0.57214 |  0:04:55s\n",
      "epoch 66 | loss: 0.34272 | val_0_auc: 0.58897 |  0:04:58s\n",
      "epoch 67 | loss: 0.34515 | val_0_auc: 0.58707 |  0:05:02s\n",
      "epoch 68 | loss: 0.34079 | val_0_auc: 0.59441 |  0:05:06s\n",
      "epoch 69 | loss: 0.34304 | val_0_auc: 0.58748 |  0:05:10s\n",
      "epoch 70 | loss: 0.34219 | val_0_auc: 0.59291 |  0:05:14s\n",
      "epoch 71 | loss: 0.34128 | val_0_auc: 0.59117 |  0:05:19s\n",
      "epoch 72 | loss: 0.34391 | val_0_auc: 0.58788 |  0:05:24s\n",
      "epoch 73 | loss: 0.34121 | val_0_auc: 0.58182 |  0:05:28s\n",
      "epoch 74 | loss: 0.34068 | val_0_auc: 0.58375 |  0:05:34s\n",
      "epoch 75 | loss: 0.33984 | val_0_auc: 0.5885  |  0:05:39s\n",
      "epoch 76 | loss: 0.33939 | val_0_auc: 0.59253 |  0:05:43s\n",
      "epoch 77 | loss: 0.34231 | val_0_auc: 0.58878 |  0:05:47s\n",
      "epoch 78 | loss: 0.33544 | val_0_auc: 0.60109 |  0:05:51s\n",
      "epoch 79 | loss: 0.3383  | val_0_auc: 0.60128 |  0:05:55s\n",
      "epoch 80 | loss: 0.3383  | val_0_auc: 0.58798 |  0:05:59s\n",
      "epoch 81 | loss: 0.33923 | val_0_auc: 0.60538 |  0:06:03s\n",
      "epoch 82 | loss: 0.33595 | val_0_auc: 0.59873 |  0:06:08s\n",
      "epoch 83 | loss: 0.33743 | val_0_auc: 0.58842 |  0:06:13s\n",
      "epoch 84 | loss: 0.33897 | val_0_auc: 0.5911  |  0:06:17s\n",
      "epoch 85 | loss: 0.33751 | val_0_auc: 0.589   |  0:06:21s\n",
      "epoch 86 | loss: 0.33741 | val_0_auc: 0.59354 |  0:06:25s\n",
      "epoch 87 | loss: 0.33493 | val_0_auc: 0.59233 |  0:06:30s\n",
      "epoch 88 | loss: 0.3382  | val_0_auc: 0.58487 |  0:06:34s\n",
      "epoch 89 | loss: 0.33677 | val_0_auc: 0.58253 |  0:06:39s\n",
      "epoch 90 | loss: 0.337   | val_0_auc: 0.59687 |  0:06:43s\n",
      "epoch 91 | loss: 0.33775 | val_0_auc: 0.59129 |  0:06:48s\n",
      "epoch 92 | loss: 0.33767 | val_0_auc: 0.58585 |  0:06:52s\n",
      "epoch 93 | loss: 0.33764 | val_0_auc: 0.59271 |  0:06:56s\n",
      "epoch 94 | loss: 0.33384 | val_0_auc: 0.58577 |  0:07:00s\n",
      "epoch 95 | loss: 0.33707 | val_0_auc: 0.58918 |  0:07:04s\n",
      "epoch 96 | loss: 0.33575 | val_0_auc: 0.58706 |  0:07:08s\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 81 and best_val_0_auc = 0.60538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 22:44:50,762] Trial 1 finished with value: 0.6053800687704736 and parameters: {'n_d': 43, 'n_a': 39, 'n_steps': 6, 'gamma': 2.215901821067024, 'lambda_sparse': 0.0004200228677581217, 'lr': 0.0011224091389713624}. Best is trial 1 with value: 0.6053800687704736.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67479 | val_0_auc: 0.48438 |  0:00:02s\n",
      "epoch 1  | loss: 0.63068 | val_0_auc: 0.50725 |  0:00:06s\n",
      "epoch 2  | loss: 0.58608 | val_0_auc: 0.50027 |  0:00:09s\n",
      "epoch 3  | loss: 0.55782 | val_0_auc: 0.50282 |  0:00:12s\n",
      "epoch 4  | loss: 0.55906 | val_0_auc: 0.50144 |  0:00:15s\n",
      "epoch 5  | loss: 0.52902 | val_0_auc: 0.50091 |  0:00:19s\n",
      "epoch 6  | loss: 0.52663 | val_0_auc: 0.50218 |  0:00:23s\n",
      "epoch 7  | loss: 0.51995 | val_0_auc: 0.50373 |  0:00:27s\n",
      "epoch 8  | loss: 0.51573 | val_0_auc: 0.5036  |  0:00:30s\n",
      "epoch 9  | loss: 0.51351 | val_0_auc: 0.50468 |  0:00:32s\n",
      "epoch 10 | loss: 0.50564 | val_0_auc: 0.50592 |  0:00:35s\n",
      "epoch 11 | loss: 0.49106 | val_0_auc: 0.5019  |  0:00:38s\n",
      "epoch 12 | loss: 0.4746  | val_0_auc: 0.50347 |  0:00:42s\n",
      "epoch 13 | loss: 0.48085 | val_0_auc: 0.50688 |  0:00:44s\n",
      "epoch 14 | loss: 0.47807 | val_0_auc: 0.51453 |  0:00:48s\n",
      "epoch 15 | loss: 0.46447 | val_0_auc: 0.51829 |  0:00:52s\n",
      "epoch 16 | loss: 0.47052 | val_0_auc: 0.5067  |  0:00:55s\n",
      "epoch 17 | loss: 0.45799 | val_0_auc: 0.49779 |  0:00:58s\n",
      "epoch 18 | loss: 0.45657 | val_0_auc: 0.49637 |  0:01:01s\n",
      "epoch 19 | loss: 0.46152 | val_0_auc: 0.50002 |  0:01:04s\n",
      "epoch 20 | loss: 0.45294 | val_0_auc: 0.50855 |  0:01:07s\n",
      "epoch 21 | loss: 0.44838 | val_0_auc: 0.51043 |  0:01:09s\n",
      "epoch 22 | loss: 0.4456  | val_0_auc: 0.51723 |  0:01:12s\n",
      "epoch 23 | loss: 0.44041 | val_0_auc: 0.522   |  0:01:14s\n",
      "epoch 24 | loss: 0.43667 | val_0_auc: 0.51258 |  0:01:17s\n",
      "epoch 25 | loss: 0.4331  | val_0_auc: 0.51677 |  0:01:20s\n",
      "epoch 26 | loss: 0.42803 | val_0_auc: 0.50761 |  0:01:22s\n",
      "epoch 27 | loss: 0.42361 | val_0_auc: 0.51091 |  0:01:25s\n",
      "epoch 28 | loss: 0.42374 | val_0_auc: 0.51425 |  0:01:27s\n",
      "epoch 29 | loss: 0.41331 | val_0_auc: 0.50874 |  0:01:30s\n",
      "epoch 30 | loss: 0.42153 | val_0_auc: 0.51169 |  0:01:32s\n",
      "epoch 31 | loss: 0.42699 | val_0_auc: 0.51663 |  0:01:35s\n",
      "epoch 32 | loss: 0.41449 | val_0_auc: 0.50749 |  0:01:37s\n",
      "epoch 33 | loss: 0.41208 | val_0_auc: 0.51877 |  0:01:40s\n",
      "epoch 34 | loss: 0.41226 | val_0_auc: 0.51458 |  0:01:42s\n",
      "epoch 35 | loss: 0.41419 | val_0_auc: 0.5211  |  0:01:45s\n",
      "epoch 36 | loss: 0.40639 | val_0_auc: 0.52295 |  0:01:47s\n",
      "epoch 37 | loss: 0.39781 | val_0_auc: 0.53057 |  0:01:50s\n",
      "epoch 38 | loss: 0.40193 | val_0_auc: 0.52227 |  0:01:52s\n",
      "epoch 39 | loss: 0.3997  | val_0_auc: 0.52331 |  0:01:55s\n",
      "epoch 40 | loss: 0.40189 | val_0_auc: 0.5323  |  0:01:58s\n",
      "epoch 41 | loss: 0.39933 | val_0_auc: 0.53579 |  0:02:01s\n",
      "epoch 42 | loss: 0.39175 | val_0_auc: 0.54225 |  0:02:04s\n",
      "epoch 43 | loss: 0.3912  | val_0_auc: 0.541   |  0:02:07s\n",
      "epoch 44 | loss: 0.40092 | val_0_auc: 0.53122 |  0:02:09s\n",
      "epoch 45 | loss: 0.38748 | val_0_auc: 0.54367 |  0:02:12s\n",
      "epoch 46 | loss: 0.3899  | val_0_auc: 0.53831 |  0:02:15s\n",
      "epoch 47 | loss: 0.38704 | val_0_auc: 0.53594 |  0:02:18s\n",
      "epoch 48 | loss: 0.38479 | val_0_auc: 0.54113 |  0:02:21s\n",
      "epoch 49 | loss: 0.38777 | val_0_auc: 0.54544 |  0:02:25s\n",
      "epoch 50 | loss: 0.38329 | val_0_auc: 0.54569 |  0:02:29s\n",
      "epoch 51 | loss: 0.38101 | val_0_auc: 0.55942 |  0:02:32s\n",
      "epoch 52 | loss: 0.38187 | val_0_auc: 0.55978 |  0:02:37s\n",
      "epoch 53 | loss: 0.3809  | val_0_auc: 0.55939 |  0:02:40s\n",
      "epoch 54 | loss: 0.38287 | val_0_auc: 0.55889 |  0:02:43s\n",
      "epoch 55 | loss: 0.37835 | val_0_auc: 0.55808 |  0:02:47s\n",
      "epoch 56 | loss: 0.381   | val_0_auc: 0.55413 |  0:02:50s\n",
      "epoch 57 | loss: 0.37216 | val_0_auc: 0.55362 |  0:02:53s\n",
      "epoch 58 | loss: 0.37782 | val_0_auc: 0.5558  |  0:02:56s\n",
      "epoch 59 | loss: 0.38122 | val_0_auc: 0.55928 |  0:02:59s\n",
      "epoch 60 | loss: 0.37753 | val_0_auc: 0.55224 |  0:03:02s\n",
      "epoch 61 | loss: 0.37342 | val_0_auc: 0.5519  |  0:03:05s\n",
      "epoch 62 | loss: 0.37741 | val_0_auc: 0.54697 |  0:03:08s\n",
      "epoch 63 | loss: 0.3725  | val_0_auc: 0.5553  |  0:03:11s\n",
      "epoch 64 | loss: 0.36544 | val_0_auc: 0.55862 |  0:03:14s\n",
      "epoch 65 | loss: 0.37407 | val_0_auc: 0.56381 |  0:03:18s\n",
      "epoch 66 | loss: 0.37125 | val_0_auc: 0.55436 |  0:03:22s\n",
      "epoch 67 | loss: 0.36991 | val_0_auc: 0.55957 |  0:03:25s\n",
      "epoch 68 | loss: 0.36917 | val_0_auc: 0.55597 |  0:03:27s\n",
      "epoch 69 | loss: 0.36365 | val_0_auc: 0.56574 |  0:03:30s\n",
      "epoch 70 | loss: 0.36381 | val_0_auc: 0.56383 |  0:03:32s\n",
      "epoch 71 | loss: 0.36822 | val_0_auc: 0.56273 |  0:03:35s\n",
      "epoch 72 | loss: 0.36619 | val_0_auc: 0.56658 |  0:03:38s\n",
      "epoch 73 | loss: 0.36626 | val_0_auc: 0.56543 |  0:03:42s\n",
      "epoch 74 | loss: 0.36564 | val_0_auc: 0.56128 |  0:03:45s\n",
      "epoch 75 | loss: 0.36339 | val_0_auc: 0.55875 |  0:03:49s\n",
      "epoch 76 | loss: 0.35994 | val_0_auc: 0.56744 |  0:03:53s\n",
      "epoch 77 | loss: 0.3608  | val_0_auc: 0.55844 |  0:03:57s\n",
      "epoch 78 | loss: 0.36043 | val_0_auc: 0.55681 |  0:04:01s\n",
      "epoch 79 | loss: 0.36209 | val_0_auc: 0.56754 |  0:04:04s\n",
      "epoch 80 | loss: 0.35948 | val_0_auc: 0.56461 |  0:04:06s\n",
      "epoch 81 | loss: 0.36174 | val_0_auc: 0.56428 |  0:04:10s\n",
      "epoch 82 | loss: 0.36156 | val_0_auc: 0.56391 |  0:04:13s\n",
      "epoch 83 | loss: 0.36282 | val_0_auc: 0.56663 |  0:04:17s\n",
      "epoch 84 | loss: 0.36038 | val_0_auc: 0.57184 |  0:04:20s\n",
      "epoch 85 | loss: 0.36257 | val_0_auc: 0.57285 |  0:04:23s\n",
      "epoch 86 | loss: 0.35973 | val_0_auc: 0.57116 |  0:04:27s\n",
      "epoch 87 | loss: 0.35641 | val_0_auc: 0.57303 |  0:04:31s\n",
      "epoch 88 | loss: 0.35529 | val_0_auc: 0.57307 |  0:04:35s\n",
      "epoch 89 | loss: 0.35577 | val_0_auc: 0.57762 |  0:04:39s\n",
      "epoch 90 | loss: 0.3571  | val_0_auc: 0.57786 |  0:04:42s\n",
      "epoch 91 | loss: 0.35821 | val_0_auc: 0.57829 |  0:04:46s\n",
      "epoch 92 | loss: 0.35452 | val_0_auc: 0.568   |  0:04:51s\n",
      "epoch 93 | loss: 0.35361 | val_0_auc: 0.56828 |  0:04:55s\n",
      "epoch 94 | loss: 0.3579  | val_0_auc: 0.57087 |  0:05:01s\n",
      "epoch 95 | loss: 0.35633 | val_0_auc: 0.56289 |  0:05:06s\n",
      "epoch 96 | loss: 0.35182 | val_0_auc: 0.56803 |  0:05:11s\n",
      "epoch 97 | loss: 0.3476  | val_0_auc: 0.57591 |  0:05:16s\n",
      "epoch 98 | loss: 0.35007 | val_0_auc: 0.57493 |  0:05:19s\n",
      "epoch 99 | loss: 0.35103 | val_0_auc: 0.57418 |  0:05:22s\n",
      "epoch 100| loss: 0.35026 | val_0_auc: 0.57815 |  0:05:25s\n",
      "epoch 101| loss: 0.34989 | val_0_auc: 0.57043 |  0:05:29s\n",
      "epoch 102| loss: 0.34835 | val_0_auc: 0.57181 |  0:05:32s\n",
      "epoch 103| loss: 0.34967 | val_0_auc: 0.56333 |  0:05:35s\n",
      "epoch 104| loss: 0.3483  | val_0_auc: 0.57065 |  0:05:38s\n",
      "epoch 105| loss: 0.34779 | val_0_auc: 0.56524 |  0:05:42s\n",
      "epoch 106| loss: 0.34903 | val_0_auc: 0.56489 |  0:05:46s\n",
      "\n",
      "Early stopping occurred at epoch 106 with best_epoch = 91 and best_val_0_auc = 0.57829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 22:50:38,538] Trial 2 finished with value: 0.5782897413986041 and parameters: {'n_d': 27, 'n_a': 57, 'n_steps': 4, 'gamma': 1.4275180412319197, 'lambda_sparse': 1.9952060717591973e-05, 'lr': 0.0004029511532558595}. Best is trial 1 with value: 0.6053800687704736.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.51386 | val_0_auc: 0.52293 |  0:00:02s\n",
      "epoch 1  | loss: 1.1891  | val_0_auc: 0.5109  |  0:00:06s\n",
      "epoch 2  | loss: 0.9459  | val_0_auc: 0.50263 |  0:00:10s\n",
      "epoch 3  | loss: 0.766   | val_0_auc: 0.49658 |  0:00:13s\n",
      "epoch 4  | loss: 0.63106 | val_0_auc: 0.48626 |  0:00:16s\n",
      "epoch 5  | loss: 0.55119 | val_0_auc: 0.48538 |  0:00:20s\n",
      "epoch 6  | loss: 0.48792 | val_0_auc: 0.48897 |  0:00:23s\n",
      "epoch 7  | loss: 0.46645 | val_0_auc: 0.48132 |  0:00:27s\n",
      "epoch 8  | loss: 0.44765 | val_0_auc: 0.47733 |  0:00:29s\n",
      "epoch 9  | loss: 0.42949 | val_0_auc: 0.48115 |  0:00:32s\n",
      "epoch 10 | loss: 0.42351 | val_0_auc: 0.48617 |  0:00:34s\n",
      "epoch 11 | loss: 0.4115  | val_0_auc: 0.4908  |  0:00:37s\n",
      "epoch 12 | loss: 0.40501 | val_0_auc: 0.49061 |  0:00:40s\n",
      "epoch 13 | loss: 0.40044 | val_0_auc: 0.49413 |  0:00:42s\n",
      "epoch 14 | loss: 0.39722 | val_0_auc: 0.49889 |  0:00:46s\n",
      "epoch 15 | loss: 0.39716 | val_0_auc: 0.50272 |  0:00:49s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_0_auc = 0.52293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 22:51:29,468] Trial 3 finished with value: 0.5229290016073572 and parameters: {'n_d': 52, 'n_a': 48, 'n_steps': 3, 'gamma': 1.9639129306162544, 'lambda_sparse': 0.0004285734583978557, 'lr': 0.0005972595372105907}. Best is trial 1 with value: 0.6053800687704736.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.8001  | val_0_auc: 0.46334 |  0:00:03s\n",
      "epoch 1  | loss: 0.65621 | val_0_auc: 0.45791 |  0:00:06s\n",
      "epoch 2  | loss: 0.60561 | val_0_auc: 0.46549 |  0:00:10s\n",
      "epoch 3  | loss: 0.58093 | val_0_auc: 0.46991 |  0:00:14s\n",
      "epoch 4  | loss: 0.56148 | val_0_auc: 0.46187 |  0:00:18s\n",
      "epoch 5  | loss: 0.53528 | val_0_auc: 0.48195 |  0:00:21s\n",
      "epoch 6  | loss: 0.5425  | val_0_auc: 0.48662 |  0:00:24s\n",
      "epoch 7  | loss: 0.51111 | val_0_auc: 0.50268 |  0:00:28s\n",
      "epoch 8  | loss: 0.48685 | val_0_auc: 0.48421 |  0:00:31s\n",
      "epoch 9  | loss: 0.48201 | val_0_auc: 0.49379 |  0:00:35s\n",
      "epoch 10 | loss: 0.46227 | val_0_auc: 0.49083 |  0:00:39s\n",
      "epoch 11 | loss: 0.46652 | val_0_auc: 0.48948 |  0:00:43s\n",
      "epoch 12 | loss: 0.45712 | val_0_auc: 0.4962  |  0:00:47s\n",
      "epoch 13 | loss: 0.44162 | val_0_auc: 0.49249 |  0:00:51s\n",
      "epoch 14 | loss: 0.43543 | val_0_auc: 0.50807 |  0:00:55s\n",
      "epoch 15 | loss: 0.44171 | val_0_auc: 0.51988 |  0:00:58s\n",
      "epoch 16 | loss: 0.42019 | val_0_auc: 0.51767 |  0:01:01s\n",
      "epoch 17 | loss: 0.41671 | val_0_auc: 0.50554 |  0:01:04s\n",
      "epoch 18 | loss: 0.4082  | val_0_auc: 0.51129 |  0:01:06s\n",
      "epoch 19 | loss: 0.40564 | val_0_auc: 0.50703 |  0:01:09s\n",
      "epoch 20 | loss: 0.4075  | val_0_auc: 0.51378 |  0:01:12s\n",
      "epoch 21 | loss: 0.40345 | val_0_auc: 0.51666 |  0:01:15s\n",
      "epoch 22 | loss: 0.39175 | val_0_auc: 0.51887 |  0:01:18s\n",
      "epoch 23 | loss: 0.39351 | val_0_auc: 0.52983 |  0:01:21s\n",
      "epoch 24 | loss: 0.39154 | val_0_auc: 0.53275 |  0:01:24s\n",
      "epoch 25 | loss: 0.38784 | val_0_auc: 0.52277 |  0:01:27s\n",
      "epoch 26 | loss: 0.38176 | val_0_auc: 0.53108 |  0:01:30s\n",
      "epoch 27 | loss: 0.38042 | val_0_auc: 0.52726 |  0:01:33s\n",
      "epoch 28 | loss: 0.38115 | val_0_auc: 0.52021 |  0:01:35s\n",
      "epoch 29 | loss: 0.37583 | val_0_auc: 0.52421 |  0:01:39s\n",
      "epoch 30 | loss: 0.37184 | val_0_auc: 0.53103 |  0:01:42s\n",
      "epoch 31 | loss: 0.37559 | val_0_auc: 0.5337  |  0:01:45s\n",
      "epoch 32 | loss: 0.37691 | val_0_auc: 0.54325 |  0:01:48s\n",
      "epoch 33 | loss: 0.37034 | val_0_auc: 0.53124 |  0:01:51s\n",
      "epoch 34 | loss: 0.37295 | val_0_auc: 0.55289 |  0:01:54s\n",
      "epoch 35 | loss: 0.36747 | val_0_auc: 0.54943 |  0:01:57s\n",
      "epoch 36 | loss: 0.36646 | val_0_auc: 0.53647 |  0:02:00s\n",
      "epoch 37 | loss: 0.36479 | val_0_auc: 0.54547 |  0:02:03s\n",
      "epoch 38 | loss: 0.36251 | val_0_auc: 0.53549 |  0:02:06s\n",
      "epoch 39 | loss: 0.36185 | val_0_auc: 0.52841 |  0:02:09s\n",
      "epoch 40 | loss: 0.36121 | val_0_auc: 0.546   |  0:02:12s\n",
      "epoch 41 | loss: 0.35822 | val_0_auc: 0.55323 |  0:02:15s\n",
      "epoch 42 | loss: 0.3646  | val_0_auc: 0.53869 |  0:02:19s\n",
      "epoch 43 | loss: 0.35298 | val_0_auc: 0.55701 |  0:02:23s\n",
      "epoch 44 | loss: 0.35483 | val_0_auc: 0.55892 |  0:02:27s\n",
      "epoch 45 | loss: 0.35921 | val_0_auc: 0.56119 |  0:02:32s\n",
      "epoch 46 | loss: 0.35095 | val_0_auc: 0.5612  |  0:02:37s\n",
      "epoch 47 | loss: 0.35452 | val_0_auc: 0.56011 |  0:02:41s\n",
      "epoch 48 | loss: 0.35662 | val_0_auc: 0.56761 |  0:02:44s\n",
      "epoch 49 | loss: 0.35236 | val_0_auc: 0.55926 |  0:02:47s\n",
      "epoch 50 | loss: 0.35287 | val_0_auc: 0.5507  |  0:02:50s\n",
      "epoch 51 | loss: 0.35698 | val_0_auc: 0.54796 |  0:02:53s\n",
      "epoch 52 | loss: 0.35466 | val_0_auc: 0.54591 |  0:02:56s\n",
      "epoch 53 | loss: 0.34852 | val_0_auc: 0.57    |  0:02:58s\n",
      "epoch 54 | loss: 0.35091 | val_0_auc: 0.55654 |  0:03:01s\n",
      "epoch 55 | loss: 0.3496  | val_0_auc: 0.56874 |  0:03:04s\n",
      "epoch 56 | loss: 0.34921 | val_0_auc: 0.54893 |  0:03:07s\n",
      "epoch 57 | loss: 0.34764 | val_0_auc: 0.54837 |  0:03:10s\n",
      "epoch 58 | loss: 0.3526  | val_0_auc: 0.56584 |  0:03:14s\n",
      "epoch 59 | loss: 0.34436 | val_0_auc: 0.55926 |  0:03:18s\n",
      "epoch 60 | loss: 0.34735 | val_0_auc: 0.56    |  0:03:21s\n",
      "epoch 61 | loss: 0.34436 | val_0_auc: 0.57443 |  0:03:26s\n",
      "epoch 62 | loss: 0.34503 | val_0_auc: 0.57711 |  0:03:30s\n",
      "epoch 63 | loss: 0.34405 | val_0_auc: 0.59714 |  0:03:34s\n",
      "epoch 64 | loss: 0.34454 | val_0_auc: 0.586   |  0:03:38s\n",
      "epoch 65 | loss: 0.34676 | val_0_auc: 0.58806 |  0:03:41s\n",
      "epoch 66 | loss: 0.34033 | val_0_auc: 0.58038 |  0:03:45s\n",
      "epoch 67 | loss: 0.34285 | val_0_auc: 0.57795 |  0:03:50s\n",
      "epoch 68 | loss: 0.33802 | val_0_auc: 0.58589 |  0:03:55s\n",
      "epoch 69 | loss: 0.34252 | val_0_auc: 0.5967  |  0:03:58s\n",
      "epoch 70 | loss: 0.3438  | val_0_auc: 0.59663 |  0:04:03s\n",
      "epoch 71 | loss: 0.33958 | val_0_auc: 0.5785  |  0:04:06s\n",
      "epoch 72 | loss: 0.34355 | val_0_auc: 0.57599 |  0:04:09s\n",
      "epoch 73 | loss: 0.34138 | val_0_auc: 0.58977 |  0:04:12s\n",
      "epoch 74 | loss: 0.33656 | val_0_auc: 0.58898 |  0:04:16s\n",
      "epoch 75 | loss: 0.34364 | val_0_auc: 0.59582 |  0:04:19s\n",
      "epoch 76 | loss: 0.34163 | val_0_auc: 0.58464 |  0:04:22s\n",
      "epoch 77 | loss: 0.34117 | val_0_auc: 0.58233 |  0:04:25s\n",
      "epoch 78 | loss: 0.34105 | val_0_auc: 0.59524 |  0:04:28s\n",
      "\n",
      "Early stopping occurred at epoch 78 with best_epoch = 63 and best_val_0_auc = 0.59714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 22:55:59,860] Trial 4 finished with value: 0.5971358776373884 and parameters: {'n_d': 50, 'n_a': 17, 'n_steps': 5, 'gamma': 1.718448068103315, 'lambda_sparse': 3.0075791367579284e-06, 'lr': 0.001143851832541374}. Best is trial 1 with value: 0.6053800687704736.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73808 | val_0_auc: 0.457   |  0:00:04s\n",
      "epoch 1  | loss: 0.65003 | val_0_auc: 0.4579  |  0:00:09s\n",
      "epoch 2  | loss: 0.61995 | val_0_auc: 0.46136 |  0:00:13s\n",
      "epoch 3  | loss: 0.56503 | val_0_auc: 0.47893 |  0:00:19s\n",
      "epoch 4  | loss: 0.53155 | val_0_auc: 0.49563 |  0:00:23s\n",
      "epoch 5  | loss: 0.51728 | val_0_auc: 0.48952 |  0:00:26s\n",
      "epoch 6  | loss: 0.50176 | val_0_auc: 0.4905  |  0:00:30s\n",
      "epoch 7  | loss: 0.48263 | val_0_auc: 0.48768 |  0:00:34s\n",
      "epoch 8  | loss: 0.46376 | val_0_auc: 0.50239 |  0:00:37s\n",
      "epoch 9  | loss: 0.46378 | val_0_auc: 0.50234 |  0:00:41s\n",
      "epoch 10 | loss: 0.43778 | val_0_auc: 0.51415 |  0:00:45s\n",
      "epoch 11 | loss: 0.44432 | val_0_auc: 0.50292 |  0:00:50s\n",
      "epoch 12 | loss: 0.42228 | val_0_auc: 0.5066  |  0:00:53s\n",
      "epoch 13 | loss: 0.39929 | val_0_auc: 0.52666 |  0:00:58s\n",
      "epoch 14 | loss: 0.40564 | val_0_auc: 0.51921 |  0:01:03s\n",
      "epoch 15 | loss: 0.40926 | val_0_auc: 0.52256 |  0:01:07s\n",
      "epoch 16 | loss: 0.40461 | val_0_auc: 0.51321 |  0:01:10s\n",
      "epoch 17 | loss: 0.39488 | val_0_auc: 0.52559 |  0:01:14s\n",
      "epoch 18 | loss: 0.38642 | val_0_auc: 0.53819 |  0:01:18s\n",
      "epoch 19 | loss: 0.38235 | val_0_auc: 0.53658 |  0:01:21s\n",
      "epoch 20 | loss: 0.38528 | val_0_auc: 0.5371  |  0:01:25s\n",
      "epoch 21 | loss: 0.38486 | val_0_auc: 0.53762 |  0:01:28s\n",
      "epoch 22 | loss: 0.37449 | val_0_auc: 0.53624 |  0:01:32s\n",
      "epoch 23 | loss: 0.37459 | val_0_auc: 0.53491 |  0:01:35s\n",
      "epoch 24 | loss: 0.37012 | val_0_auc: 0.53305 |  0:01:39s\n",
      "epoch 25 | loss: 0.36791 | val_0_auc: 0.55296 |  0:01:42s\n",
      "epoch 26 | loss: 0.37201 | val_0_auc: 0.5448  |  0:01:46s\n",
      "epoch 27 | loss: 0.36176 | val_0_auc: 0.55164 |  0:01:49s\n",
      "epoch 28 | loss: 0.36393 | val_0_auc: 0.56081 |  0:01:53s\n",
      "epoch 29 | loss: 0.36121 | val_0_auc: 0.56423 |  0:01:56s\n",
      "epoch 30 | loss: 0.35945 | val_0_auc: 0.55386 |  0:02:00s\n",
      "epoch 31 | loss: 0.36375 | val_0_auc: 0.55634 |  0:02:04s\n",
      "epoch 32 | loss: 0.35885 | val_0_auc: 0.56357 |  0:02:07s\n",
      "epoch 33 | loss: 0.35695 | val_0_auc: 0.56084 |  0:02:11s\n",
      "epoch 34 | loss: 0.35655 | val_0_auc: 0.57235 |  0:02:14s\n",
      "epoch 35 | loss: 0.35013 | val_0_auc: 0.55843 |  0:02:18s\n",
      "epoch 36 | loss: 0.35022 | val_0_auc: 0.57153 |  0:02:21s\n",
      "epoch 37 | loss: 0.3496  | val_0_auc: 0.57267 |  0:02:25s\n",
      "epoch 38 | loss: 0.34972 | val_0_auc: 0.57457 |  0:02:28s\n",
      "epoch 39 | loss: 0.3482  | val_0_auc: 0.58313 |  0:02:32s\n",
      "epoch 40 | loss: 0.34903 | val_0_auc: 0.58756 |  0:02:35s\n",
      "epoch 41 | loss: 0.3465  | val_0_auc: 0.58923 |  0:02:39s\n",
      "epoch 42 | loss: 0.34761 | val_0_auc: 0.58257 |  0:02:42s\n",
      "epoch 43 | loss: 0.34685 | val_0_auc: 0.57482 |  0:02:46s\n",
      "epoch 44 | loss: 0.34556 | val_0_auc: 0.58707 |  0:02:50s\n",
      "epoch 45 | loss: 0.34307 | val_0_auc: 0.57436 |  0:02:53s\n",
      "epoch 46 | loss: 0.34343 | val_0_auc: 0.60362 |  0:02:57s\n",
      "epoch 47 | loss: 0.34277 | val_0_auc: 0.59722 |  0:03:01s\n",
      "epoch 48 | loss: 0.34398 | val_0_auc: 0.58346 |  0:03:04s\n",
      "epoch 49 | loss: 0.34127 | val_0_auc: 0.58598 |  0:03:08s\n",
      "epoch 50 | loss: 0.34105 | val_0_auc: 0.58231 |  0:03:11s\n",
      "epoch 51 | loss: 0.3409  | val_0_auc: 0.58199 |  0:03:16s\n",
      "epoch 52 | loss: 0.33952 | val_0_auc: 0.59626 |  0:03:19s\n",
      "epoch 53 | loss: 0.34004 | val_0_auc: 0.60029 |  0:03:23s\n",
      "epoch 54 | loss: 0.34183 | val_0_auc: 0.60181 |  0:03:27s\n",
      "epoch 55 | loss: 0.3427  | val_0_auc: 0.60502 |  0:03:33s\n",
      "epoch 56 | loss: 0.33938 | val_0_auc: 0.59604 |  0:03:37s\n",
      "epoch 57 | loss: 0.33882 | val_0_auc: 0.61264 |  0:03:40s\n",
      "epoch 58 | loss: 0.33909 | val_0_auc: 0.62008 |  0:03:44s\n",
      "epoch 59 | loss: 0.33898 | val_0_auc: 0.61743 |  0:03:48s\n",
      "epoch 60 | loss: 0.33907 | val_0_auc: 0.60343 |  0:03:52s\n",
      "epoch 61 | loss: 0.33522 | val_0_auc: 0.61562 |  0:03:56s\n",
      "epoch 62 | loss: 0.33958 | val_0_auc: 0.5947  |  0:04:01s\n",
      "epoch 63 | loss: 0.33595 | val_0_auc: 0.60427 |  0:04:05s\n",
      "epoch 64 | loss: 0.33745 | val_0_auc: 0.59841 |  0:04:10s\n",
      "epoch 65 | loss: 0.33589 | val_0_auc: 0.60405 |  0:04:16s\n",
      "epoch 66 | loss: 0.33698 | val_0_auc: 0.60152 |  0:04:22s\n",
      "epoch 67 | loss: 0.33499 | val_0_auc: 0.59465 |  0:04:27s\n",
      "epoch 68 | loss: 0.33411 | val_0_auc: 0.59509 |  0:04:31s\n",
      "epoch 69 | loss: 0.33597 | val_0_auc: 0.5964  |  0:04:35s\n",
      "epoch 70 | loss: 0.33285 | val_0_auc: 0.60146 |  0:04:39s\n",
      "epoch 71 | loss: 0.33619 | val_0_auc: 0.5951  |  0:04:43s\n",
      "epoch 72 | loss: 0.33185 | val_0_auc: 0.60206 |  0:04:49s\n",
      "epoch 73 | loss: 0.33672 | val_0_auc: 0.60345 |  0:04:53s\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 58 and best_val_0_auc = 0.62008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:00:54,919] Trial 5 finished with value: 0.6200798846365134 and parameters: {'n_d': 49, 'n_a': 43, 'n_steps': 6, 'gamma': 1.6816661069103733, 'lambda_sparse': 0.0006069520269805606, 'lr': 0.0014454446447764071}. Best is trial 5 with value: 0.6200798846365134.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.96317 | val_0_auc: 0.50984 |  0:00:01s\n",
      "epoch 1  | loss: 0.89497 | val_0_auc: 0.50011 |  0:00:03s\n",
      "epoch 2  | loss: 0.832   | val_0_auc: 0.50787 |  0:00:05s\n",
      "epoch 3  | loss: 0.78224 | val_0_auc: 0.50195 |  0:00:07s\n",
      "epoch 4  | loss: 0.72892 | val_0_auc: 0.49849 |  0:00:08s\n",
      "epoch 5  | loss: 0.68832 | val_0_auc: 0.49137 |  0:00:10s\n",
      "epoch 6  | loss: 0.6449  | val_0_auc: 0.49176 |  0:00:12s\n",
      "epoch 7  | loss: 0.62199 | val_0_auc: 0.48861 |  0:00:14s\n",
      "epoch 8  | loss: 0.58755 | val_0_auc: 0.4932  |  0:00:16s\n",
      "epoch 9  | loss: 0.56368 | val_0_auc: 0.49222 |  0:00:18s\n",
      "epoch 10 | loss: 0.55503 | val_0_auc: 0.48935 |  0:00:20s\n",
      "epoch 11 | loss: 0.54234 | val_0_auc: 0.48875 |  0:00:22s\n",
      "epoch 12 | loss: 0.51419 | val_0_auc: 0.48485 |  0:00:24s\n",
      "epoch 13 | loss: 0.50251 | val_0_auc: 0.48917 |  0:00:26s\n",
      "epoch 14 | loss: 0.49511 | val_0_auc: 0.4861  |  0:00:28s\n",
      "epoch 15 | loss: 0.4871  | val_0_auc: 0.48839 |  0:00:29s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_0_auc = 0.50984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:01:25,589] Trial 6 finished with value: 0.5098440456570836 and parameters: {'n_d': 41, 'n_a': 19, 'n_steps': 3, 'gamma': 2.333120487896205, 'lambda_sparse': 0.00017985401209856777, 'lr': 0.00025230373810459985}. Best is trial 5 with value: 0.6200798846365134.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.68346 | val_0_auc: 0.54401 |  0:00:03s\n",
      "epoch 1  | loss: 3.38298 | val_0_auc: 0.53694 |  0:00:06s\n",
      "epoch 2  | loss: 3.09384 | val_0_auc: 0.54049 |  0:00:10s\n",
      "epoch 3  | loss: 2.7925  | val_0_auc: 0.54773 |  0:00:13s\n",
      "epoch 4  | loss: 2.55355 | val_0_auc: 0.53978 |  0:00:17s\n",
      "epoch 5  | loss: 2.27412 | val_0_auc: 0.52725 |  0:00:21s\n",
      "epoch 6  | loss: 2.03481 | val_0_auc: 0.53304 |  0:00:25s\n",
      "epoch 7  | loss: 1.82809 | val_0_auc: 0.53253 |  0:00:28s\n",
      "epoch 8  | loss: 1.5961  | val_0_auc: 0.52656 |  0:00:32s\n",
      "epoch 9  | loss: 1.46422 | val_0_auc: 0.51828 |  0:00:35s\n",
      "epoch 10 | loss: 1.2843  | val_0_auc: 0.52819 |  0:00:39s\n",
      "epoch 11 | loss: 1.11662 | val_0_auc: 0.51616 |  0:00:42s\n",
      "epoch 12 | loss: 0.99948 | val_0_auc: 0.49654 |  0:00:45s\n",
      "epoch 13 | loss: 0.90101 | val_0_auc: 0.50188 |  0:00:49s\n",
      "epoch 14 | loss: 0.81015 | val_0_auc: 0.49371 |  0:00:52s\n",
      "epoch 15 | loss: 0.75276 | val_0_auc: 0.48785 |  0:00:56s\n",
      "epoch 16 | loss: 0.65541 | val_0_auc: 0.49186 |  0:01:00s\n",
      "epoch 17 | loss: 0.61107 | val_0_auc: 0.49698 |  0:01:03s\n",
      "epoch 18 | loss: 0.59198 | val_0_auc: 0.4946  |  0:01:07s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 3 and best_val_0_auc = 0.54773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:02:34,437] Trial 7 finished with value: 0.5477298622555902 and parameters: {'n_d': 25, 'n_a': 16, 'n_steps': 7, 'gamma': 1.178548075422791, 'lambda_sparse': 8.240785726735729e-06, 'lr': 0.0004489891410695522}. Best is trial 5 with value: 0.6200798846365134.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.85305 | val_0_auc: 0.52109 |  0:00:02s\n",
      "epoch 1  | loss: 2.61999 | val_0_auc: 0.52762 |  0:00:06s\n",
      "epoch 2  | loss: 2.4587  | val_0_auc: 0.50311 |  0:00:11s\n",
      "epoch 3  | loss: 2.29416 | val_0_auc: 0.51355 |  0:00:14s\n",
      "epoch 4  | loss: 2.16573 | val_0_auc: 0.51287 |  0:00:19s\n",
      "epoch 5  | loss: 2.0097  | val_0_auc: 0.48725 |  0:00:24s\n",
      "epoch 6  | loss: 1.8404  | val_0_auc: 0.49742 |  0:00:28s\n",
      "epoch 7  | loss: 1.7232  | val_0_auc: 0.4903  |  0:00:31s\n",
      "epoch 8  | loss: 1.60275 | val_0_auc: 0.48273 |  0:00:35s\n",
      "epoch 9  | loss: 1.44744 | val_0_auc: 0.49321 |  0:00:40s\n",
      "epoch 10 | loss: 1.37162 | val_0_auc: 0.49436 |  0:00:43s\n",
      "epoch 11 | loss: 1.24728 | val_0_auc: 0.48644 |  0:00:48s\n",
      "epoch 12 | loss: 1.13758 | val_0_auc: 0.4894  |  0:00:52s\n",
      "epoch 13 | loss: 1.0638  | val_0_auc: 0.48356 |  0:00:56s\n",
      "epoch 14 | loss: 0.98731 | val_0_auc: 0.48104 |  0:01:00s\n",
      "epoch 15 | loss: 0.93009 | val_0_auc: 0.48548 |  0:01:03s\n",
      "epoch 16 | loss: 0.86713 | val_0_auc: 0.48968 |  0:01:07s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 1 and best_val_0_auc = 0.52762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:03:42,843] Trial 8 finished with value: 0.5276169148914525 and parameters: {'n_d': 48, 'n_a': 32, 'n_steps': 5, 'gamma': 2.0890609747464928, 'lambda_sparse': 0.0005525086193360339, 'lr': 0.00021625246921196166}. Best is trial 5 with value: 0.6200798846365134.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.90789 | val_0_auc: 0.50032 |  0:00:02s\n",
      "epoch 1  | loss: 0.84976 | val_0_auc: 0.50006 |  0:00:04s\n",
      "epoch 2  | loss: 0.83135 | val_0_auc: 0.51223 |  0:00:06s\n",
      "epoch 3  | loss: 0.78869 | val_0_auc: 0.51576 |  0:00:08s\n",
      "epoch 4  | loss: 0.75777 | val_0_auc: 0.51229 |  0:00:10s\n",
      "epoch 5  | loss: 0.72497 | val_0_auc: 0.50941 |  0:00:13s\n",
      "epoch 6  | loss: 0.69165 | val_0_auc: 0.50787 |  0:00:15s\n",
      "epoch 7  | loss: 0.67145 | val_0_auc: 0.50764 |  0:00:17s\n",
      "epoch 8  | loss: 0.64329 | val_0_auc: 0.50706 |  0:00:19s\n",
      "epoch 9  | loss: 0.61793 | val_0_auc: 0.50659 |  0:00:21s\n",
      "epoch 10 | loss: 0.588   | val_0_auc: 0.50265 |  0:00:24s\n",
      "epoch 11 | loss: 0.56916 | val_0_auc: 0.50373 |  0:00:26s\n",
      "epoch 12 | loss: 0.55289 | val_0_auc: 0.50294 |  0:00:28s\n",
      "epoch 13 | loss: 0.53983 | val_0_auc: 0.50134 |  0:00:30s\n",
      "epoch 14 | loss: 0.51541 | val_0_auc: 0.507   |  0:00:32s\n",
      "epoch 15 | loss: 0.51862 | val_0_auc: 0.5094  |  0:00:34s\n",
      "epoch 16 | loss: 0.50743 | val_0_auc: 0.49906 |  0:00:36s\n",
      "epoch 17 | loss: 0.49867 | val_0_auc: 0.49802 |  0:00:38s\n",
      "epoch 18 | loss: 0.48793 | val_0_auc: 0.50193 |  0:00:40s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 3 and best_val_0_auc = 0.51576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:04:24,639] Trial 9 finished with value: 0.5157556613562839 and parameters: {'n_d': 26, 'n_a': 42, 'n_steps': 3, 'gamma': 1.7282856468209404, 'lambda_sparse': 3.692625332011417e-05, 'lr': 0.00019534787139402805}. Best is trial 5 with value: 0.6200798846365134.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.48417 | val_0_auc: 0.47808 |  0:00:04s\n",
      "epoch 1  | loss: 0.84481 | val_0_auc: 0.46537 |  0:00:08s\n",
      "epoch 2  | loss: 0.66734 | val_0_auc: 0.46438 |  0:00:12s\n",
      "epoch 3  | loss: 0.62804 | val_0_auc: 0.48541 |  0:00:16s\n",
      "epoch 4  | loss: 0.54595 | val_0_auc: 0.48851 |  0:00:20s\n",
      "epoch 5  | loss: 0.53064 | val_0_auc: 0.49073 |  0:00:25s\n",
      "epoch 6  | loss: 0.48821 | val_0_auc: 0.49172 |  0:00:30s\n",
      "epoch 7  | loss: 0.4665  | val_0_auc: 0.48144 |  0:00:34s\n",
      "epoch 8  | loss: 0.44441 | val_0_auc: 0.50087 |  0:00:39s\n",
      "epoch 9  | loss: 0.44544 | val_0_auc: 0.48833 |  0:00:43s\n",
      "epoch 10 | loss: 0.43279 | val_0_auc: 0.49468 |  0:00:47s\n",
      "epoch 11 | loss: 0.42492 | val_0_auc: 0.53298 |  0:00:51s\n",
      "epoch 12 | loss: 0.41477 | val_0_auc: 0.55131 |  0:00:55s\n",
      "epoch 13 | loss: 0.41064 | val_0_auc: 0.54493 |  0:00:59s\n",
      "epoch 14 | loss: 0.41215 | val_0_auc: 0.52187 |  0:01:03s\n",
      "epoch 15 | loss: 0.41307 | val_0_auc: 0.54844 |  0:01:07s\n",
      "epoch 16 | loss: 0.43196 | val_0_auc: 0.53603 |  0:01:12s\n",
      "epoch 17 | loss: 0.405   | val_0_auc: 0.50628 |  0:01:16s\n",
      "epoch 18 | loss: 0.39228 | val_0_auc: 0.52909 |  0:01:20s\n",
      "epoch 19 | loss: 0.39131 | val_0_auc: 0.54568 |  0:01:25s\n",
      "epoch 20 | loss: 0.3876  | val_0_auc: 0.54458 |  0:01:29s\n",
      "epoch 21 | loss: 0.3865  | val_0_auc: 0.55162 |  0:01:33s\n",
      "epoch 22 | loss: 0.37553 | val_0_auc: 0.5491  |  0:01:38s\n",
      "epoch 23 | loss: 0.37624 | val_0_auc: 0.54521 |  0:01:42s\n",
      "epoch 24 | loss: 0.37243 | val_0_auc: 0.56451 |  0:01:46s\n",
      "epoch 25 | loss: 0.36788 | val_0_auc: 0.56257 |  0:01:50s\n",
      "epoch 26 | loss: 0.37287 | val_0_auc: 0.57542 |  0:01:54s\n",
      "epoch 27 | loss: 0.36255 | val_0_auc: 0.56191 |  0:01:59s\n",
      "epoch 28 | loss: 0.36267 | val_0_auc: 0.55539 |  0:02:03s\n",
      "epoch 29 | loss: 0.37598 | val_0_auc: 0.55017 |  0:02:07s\n",
      "epoch 30 | loss: 0.36146 | val_0_auc: 0.55841 |  0:02:11s\n",
      "epoch 31 | loss: 0.37269 | val_0_auc: 0.55712 |  0:02:17s\n",
      "epoch 32 | loss: 0.37011 | val_0_auc: 0.55514 |  0:02:23s\n",
      "epoch 33 | loss: 0.36138 | val_0_auc: 0.56673 |  0:02:28s\n",
      "epoch 34 | loss: 0.35876 | val_0_auc: 0.55451 |  0:02:36s\n",
      "epoch 35 | loss: 0.35747 | val_0_auc: 0.5407  |  0:02:44s\n",
      "epoch 36 | loss: 0.36443 | val_0_auc: 0.56526 |  0:02:52s\n",
      "epoch 37 | loss: 0.35589 | val_0_auc: 0.55434 |  0:03:01s\n",
      "epoch 38 | loss: 0.36049 | val_0_auc: 0.58331 |  0:03:09s\n",
      "epoch 39 | loss: 0.36179 | val_0_auc: 0.56014 |  0:03:18s\n",
      "epoch 40 | loss: 0.36044 | val_0_auc: 0.57733 |  0:03:26s\n",
      "epoch 41 | loss: 0.35766 | val_0_auc: 0.56015 |  0:03:34s\n",
      "epoch 42 | loss: 0.37484 | val_0_auc: 0.5469  |  0:03:43s\n",
      "epoch 43 | loss: 0.35276 | val_0_auc: 0.54258 |  0:03:53s\n",
      "epoch 44 | loss: 0.35087 | val_0_auc: 0.56769 |  0:04:02s\n",
      "epoch 45 | loss: 0.35235 | val_0_auc: 0.5495  |  0:04:11s\n",
      "epoch 46 | loss: 0.34934 | val_0_auc: 0.58674 |  0:04:21s\n",
      "epoch 47 | loss: 0.35243 | val_0_auc: 0.58804 |  0:04:31s\n",
      "epoch 48 | loss: 0.34025 | val_0_auc: 0.58484 |  0:04:41s\n",
      "epoch 49 | loss: 0.34939 | val_0_auc: 0.58086 |  0:04:50s\n",
      "epoch 50 | loss: 0.35914 | val_0_auc: 0.56155 |  0:04:58s\n",
      "epoch 51 | loss: 0.35492 | val_0_auc: 0.57404 |  0:05:06s\n",
      "epoch 52 | loss: 0.34694 | val_0_auc: 0.56854 |  0:05:15s\n",
      "epoch 53 | loss: 0.34512 | val_0_auc: 0.58462 |  0:05:24s\n",
      "epoch 54 | loss: 0.3428  | val_0_auc: 0.56263 |  0:05:34s\n",
      "epoch 55 | loss: 0.34535 | val_0_auc: 0.58801 |  0:05:44s\n",
      "epoch 56 | loss: 0.34259 | val_0_auc: 0.56708 |  0:05:54s\n",
      "epoch 57 | loss: 0.33934 | val_0_auc: 0.58689 |  0:06:04s\n",
      "epoch 58 | loss: 0.33956 | val_0_auc: 0.58255 |  0:06:13s\n",
      "epoch 59 | loss: 0.34425 | val_0_auc: 0.57973 |  0:06:21s\n",
      "epoch 60 | loss: 0.35151 | val_0_auc: 0.56349 |  0:06:30s\n",
      "epoch 61 | loss: 0.34747 | val_0_auc: 0.60339 |  0:06:38s\n",
      "epoch 62 | loss: 0.34429 | val_0_auc: 0.57769 |  0:06:46s\n",
      "epoch 63 | loss: 0.3353  | val_0_auc: 0.58631 |  0:06:56s\n",
      "epoch 64 | loss: 0.33783 | val_0_auc: 0.59629 |  0:07:05s\n",
      "epoch 65 | loss: 0.3406  | val_0_auc: 0.58827 |  0:07:13s\n",
      "epoch 66 | loss: 0.3369  | val_0_auc: 0.58573 |  0:07:22s\n",
      "epoch 67 | loss: 0.34135 | val_0_auc: 0.58982 |  0:07:30s\n",
      "epoch 68 | loss: 0.33692 | val_0_auc: 0.58158 |  0:07:40s\n",
      "epoch 69 | loss: 0.33938 | val_0_auc: 0.59132 |  0:07:51s\n",
      "epoch 70 | loss: 0.33814 | val_0_auc: 0.61808 |  0:08:01s\n",
      "epoch 71 | loss: 0.33671 | val_0_auc: 0.60645 |  0:08:10s\n",
      "epoch 72 | loss: 0.33802 | val_0_auc: 0.60957 |  0:08:18s\n",
      "epoch 73 | loss: 0.33338 | val_0_auc: 0.6114  |  0:08:27s\n",
      "epoch 74 | loss: 0.33486 | val_0_auc: 0.60337 |  0:08:36s\n",
      "epoch 75 | loss: 0.33507 | val_0_auc: 0.61042 |  0:08:47s\n",
      "epoch 76 | loss: 0.3413  | val_0_auc: 0.61116 |  0:08:57s\n",
      "epoch 77 | loss: 0.34691 | val_0_auc: 0.60533 |  0:09:07s\n",
      "epoch 78 | loss: 0.36797 | val_0_auc: 0.59401 |  0:09:16s\n",
      "epoch 79 | loss: 0.35072 | val_0_auc: 0.62496 |  0:09:26s\n",
      "epoch 80 | loss: 0.3371  | val_0_auc: 0.58085 |  0:09:37s\n",
      "epoch 81 | loss: 0.34047 | val_0_auc: 0.6158  |  0:09:46s\n",
      "epoch 82 | loss: 0.34142 | val_0_auc: 0.58961 |  0:09:55s\n",
      "epoch 83 | loss: 0.33975 | val_0_auc: 0.59425 |  0:10:03s\n",
      "epoch 84 | loss: 0.33614 | val_0_auc: 0.60701 |  0:10:12s\n",
      "epoch 85 | loss: 0.33671 | val_0_auc: 0.58879 |  0:10:21s\n",
      "epoch 86 | loss: 0.33567 | val_0_auc: 0.60957 |  0:10:32s\n",
      "epoch 87 | loss: 0.33356 | val_0_auc: 0.60493 |  0:10:43s\n",
      "epoch 88 | loss: 0.33731 | val_0_auc: 0.58207 |  0:10:53s\n",
      "epoch 89 | loss: 0.33906 | val_0_auc: 0.59439 |  0:11:01s\n",
      "epoch 90 | loss: 0.33383 | val_0_auc: 0.59972 |  0:11:10s\n",
      "epoch 91 | loss: 0.33113 | val_0_auc: 0.60887 |  0:11:19s\n",
      "epoch 92 | loss: 0.33183 | val_0_auc: 0.62652 |  0:11:30s\n",
      "epoch 93 | loss: 0.32851 | val_0_auc: 0.61833 |  0:11:41s\n",
      "epoch 94 | loss: 0.3324  | val_0_auc: 0.61779 |  0:11:50s\n",
      "epoch 95 | loss: 0.33105 | val_0_auc: 0.61871 |  0:12:00s\n",
      "epoch 96 | loss: 0.33079 | val_0_auc: 0.60862 |  0:12:09s\n",
      "epoch 97 | loss: 0.33395 | val_0_auc: 0.60996 |  0:12:17s\n",
      "epoch 98 | loss: 0.33234 | val_0_auc: 0.5816  |  0:12:26s\n",
      "epoch 99 | loss: 0.33421 | val_0_auc: 0.59427 |  0:12:35s\n",
      "epoch 100| loss: 0.3319  | val_0_auc: 0.5982  |  0:12:45s\n",
      "epoch 101| loss: 0.33203 | val_0_auc: 0.59972 |  0:12:55s\n",
      "epoch 102| loss: 0.334   | val_0_auc: 0.60871 |  0:13:05s\n",
      "epoch 103| loss: 0.3297  | val_0_auc: 0.61159 |  0:13:16s\n",
      "epoch 104| loss: 0.33065 | val_0_auc: 0.61686 |  0:13:25s\n",
      "epoch 105| loss: 0.32944 | val_0_auc: 0.61287 |  0:13:33s\n",
      "epoch 106| loss: 0.33    | val_0_auc: 0.59894 |  0:13:42s\n",
      "epoch 107| loss: 0.32938 | val_0_auc: 0.59395 |  0:13:51s\n",
      "\n",
      "Early stopping occurred at epoch 107 with best_epoch = 92 and best_val_0_auc = 0.62652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:18:20,029] Trial 10 finished with value: 0.6265174520336121 and parameters: {'n_d': 62, 'n_a': 63, 'n_steps': 6, 'gamma': 2.4998217780223735, 'lambda_sparse': 7.342693165357964e-05, 'lr': 0.002512448480795532}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.66334 | val_0_auc: 0.48741 |  0:00:09s\n",
      "epoch 1  | loss: 0.54984 | val_0_auc: 0.48229 |  0:00:19s\n",
      "epoch 2  | loss: 0.52048 | val_0_auc: 0.51016 |  0:00:29s\n",
      "epoch 3  | loss: 0.48834 | val_0_auc: 0.5277  |  0:00:40s\n",
      "epoch 4  | loss: 0.46365 | val_0_auc: 0.52501 |  0:00:49s\n",
      "epoch 5  | loss: 0.43771 | val_0_auc: 0.50547 |  0:00:58s\n",
      "epoch 6  | loss: 0.42846 | val_0_auc: 0.51853 |  0:01:07s\n",
      "epoch 7  | loss: 0.41941 | val_0_auc: 0.53282 |  0:01:16s\n",
      "epoch 8  | loss: 0.40564 | val_0_auc: 0.54497 |  0:01:26s\n",
      "epoch 9  | loss: 0.40342 | val_0_auc: 0.54688 |  0:01:38s\n",
      "epoch 10 | loss: 0.39369 | val_0_auc: 0.5648  |  0:01:47s\n",
      "epoch 11 | loss: 0.39596 | val_0_auc: 0.55964 |  0:01:55s\n",
      "epoch 12 | loss: 0.38286 | val_0_auc: 0.56468 |  0:02:04s\n",
      "epoch 13 | loss: 0.38282 | val_0_auc: 0.57551 |  0:02:13s\n",
      "epoch 14 | loss: 0.37771 | val_0_auc: 0.55364 |  0:02:21s\n",
      "epoch 15 | loss: 0.39295 | val_0_auc: 0.56056 |  0:02:31s\n",
      "epoch 16 | loss: 0.39769 | val_0_auc: 0.50735 |  0:02:41s\n",
      "epoch 17 | loss: 0.39499 | val_0_auc: 0.57889 |  0:02:52s\n",
      "epoch 18 | loss: 0.3819  | val_0_auc: 0.52648 |  0:03:02s\n",
      "epoch 19 | loss: 0.36968 | val_0_auc: 0.56552 |  0:03:11s\n",
      "epoch 20 | loss: 0.37694 | val_0_auc: 0.53466 |  0:03:19s\n",
      "epoch 21 | loss: 0.36957 | val_0_auc: 0.57348 |  0:03:27s\n",
      "epoch 22 | loss: 0.36256 | val_0_auc: 0.55435 |  0:03:37s\n",
      "epoch 23 | loss: 0.36269 | val_0_auc: 0.59967 |  0:03:47s\n",
      "epoch 24 | loss: 0.36299 | val_0_auc: 0.57168 |  0:03:58s\n",
      "epoch 25 | loss: 0.35233 | val_0_auc: 0.58596 |  0:04:06s\n",
      "epoch 26 | loss: 0.35566 | val_0_auc: 0.57136 |  0:04:15s\n",
      "epoch 27 | loss: 0.35129 | val_0_auc: 0.57939 |  0:04:24s\n",
      "epoch 28 | loss: 0.35614 | val_0_auc: 0.56192 |  0:04:32s\n",
      "epoch 29 | loss: 0.36155 | val_0_auc: 0.60008 |  0:04:41s\n",
      "epoch 30 | loss: 0.35328 | val_0_auc: 0.59849 |  0:04:51s\n",
      "epoch 31 | loss: 0.34762 | val_0_auc: 0.58547 |  0:05:00s\n",
      "epoch 32 | loss: 0.3463  | val_0_auc: 0.58448 |  0:05:09s\n",
      "epoch 33 | loss: 0.34675 | val_0_auc: 0.58168 |  0:05:21s\n",
      "epoch 34 | loss: 0.34663 | val_0_auc: 0.60286 |  0:05:33s\n",
      "epoch 35 | loss: 0.35179 | val_0_auc: 0.56674 |  0:05:45s\n",
      "epoch 36 | loss: 0.35862 | val_0_auc: 0.6022  |  0:05:57s\n",
      "epoch 37 | loss: 0.35139 | val_0_auc: 0.5821  |  0:06:10s\n",
      "epoch 38 | loss: 0.35158 | val_0_auc: 0.59639 |  0:06:22s\n",
      "epoch 39 | loss: 0.35252 | val_0_auc: 0.58878 |  0:06:35s\n",
      "epoch 40 | loss: 0.34765 | val_0_auc: 0.58866 |  0:06:45s\n",
      "epoch 41 | loss: 0.34196 | val_0_auc: 0.59023 |  0:06:54s\n",
      "epoch 42 | loss: 0.34172 | val_0_auc: 0.5934  |  0:07:04s\n",
      "epoch 43 | loss: 0.33841 | val_0_auc: 0.60676 |  0:07:14s\n",
      "epoch 44 | loss: 0.34035 | val_0_auc: 0.60955 |  0:07:24s\n",
      "epoch 45 | loss: 0.33761 | val_0_auc: 0.6165  |  0:07:34s\n",
      "epoch 46 | loss: 0.3331  | val_0_auc: 0.61765 |  0:07:43s\n",
      "epoch 47 | loss: 0.33725 | val_0_auc: 0.60959 |  0:07:52s\n",
      "epoch 48 | loss: 0.33743 | val_0_auc: 0.59457 |  0:08:00s\n",
      "epoch 49 | loss: 0.34017 | val_0_auc: 0.59802 |  0:08:08s\n",
      "epoch 50 | loss: 0.33923 | val_0_auc: 0.60924 |  0:08:17s\n",
      "epoch 51 | loss: 0.33687 | val_0_auc: 0.60181 |  0:08:26s\n",
      "epoch 52 | loss: 0.33624 | val_0_auc: 0.60966 |  0:08:35s\n",
      "epoch 53 | loss: 0.35789 | val_0_auc: 0.60934 |  0:08:46s\n",
      "epoch 54 | loss: 0.38106 | val_0_auc: 0.58225 |  0:08:56s\n",
      "epoch 55 | loss: 0.36998 | val_0_auc: 0.62106 |  0:09:05s\n",
      "epoch 56 | loss: 0.34567 | val_0_auc: 0.59495 |  0:09:13s\n",
      "epoch 57 | loss: 0.33516 | val_0_auc: 0.6121  |  0:09:22s\n",
      "epoch 58 | loss: 0.33322 | val_0_auc: 0.6122  |  0:09:30s\n",
      "epoch 59 | loss: 0.33108 | val_0_auc: 0.60965 |  0:09:38s\n",
      "epoch 60 | loss: 0.33228 | val_0_auc: 0.60008 |  0:09:47s\n",
      "epoch 61 | loss: 0.33193 | val_0_auc: 0.6019  |  0:09:57s\n",
      "epoch 62 | loss: 0.32998 | val_0_auc: 0.60898 |  0:10:07s\n",
      "epoch 63 | loss: 0.33076 | val_0_auc: 0.61522 |  0:10:18s\n",
      "epoch 64 | loss: 0.33166 | val_0_auc: 0.60899 |  0:10:27s\n",
      "epoch 65 | loss: 0.33388 | val_0_auc: 0.61093 |  0:10:36s\n",
      "epoch 66 | loss: 0.33328 | val_0_auc: 0.61473 |  0:10:46s\n",
      "epoch 67 | loss: 0.33436 | val_0_auc: 0.60965 |  0:10:57s\n",
      "epoch 68 | loss: 0.33074 | val_0_auc: 0.60815 |  0:11:06s\n",
      "epoch 69 | loss: 0.32899 | val_0_auc: 0.59844 |  0:11:14s\n",
      "epoch 70 | loss: 0.32946 | val_0_auc: 0.58571 |  0:11:22s\n",
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 55 and best_val_0_auc = 0.62106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:29:46,177] Trial 11 finished with value: 0.6210586685385258 and parameters: {'n_d': 64, 'n_a': 64, 'n_steps': 6, 'gamma': 2.480221051698598, 'lambda_sparse': 8.143353943470581e-05, 'lr': 0.0027299618140084404}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.65162 | val_0_auc: 0.48766 |  0:00:11s\n",
      "epoch 1  | loss: 0.5536  | val_0_auc: 0.51663 |  0:00:22s\n",
      "epoch 2  | loss: 0.52151 | val_0_auc: 0.52165 |  0:00:34s\n",
      "epoch 3  | loss: 0.48247 | val_0_auc: 0.52716 |  0:00:44s\n",
      "epoch 4  | loss: 0.45821 | val_0_auc: 0.52791 |  0:00:53s\n",
      "epoch 5  | loss: 0.43847 | val_0_auc: 0.54408 |  0:01:01s\n",
      "epoch 6  | loss: 0.43409 | val_0_auc: 0.51342 |  0:01:10s\n",
      "epoch 7  | loss: 0.41509 | val_0_auc: 0.51961 |  0:01:20s\n",
      "epoch 8  | loss: 0.40488 | val_0_auc: 0.52462 |  0:01:30s\n",
      "epoch 9  | loss: 0.3956  | val_0_auc: 0.52433 |  0:01:40s\n",
      "epoch 10 | loss: 0.3859  | val_0_auc: 0.54748 |  0:01:49s\n",
      "epoch 11 | loss: 0.37986 | val_0_auc: 0.5279  |  0:01:58s\n",
      "epoch 12 | loss: 0.37898 | val_0_auc: 0.53811 |  0:02:07s\n",
      "epoch 13 | loss: 0.3693  | val_0_auc: 0.55452 |  0:02:15s\n",
      "epoch 14 | loss: 0.36812 | val_0_auc: 0.56377 |  0:02:24s\n",
      "epoch 15 | loss: 0.37406 | val_0_auc: 0.54723 |  0:02:33s\n",
      "epoch 16 | loss: 0.36119 | val_0_auc: 0.56116 |  0:02:44s\n",
      "epoch 17 | loss: 0.36241 | val_0_auc: 0.55299 |  0:02:54s\n",
      "epoch 18 | loss: 0.36682 | val_0_auc: 0.57272 |  0:03:05s\n",
      "epoch 19 | loss: 0.36456 | val_0_auc: 0.60838 |  0:03:13s\n",
      "epoch 20 | loss: 0.36472 | val_0_auc: 0.60221 |  0:03:23s\n",
      "epoch 21 | loss: 0.35199 | val_0_auc: 0.57398 |  0:03:31s\n",
      "epoch 22 | loss: 0.35392 | val_0_auc: 0.59761 |  0:03:40s\n",
      "epoch 23 | loss: 0.35262 | val_0_auc: 0.57067 |  0:03:49s\n",
      "epoch 24 | loss: 0.35335 | val_0_auc: 0.59831 |  0:03:59s\n",
      "epoch 25 | loss: 0.3612  | val_0_auc: 0.55109 |  0:04:08s\n",
      "epoch 26 | loss: 0.35806 | val_0_auc: 0.60558 |  0:04:18s\n",
      "epoch 27 | loss: 0.35505 | val_0_auc: 0.57856 |  0:04:27s\n",
      "epoch 28 | loss: 0.35656 | val_0_auc: 0.57773 |  0:04:35s\n",
      "epoch 29 | loss: 0.35026 | val_0_auc: 0.59137 |  0:04:44s\n",
      "epoch 30 | loss: 0.35361 | val_0_auc: 0.56169 |  0:04:53s\n",
      "epoch 31 | loss: 0.3519  | val_0_auc: 0.5872  |  0:05:04s\n",
      "epoch 32 | loss: 0.34756 | val_0_auc: 0.59028 |  0:05:14s\n",
      "epoch 33 | loss: 0.34758 | val_0_auc: 0.58917 |  0:05:23s\n",
      "epoch 34 | loss: 0.34822 | val_0_auc: 0.58979 |  0:05:32s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 19 and best_val_0_auc = 0.60838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:35:22,668] Trial 12 finished with value: 0.6083831817534436 and parameters: {'n_d': 64, 'n_a': 64, 'n_steps': 6, 'gamma': 2.4138479750901247, 'lambda_sparse': 9.873739611866413e-05, 'lr': 0.002565470851486543}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.9328  | val_0_auc: 0.5093  |  0:00:07s\n",
      "epoch 1  | loss: 0.61444 | val_0_auc: 0.47947 |  0:00:14s\n",
      "epoch 2  | loss: 0.56648 | val_0_auc: 0.4729  |  0:00:21s\n",
      "epoch 3  | loss: 0.50898 | val_0_auc: 0.51162 |  0:00:30s\n",
      "epoch 4  | loss: 0.4547  | val_0_auc: 0.52258 |  0:00:38s\n",
      "epoch 5  | loss: 0.43712 | val_0_auc: 0.49567 |  0:00:47s\n",
      "epoch 6  | loss: 0.41078 | val_0_auc: 0.51288 |  0:00:57s\n",
      "epoch 7  | loss: 0.40943 | val_0_auc: 0.54068 |  0:01:04s\n",
      "epoch 8  | loss: 0.38939 | val_0_auc: 0.54055 |  0:01:12s\n",
      "epoch 9  | loss: 0.39121 | val_0_auc: 0.5216  |  0:01:19s\n",
      "epoch 10 | loss: 0.38075 | val_0_auc: 0.53692 |  0:01:27s\n",
      "epoch 11 | loss: 0.37872 | val_0_auc: 0.52542 |  0:01:35s\n",
      "epoch 12 | loss: 0.37862 | val_0_auc: 0.52997 |  0:01:44s\n",
      "epoch 13 | loss: 0.38184 | val_0_auc: 0.54664 |  0:01:53s\n",
      "epoch 14 | loss: 0.37601 | val_0_auc: 0.51207 |  0:02:02s\n",
      "epoch 15 | loss: 0.37356 | val_0_auc: 0.52398 |  0:02:09s\n",
      "epoch 16 | loss: 0.36661 | val_0_auc: 0.53578 |  0:02:16s\n",
      "epoch 17 | loss: 0.36568 | val_0_auc: 0.55563 |  0:02:24s\n",
      "epoch 18 | loss: 0.36932 | val_0_auc: 0.56493 |  0:02:32s\n",
      "epoch 19 | loss: 0.35563 | val_0_auc: 0.56832 |  0:02:40s\n",
      "epoch 20 | loss: 0.35277 | val_0_auc: 0.56184 |  0:02:47s\n",
      "epoch 21 | loss: 0.35891 | val_0_auc: 0.56497 |  0:02:54s\n",
      "epoch 22 | loss: 0.36076 | val_0_auc: 0.56273 |  0:03:02s\n",
      "epoch 23 | loss: 0.35778 | val_0_auc: 0.56112 |  0:03:10s\n",
      "epoch 24 | loss: 0.35412 | val_0_auc: 0.57591 |  0:03:19s\n",
      "epoch 25 | loss: 0.35456 | val_0_auc: 0.56388 |  0:03:28s\n",
      "epoch 26 | loss: 0.35363 | val_0_auc: 0.54823 |  0:03:37s\n",
      "epoch 27 | loss: 0.35056 | val_0_auc: 0.56503 |  0:03:45s\n",
      "epoch 28 | loss: 0.34667 | val_0_auc: 0.58419 |  0:03:52s\n",
      "epoch 29 | loss: 0.34556 | val_0_auc: 0.57527 |  0:03:59s\n",
      "epoch 30 | loss: 0.34875 | val_0_auc: 0.55064 |  0:04:06s\n",
      "epoch 31 | loss: 0.34904 | val_0_auc: 0.54103 |  0:04:13s\n",
      "epoch 32 | loss: 0.34906 | val_0_auc: 0.53945 |  0:04:21s\n",
      "epoch 33 | loss: 0.34619 | val_0_auc: 0.57541 |  0:04:30s\n",
      "epoch 34 | loss: 0.34222 | val_0_auc: 0.59441 |  0:04:39s\n",
      "epoch 35 | loss: 0.34342 | val_0_auc: 0.5609  |  0:04:48s\n",
      "epoch 36 | loss: 0.34417 | val_0_auc: 0.5809  |  0:04:56s\n",
      "epoch 37 | loss: 0.33892 | val_0_auc: 0.57317 |  0:05:04s\n",
      "epoch 38 | loss: 0.34148 | val_0_auc: 0.5941  |  0:05:13s\n",
      "epoch 39 | loss: 0.33863 | val_0_auc: 0.5854  |  0:05:21s\n",
      "epoch 40 | loss: 0.33965 | val_0_auc: 0.57211 |  0:05:29s\n",
      "epoch 41 | loss: 0.33685 | val_0_auc: 0.56197 |  0:05:36s\n",
      "epoch 42 | loss: 0.33861 | val_0_auc: 0.58183 |  0:05:44s\n",
      "epoch 43 | loss: 0.33602 | val_0_auc: 0.59276 |  0:05:52s\n",
      "epoch 44 | loss: 0.33628 | val_0_auc: 0.5855  |  0:06:00s\n",
      "epoch 45 | loss: 0.33764 | val_0_auc: 0.57411 |  0:06:07s\n",
      "epoch 46 | loss: 0.3374  | val_0_auc: 0.58047 |  0:06:14s\n",
      "epoch 47 | loss: 0.33387 | val_0_auc: 0.56964 |  0:06:22s\n",
      "epoch 48 | loss: 0.33353 | val_0_auc: 0.58685 |  0:06:31s\n",
      "epoch 49 | loss: 0.33537 | val_0_auc: 0.58338 |  0:06:40s\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 34 and best_val_0_auc = 0.59441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:42:06,803] Trial 13 finished with value: 0.5944050234999695 and parameters: {'n_d': 63, 'n_a': 64, 'n_steps': 5, 'gamma': 2.4753238097564956, 'lambda_sparse': 7.306239991212684e-05, 'lr': 0.002917233043534821}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.28658 | val_0_auc: 0.53835 |  0:00:08s\n",
      "epoch 1  | loss: 5.18445 | val_0_auc: 0.52462 |  0:00:17s\n",
      "epoch 2  | loss: 5.01522 | val_0_auc: 0.51895 |  0:00:26s\n",
      "epoch 3  | loss: 4.88107 | val_0_auc: 0.51808 |  0:00:35s\n",
      "epoch 4  | loss: 4.78742 | val_0_auc: 0.5175  |  0:00:45s\n",
      "epoch 5  | loss: 4.62334 | val_0_auc: 0.51803 |  0:00:53s\n",
      "epoch 6  | loss: 4.51833 | val_0_auc: 0.51861 |  0:01:01s\n",
      "epoch 7  | loss: 4.35087 | val_0_auc: 0.51127 |  0:01:09s\n",
      "epoch 8  | loss: 4.30351 | val_0_auc: 0.52208 |  0:01:17s\n",
      "epoch 9  | loss: 4.22816 | val_0_auc: 0.5147  |  0:01:25s\n",
      "epoch 10 | loss: 4.09013 | val_0_auc: 0.53428 |  0:01:33s\n",
      "epoch 11 | loss: 4.00058 | val_0_auc: 0.52054 |  0:01:42s\n",
      "epoch 12 | loss: 3.88523 | val_0_auc: 0.51206 |  0:01:52s\n",
      "epoch 13 | loss: 3.79267 | val_0_auc: 0.5158  |  0:02:02s\n",
      "epoch 14 | loss: 3.67131 | val_0_auc: 0.51364 |  0:02:11s\n",
      "epoch 15 | loss: 3.55497 | val_0_auc: 0.51891 |  0:02:19s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_0_auc = 0.53835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:44:29,960] Trial 14 finished with value: 0.5383537813587255 and parameters: {'n_d': 59, 'n_a': 54, 'n_steps': 6, 'gamma': 2.0030993226791036, 'lambda_sparse': 1.6641541680307846e-05, 'lr': 0.00010469268925869643}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.81069 | val_0_auc: 0.47748 |  0:00:08s\n",
      "epoch 1  | loss: 0.65747 | val_0_auc: 0.44655 |  0:00:18s\n",
      "epoch 2  | loss: 0.60194 | val_0_auc: 0.4495  |  0:00:29s\n",
      "epoch 3  | loss: 0.58407 | val_0_auc: 0.45842 |  0:00:40s\n",
      "epoch 4  | loss: 0.55962 | val_0_auc: 0.45056 |  0:00:49s\n",
      "epoch 5  | loss: 0.53366 | val_0_auc: 0.46763 |  0:00:58s\n",
      "epoch 6  | loss: 0.50811 | val_0_auc: 0.47123 |  0:01:07s\n",
      "epoch 7  | loss: 0.48915 | val_0_auc: 0.47501 |  0:01:16s\n",
      "epoch 8  | loss: 0.48077 | val_0_auc: 0.48694 |  0:01:26s\n",
      "epoch 9  | loss: 0.47388 | val_0_auc: 0.49477 |  0:01:36s\n",
      "epoch 10 | loss: 0.45602 | val_0_auc: 0.49197 |  0:01:46s\n",
      "epoch 11 | loss: 0.44374 | val_0_auc: 0.5043  |  0:01:57s\n",
      "epoch 12 | loss: 0.43038 | val_0_auc: 0.50627 |  0:02:07s\n",
      "epoch 13 | loss: 0.42539 | val_0_auc: 0.49843 |  0:02:17s\n",
      "epoch 14 | loss: 0.41138 | val_0_auc: 0.49268 |  0:02:26s\n",
      "epoch 15 | loss: 0.4104  | val_0_auc: 0.50834 |  0:02:36s\n",
      "epoch 16 | loss: 0.40335 | val_0_auc: 0.52267 |  0:02:46s\n",
      "epoch 17 | loss: 0.40632 | val_0_auc: 0.51149 |  0:02:56s\n",
      "epoch 18 | loss: 0.39658 | val_0_auc: 0.53696 |  0:03:06s\n",
      "epoch 19 | loss: 0.39787 | val_0_auc: 0.53292 |  0:03:16s\n",
      "epoch 20 | loss: 0.39112 | val_0_auc: 0.51279 |  0:03:26s\n",
      "epoch 21 | loss: 0.39422 | val_0_auc: 0.5298  |  0:03:37s\n",
      "epoch 22 | loss: 0.3815  | val_0_auc: 0.51264 |  0:03:47s\n",
      "epoch 23 | loss: 0.384   | val_0_auc: 0.53305 |  0:03:56s\n",
      "epoch 24 | loss: 0.3805  | val_0_auc: 0.52256 |  0:04:05s\n",
      "epoch 25 | loss: 0.37881 | val_0_auc: 0.53767 |  0:04:15s\n",
      "epoch 26 | loss: 0.37386 | val_0_auc: 0.50923 |  0:04:24s\n",
      "epoch 27 | loss: 0.39134 | val_0_auc: 0.52199 |  0:04:34s\n",
      "epoch 28 | loss: 0.37689 | val_0_auc: 0.527   |  0:04:44s\n",
      "epoch 29 | loss: 0.37288 | val_0_auc: 0.52139 |  0:04:54s\n",
      "epoch 30 | loss: 0.36879 | val_0_auc: 0.51977 |  0:05:05s\n",
      "epoch 31 | loss: 0.36415 | val_0_auc: 0.51593 |  0:05:16s\n",
      "epoch 32 | loss: 0.35938 | val_0_auc: 0.5367  |  0:05:27s\n",
      "epoch 33 | loss: 0.36321 | val_0_auc: 0.53108 |  0:05:37s\n",
      "epoch 34 | loss: 0.35836 | val_0_auc: 0.53926 |  0:05:46s\n",
      "epoch 35 | loss: 0.36089 | val_0_auc: 0.53651 |  0:05:55s\n",
      "epoch 36 | loss: 0.35992 | val_0_auc: 0.54583 |  0:06:04s\n",
      "epoch 37 | loss: 0.35838 | val_0_auc: 0.54636 |  0:06:12s\n",
      "epoch 38 | loss: 0.36073 | val_0_auc: 0.55944 |  0:06:22s\n",
      "epoch 39 | loss: 0.35273 | val_0_auc: 0.56116 |  0:06:33s\n",
      "epoch 40 | loss: 0.34822 | val_0_auc: 0.58087 |  0:06:44s\n",
      "epoch 41 | loss: 0.35034 | val_0_auc: 0.57176 |  0:06:55s\n",
      "epoch 42 | loss: 0.35348 | val_0_auc: 0.56826 |  0:07:06s\n",
      "epoch 43 | loss: 0.34987 | val_0_auc: 0.55992 |  0:07:15s\n",
      "epoch 44 | loss: 0.34773 | val_0_auc: 0.56985 |  0:07:26s\n",
      "epoch 45 | loss: 0.34879 | val_0_auc: 0.59094 |  0:07:36s\n",
      "epoch 46 | loss: 0.34614 | val_0_auc: 0.58332 |  0:07:46s\n",
      "epoch 47 | loss: 0.34321 | val_0_auc: 0.57982 |  0:07:54s\n",
      "epoch 48 | loss: 0.34568 | val_0_auc: 0.57363 |  0:08:03s\n",
      "epoch 49 | loss: 0.34439 | val_0_auc: 0.58808 |  0:08:12s\n",
      "epoch 50 | loss: 0.3454  | val_0_auc: 0.58743 |  0:08:22s\n",
      "epoch 51 | loss: 0.34365 | val_0_auc: 0.57127 |  0:08:32s\n",
      "epoch 52 | loss: 0.34683 | val_0_auc: 0.57237 |  0:08:43s\n",
      "epoch 53 | loss: 0.34271 | val_0_auc: 0.5754  |  0:08:54s\n",
      "epoch 54 | loss: 0.34314 | val_0_auc: 0.59189 |  0:09:03s\n",
      "epoch 55 | loss: 0.34133 | val_0_auc: 0.60141 |  0:09:12s\n",
      "epoch 56 | loss: 0.33958 | val_0_auc: 0.5912  |  0:09:21s\n",
      "epoch 57 | loss: 0.34012 | val_0_auc: 0.58663 |  0:09:32s\n",
      "epoch 58 | loss: 0.33853 | val_0_auc: 0.57253 |  0:09:43s\n",
      "epoch 59 | loss: 0.34377 | val_0_auc: 0.56348 |  0:09:54s\n",
      "epoch 60 | loss: 0.34016 | val_0_auc: 0.58527 |  0:10:03s\n",
      "epoch 61 | loss: 0.34352 | val_0_auc: 0.57655 |  0:10:11s\n",
      "epoch 62 | loss: 0.3368  | val_0_auc: 0.59447 |  0:10:21s\n",
      "epoch 63 | loss: 0.34547 | val_0_auc: 0.58744 |  0:10:31s\n",
      "epoch 64 | loss: 0.34544 | val_0_auc: 0.59882 |  0:10:41s\n",
      "epoch 65 | loss: 0.34277 | val_0_auc: 0.59035 |  0:10:52s\n",
      "epoch 66 | loss: 0.34055 | val_0_auc: 0.58017 |  0:11:02s\n",
      "epoch 67 | loss: 0.3399  | val_0_auc: 0.58647 |  0:11:12s\n",
      "epoch 68 | loss: 0.33705 | val_0_auc: 0.58358 |  0:11:21s\n",
      "epoch 69 | loss: 0.3365  | val_0_auc: 0.59765 |  0:11:31s\n",
      "epoch 70 | loss: 0.33614 | val_0_auc: 0.59856 |  0:11:41s\n",
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 55 and best_val_0_auc = 0.60141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 23:56:17,049] Trial 15 finished with value: 0.601414067427618 and parameters: {'n_d': 34, 'n_a': 56, 'n_steps': 7, 'gamma': 2.222935664985713, 'lambda_sparse': 0.0001553865413626261, 'lr': 0.001738198641445551}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.17215 | val_0_auc: 0.51739 |  0:00:07s\n",
      "epoch 1  | loss: 0.87744 | val_0_auc: 0.49483 |  0:00:13s\n",
      "epoch 2  | loss: 0.51266 | val_0_auc: 0.49318 |  0:00:20s\n",
      "epoch 3  | loss: 0.49    | val_0_auc: 0.49083 |  0:00:26s\n",
      "epoch 4  | loss: 0.4588  | val_0_auc: 0.51513 |  0:00:32s\n",
      "epoch 5  | loss: 0.42277 | val_0_auc: 0.51229 |  0:00:39s\n",
      "epoch 6  | loss: 0.41986 | val_0_auc: 0.50336 |  0:00:45s\n",
      "epoch 7  | loss: 0.39728 | val_0_auc: 0.51225 |  0:00:52s\n",
      "epoch 8  | loss: 0.39793 | val_0_auc: 0.51401 |  0:00:59s\n",
      "epoch 9  | loss: 0.38526 | val_0_auc: 0.52342 |  0:01:07s\n",
      "epoch 10 | loss: 0.38233 | val_0_auc: 0.53808 |  0:01:14s\n",
      "epoch 11 | loss: 0.37756 | val_0_auc: 0.54225 |  0:01:21s\n",
      "epoch 12 | loss: 0.36972 | val_0_auc: 0.54905 |  0:01:28s\n",
      "epoch 13 | loss: 0.36763 | val_0_auc: 0.54987 |  0:01:35s\n",
      "epoch 14 | loss: 0.36812 | val_0_auc: 0.53164 |  0:01:41s\n",
      "epoch 15 | loss: 0.36047 | val_0_auc: 0.54283 |  0:01:48s\n",
      "epoch 16 | loss: 0.36518 | val_0_auc: 0.54248 |  0:01:54s\n",
      "epoch 17 | loss: 0.35886 | val_0_auc: 0.55096 |  0:02:00s\n",
      "epoch 18 | loss: 0.36486 | val_0_auc: 0.54644 |  0:02:06s\n",
      "epoch 19 | loss: 0.36045 | val_0_auc: 0.55753 |  0:02:13s\n",
      "epoch 20 | loss: 0.35467 | val_0_auc: 0.56803 |  0:02:20s\n",
      "epoch 21 | loss: 0.35504 | val_0_auc: 0.56393 |  0:02:27s\n",
      "epoch 22 | loss: 0.35333 | val_0_auc: 0.5721  |  0:02:35s\n",
      "epoch 23 | loss: 0.35261 | val_0_auc: 0.56942 |  0:02:42s\n",
      "epoch 24 | loss: 0.3447  | val_0_auc: 0.57891 |  0:02:49s\n",
      "epoch 25 | loss: 0.3453  | val_0_auc: 0.57937 |  0:02:57s\n",
      "epoch 26 | loss: 0.34925 | val_0_auc: 0.58327 |  0:03:03s\n",
      "epoch 27 | loss: 0.34982 | val_0_auc: 0.57721 |  0:03:10s\n",
      "epoch 28 | loss: 0.34526 | val_0_auc: 0.57519 |  0:03:16s\n",
      "epoch 29 | loss: 0.34467 | val_0_auc: 0.55739 |  0:03:22s\n",
      "epoch 30 | loss: 0.34383 | val_0_auc: 0.55174 |  0:03:28s\n",
      "epoch 31 | loss: 0.34513 | val_0_auc: 0.55562 |  0:03:35s\n",
      "epoch 32 | loss: 0.34547 | val_0_auc: 0.55124 |  0:03:41s\n",
      "epoch 33 | loss: 0.34364 | val_0_auc: 0.56074 |  0:03:48s\n",
      "epoch 34 | loss: 0.34167 | val_0_auc: 0.57463 |  0:03:55s\n",
      "epoch 35 | loss: 0.34134 | val_0_auc: 0.57032 |  0:04:02s\n",
      "epoch 36 | loss: 0.3421  | val_0_auc: 0.57165 |  0:04:10s\n",
      "epoch 37 | loss: 0.3387  | val_0_auc: 0.55007 |  0:04:17s\n",
      "epoch 38 | loss: 0.34085 | val_0_auc: 0.56061 |  0:04:24s\n",
      "epoch 39 | loss: 0.33936 | val_0_auc: 0.57792 |  0:04:31s\n",
      "epoch 40 | loss: 0.34067 | val_0_auc: 0.56811 |  0:04:37s\n",
      "epoch 41 | loss: 0.33925 | val_0_auc: 0.5666  |  0:04:43s\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 26 and best_val_0_auc = 0.58327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 00:01:04,044] Trial 16 finished with value: 0.5832706413151845 and parameters: {'n_d': 58, 'n_a': 51, 'n_steps': 4, 'gamma': 2.4884014755878505, 'lambda_sparse': 4.375956240811789e-05, 'lr': 0.002059391491206233}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.76513 | val_0_auc: 0.51115 |  0:00:08s\n",
      "epoch 1  | loss: 3.37358 | val_0_auc: 0.52976 |  0:00:17s\n",
      "epoch 2  | loss: 2.98371 | val_0_auc: 0.5252  |  0:00:26s\n",
      "epoch 3  | loss: 2.63825 | val_0_auc: 0.51505 |  0:00:35s\n",
      "epoch 4  | loss: 2.29832 | val_0_auc: 0.51826 |  0:00:44s\n",
      "epoch 5  | loss: 1.96881 | val_0_auc: 0.50375 |  0:00:53s\n",
      "epoch 6  | loss: 1.67574 | val_0_auc: 0.51812 |  0:01:00s\n",
      "epoch 7  | loss: 1.42981 | val_0_auc: 0.48876 |  0:01:08s\n",
      "epoch 8  | loss: 1.21553 | val_0_auc: 0.50372 |  0:01:15s\n",
      "epoch 9  | loss: 1.07149 | val_0_auc: 0.48288 |  0:01:23s\n",
      "epoch 10 | loss: 0.93372 | val_0_auc: 0.46883 |  0:01:30s\n",
      "epoch 11 | loss: 0.81544 | val_0_auc: 0.4727  |  0:01:37s\n",
      "epoch 12 | loss: 0.70646 | val_0_auc: 0.46808 |  0:01:44s\n",
      "epoch 13 | loss: 0.63999 | val_0_auc: 0.47842 |  0:01:53s\n",
      "epoch 14 | loss: 0.57498 | val_0_auc: 0.46184 |  0:02:02s\n",
      "epoch 15 | loss: 0.53813 | val_0_auc: 0.47455 |  0:02:11s\n",
      "epoch 16 | loss: 0.49716 | val_0_auc: 0.47424 |  0:02:19s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 1 and best_val_0_auc = 0.52976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 00:03:28,454] Trial 17 finished with value: 0.5297591253128243 and parameters: {'n_d': 17, 'n_a': 60, 'n_steps': 6, 'gamma': 2.264261660468291, 'lambda_sparse': 6.679911562451356e-06, 'lr': 0.0008510412773232046}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.6758  | val_0_auc: 0.4946  |  0:00:06s\n",
      "epoch 1  | loss: 0.74755 | val_0_auc: 0.48032 |  0:00:13s\n",
      "epoch 2  | loss: 0.53474 | val_0_auc: 0.46702 |  0:00:19s\n",
      "epoch 3  | loss: 0.51511 | val_0_auc: 0.47703 |  0:00:25s\n",
      "epoch 4  | loss: 0.47686 | val_0_auc: 0.50211 |  0:00:31s\n",
      "epoch 5  | loss: 0.4348  | val_0_auc: 0.50612 |  0:00:37s\n",
      "epoch 6  | loss: 0.4336  | val_0_auc: 0.51679 |  0:00:44s\n",
      "epoch 7  | loss: 0.41779 | val_0_auc: 0.51725 |  0:00:50s\n",
      "epoch 8  | loss: 0.40644 | val_0_auc: 0.52111 |  0:00:55s\n",
      "epoch 9  | loss: 0.39719 | val_0_auc: 0.52514 |  0:01:01s\n",
      "epoch 10 | loss: 0.3984  | val_0_auc: 0.50159 |  0:01:08s\n",
      "epoch 11 | loss: 0.39366 | val_0_auc: 0.51786 |  0:01:14s\n",
      "epoch 12 | loss: 0.39088 | val_0_auc: 0.52536 |  0:01:21s\n",
      "epoch 13 | loss: 0.38835 | val_0_auc: 0.50655 |  0:01:28s\n",
      "epoch 14 | loss: 0.37777 | val_0_auc: 0.51871 |  0:01:36s\n",
      "epoch 15 | loss: 0.37894 | val_0_auc: 0.51311 |  0:01:42s\n",
      "epoch 16 | loss: 0.37233 | val_0_auc: 0.52236 |  0:01:48s\n",
      "epoch 17 | loss: 0.3723  | val_0_auc: 0.55772 |  0:01:54s\n",
      "epoch 18 | loss: 0.37628 | val_0_auc: 0.55675 |  0:02:01s\n",
      "epoch 19 | loss: 0.37358 | val_0_auc: 0.56056 |  0:02:06s\n",
      "epoch 20 | loss: 0.36398 | val_0_auc: 0.56022 |  0:02:12s\n",
      "epoch 21 | loss: 0.36232 | val_0_auc: 0.54243 |  0:02:17s\n",
      "epoch 22 | loss: 0.36743 | val_0_auc: 0.53154 |  0:02:24s\n",
      "epoch 23 | loss: 0.35739 | val_0_auc: 0.54594 |  0:02:30s\n",
      "epoch 24 | loss: 0.35922 | val_0_auc: 0.55327 |  0:02:37s\n",
      "epoch 25 | loss: 0.35851 | val_0_auc: 0.55501 |  0:02:43s\n",
      "epoch 26 | loss: 0.35808 | val_0_auc: 0.56397 |  0:02:50s\n",
      "epoch 27 | loss: 0.35107 | val_0_auc: 0.55567 |  0:02:57s\n",
      "epoch 28 | loss: 0.35593 | val_0_auc: 0.56999 |  0:03:04s\n",
      "epoch 29 | loss: 0.35169 | val_0_auc: 0.56378 |  0:03:10s\n",
      "epoch 30 | loss: 0.35004 | val_0_auc: 0.5732  |  0:03:16s\n",
      "epoch 31 | loss: 0.34998 | val_0_auc: 0.57351 |  0:03:22s\n",
      "epoch 32 | loss: 0.34919 | val_0_auc: 0.57852 |  0:03:27s\n",
      "epoch 33 | loss: 0.35173 | val_0_auc: 0.57521 |  0:03:33s\n",
      "epoch 34 | loss: 0.34473 | val_0_auc: 0.58839 |  0:03:39s\n",
      "epoch 35 | loss: 0.34343 | val_0_auc: 0.58507 |  0:03:45s\n",
      "epoch 36 | loss: 0.3466  | val_0_auc: 0.59093 |  0:03:50s\n",
      "epoch 37 | loss: 0.34372 | val_0_auc: 0.5907  |  0:03:56s\n",
      "epoch 38 | loss: 0.34228 | val_0_auc: 0.59138 |  0:04:03s\n",
      "epoch 39 | loss: 0.34707 | val_0_auc: 0.58761 |  0:04:10s\n",
      "epoch 40 | loss: 0.34128 | val_0_auc: 0.59439 |  0:04:16s\n",
      "epoch 41 | loss: 0.34116 | val_0_auc: 0.59465 |  0:04:23s\n",
      "epoch 42 | loss: 0.33985 | val_0_auc: 0.60584 |  0:04:29s\n",
      "epoch 43 | loss: 0.33557 | val_0_auc: 0.61108 |  0:04:35s\n",
      "epoch 44 | loss: 0.33746 | val_0_auc: 0.60213 |  0:04:41s\n",
      "epoch 45 | loss: 0.33855 | val_0_auc: 0.59166 |  0:04:47s\n",
      "epoch 46 | loss: 0.33526 | val_0_auc: 0.6029  |  0:04:52s\n",
      "epoch 47 | loss: 0.33459 | val_0_auc: 0.59414 |  0:04:58s\n",
      "epoch 48 | loss: 0.33658 | val_0_auc: 0.58633 |  0:05:04s\n",
      "epoch 49 | loss: 0.335   | val_0_auc: 0.59587 |  0:05:10s\n",
      "epoch 50 | loss: 0.33287 | val_0_auc: 0.59711 |  0:05:17s\n",
      "epoch 51 | loss: 0.33518 | val_0_auc: 0.5982  |  0:05:23s\n",
      "epoch 52 | loss: 0.33396 | val_0_auc: 0.59944 |  0:05:30s\n",
      "epoch 53 | loss: 0.33159 | val_0_auc: 0.60492 |  0:05:36s\n",
      "epoch 54 | loss: 0.33035 | val_0_auc: 0.60286 |  0:05:42s\n",
      "epoch 55 | loss: 0.33001 | val_0_auc: 0.6055  |  0:05:48s\n",
      "epoch 56 | loss: 0.33047 | val_0_auc: 0.5951  |  0:05:53s\n",
      "epoch 57 | loss: 0.32942 | val_0_auc: 0.60029 |  0:05:59s\n",
      "epoch 58 | loss: 0.32863 | val_0_auc: 0.60422 |  0:06:05s\n",
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 43 and best_val_0_auc = 0.61108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 00:09:37,050] Trial 18 finished with value: 0.6110824991352825 and parameters: {'n_d': 58, 'n_a': 31, 'n_steps': 4, 'gamma': 1.8928097840835036, 'lambda_sparse': 1.0244628039297264e-06, 'lr': 0.0020720336457667235}. Best is trial 10 with value: 0.6265174520336121.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.82149 | val_0_auc: 0.52359 |  0:00:08s\n",
      "epoch 1  | loss: 1.21788 | val_0_auc: 0.5151  |  0:00:17s\n",
      "epoch 2  | loss: 0.80877 | val_0_auc: 0.50251 |  0:00:24s\n",
      "epoch 3  | loss: 0.59536 | val_0_auc: 0.49983 |  0:00:32s\n",
      "epoch 4  | loss: 0.51049 | val_0_auc: 0.49599 |  0:00:40s\n",
      "epoch 5  | loss: 0.48689 | val_0_auc: 0.49031 |  0:00:49s\n",
      "epoch 6  | loss: 0.48031 | val_0_auc: 0.50019 |  0:00:57s\n",
      "epoch 7  | loss: 0.4626  | val_0_auc: 0.50598 |  0:01:04s\n",
      "epoch 8  | loss: 0.45366 | val_0_auc: 0.50452 |  0:01:11s\n",
      "epoch 9  | loss: 0.44697 | val_0_auc: 0.51695 |  0:01:18s\n",
      "epoch 10 | loss: 0.43982 | val_0_auc: 0.52162 |  0:01:26s\n",
      "epoch 11 | loss: 0.43031 | val_0_auc: 0.53155 |  0:01:34s\n",
      "epoch 12 | loss: 0.42957 | val_0_auc: 0.52063 |  0:01:42s\n",
      "epoch 13 | loss: 0.41524 | val_0_auc: 0.52292 |  0:01:50s\n",
      "epoch 14 | loss: 0.41241 | val_0_auc: 0.51126 |  0:01:58s\n",
      "epoch 15 | loss: 0.39821 | val_0_auc: 0.51679 |  0:02:05s\n",
      "epoch 16 | loss: 0.40362 | val_0_auc: 0.52531 |  0:02:12s\n",
      "epoch 17 | loss: 0.3944  | val_0_auc: 0.53251 |  0:02:19s\n",
      "epoch 18 | loss: 0.38522 | val_0_auc: 0.53176 |  0:02:27s\n",
      "epoch 19 | loss: 0.39576 | val_0_auc: 0.52554 |  0:02:35s\n",
      "epoch 20 | loss: 0.38792 | val_0_auc: 0.51533 |  0:02:43s\n",
      "epoch 21 | loss: 0.38958 | val_0_auc: 0.50619 |  0:02:52s\n",
      "epoch 22 | loss: 0.38395 | val_0_auc: 0.51768 |  0:03:00s\n",
      "epoch 23 | loss: 0.37778 | val_0_auc: 0.52897 |  0:03:07s\n",
      "epoch 24 | loss: 0.37823 | val_0_auc: 0.53465 |  0:03:13s\n",
      "epoch 25 | loss: 0.37703 | val_0_auc: 0.54581 |  0:03:20s\n",
      "epoch 26 | loss: 0.37296 | val_0_auc: 0.54086 |  0:03:28s\n",
      "epoch 27 | loss: 0.37038 | val_0_auc: 0.54609 |  0:03:36s\n",
      "epoch 28 | loss: 0.38017 | val_0_auc: 0.5426  |  0:03:44s\n",
      "epoch 29 | loss: 0.36693 | val_0_auc: 0.5365  |  0:03:52s\n",
      "epoch 30 | loss: 0.37019 | val_0_auc: 0.54307 |  0:04:00s\n",
      "epoch 31 | loss: 0.36258 | val_0_auc: 0.54462 |  0:04:07s\n",
      "epoch 32 | loss: 0.36599 | val_0_auc: 0.56603 |  0:04:15s\n",
      "epoch 33 | loss: 0.3657  | val_0_auc: 0.54904 |  0:04:23s\n",
      "epoch 34 | loss: 0.36626 | val_0_auc: 0.55629 |  0:04:31s\n",
      "epoch 35 | loss: 0.36397 | val_0_auc: 0.55468 |  0:04:39s\n",
      "epoch 36 | loss: 0.36543 | val_0_auc: 0.55837 |  0:04:47s\n",
      "epoch 37 | loss: 0.36298 | val_0_auc: 0.55485 |  0:04:55s\n",
      "epoch 38 | loss: 0.36022 | val_0_auc: 0.55989 |  0:05:04s\n",
      "epoch 39 | loss: 0.36238 | val_0_auc: 0.57167 |  0:05:12s\n",
      "epoch 40 | loss: 0.35968 | val_0_auc: 0.56263 |  0:05:20s\n",
      "epoch 41 | loss: 0.36249 | val_0_auc: 0.55715 |  0:05:28s\n",
      "epoch 42 | loss: 0.36197 | val_0_auc: 0.55547 |  0:05:34s\n",
      "epoch 43 | loss: 0.35879 | val_0_auc: 0.55886 |  0:05:42s\n",
      "epoch 44 | loss: 0.36106 | val_0_auc: 0.55958 |  0:05:49s\n",
      "epoch 45 | loss: 0.35946 | val_0_auc: 0.55567 |  0:05:57s\n",
      "epoch 46 | loss: 0.35777 | val_0_auc: 0.56604 |  0:06:05s\n",
      "epoch 47 | loss: 0.35745 | val_0_auc: 0.56206 |  0:06:14s\n",
      "epoch 48 | loss: 0.35555 | val_0_auc: 0.56512 |  0:06:22s\n",
      "epoch 49 | loss: 0.35257 | val_0_auc: 0.57329 |  0:06:30s\n",
      "epoch 50 | loss: 0.35484 | val_0_auc: 0.56321 |  0:06:38s\n",
      "epoch 51 | loss: 0.35205 | val_0_auc: 0.55672 |  0:06:45s\n",
      "epoch 52 | loss: 0.35108 | val_0_auc: 0.56087 |  0:06:52s\n",
      "epoch 53 | loss: 0.35112 | val_0_auc: 0.5728  |  0:06:59s\n",
      "epoch 54 | loss: 0.34713 | val_0_auc: 0.57404 |  0:07:07s\n",
      "epoch 55 | loss: 0.34913 | val_0_auc: 0.55955 |  0:07:15s\n",
      "epoch 56 | loss: 0.35152 | val_0_auc: 0.57567 |  0:07:24s\n",
      "epoch 57 | loss: 0.35069 | val_0_auc: 0.57763 |  0:07:32s\n",
      "epoch 58 | loss: 0.34908 | val_0_auc: 0.57601 |  0:07:41s\n",
      "epoch 59 | loss: 0.34717 | val_0_auc: 0.57111 |  0:07:49s\n",
      "epoch 60 | loss: 0.3505  | val_0_auc: 0.56231 |  0:07:58s\n",
      "epoch 61 | loss: 0.3456  | val_0_auc: 0.56958 |  0:08:05s\n",
      "epoch 62 | loss: 0.34607 | val_0_auc: 0.57069 |  0:08:12s\n",
      "epoch 63 | loss: 0.34886 | val_0_auc: 0.56944 |  0:08:19s\n",
      "epoch 64 | loss: 0.34887 | val_0_auc: 0.58065 |  0:08:26s\n",
      "epoch 65 | loss: 0.34713 | val_0_auc: 0.57007 |  0:08:33s\n",
      "epoch 66 | loss: 0.3491  | val_0_auc: 0.57221 |  0:08:41s\n",
      "epoch 67 | loss: 0.34401 | val_0_auc: 0.58432 |  0:08:49s\n",
      "epoch 68 | loss: 0.3432  | val_0_auc: 0.58364 |  0:08:57s\n",
      "epoch 69 | loss: 0.34689 | val_0_auc: 0.58104 |  0:09:05s\n",
      "epoch 70 | loss: 0.34518 | val_0_auc: 0.57799 |  0:09:13s\n",
      "epoch 71 | loss: 0.34904 | val_0_auc: 0.58863 |  0:09:21s\n",
      "epoch 72 | loss: 0.34355 | val_0_auc: 0.58926 |  0:09:29s\n",
      "epoch 73 | loss: 0.3457  | val_0_auc: 0.59235 |  0:09:37s\n",
      "epoch 74 | loss: 0.34347 | val_0_auc: 0.58133 |  0:09:44s\n",
      "epoch 75 | loss: 0.3455  | val_0_auc: 0.59099 |  0:09:51s\n",
      "epoch 76 | loss: 0.34413 | val_0_auc: 0.58663 |  0:09:58s\n",
      "epoch 77 | loss: 0.3438  | val_0_auc: 0.57875 |  0:10:05s\n",
      "epoch 78 | loss: 0.34293 | val_0_auc: 0.59994 |  0:10:14s\n",
      "epoch 79 | loss: 0.34329 | val_0_auc: 0.58324 |  0:10:22s\n",
      "epoch 80 | loss: 0.34258 | val_0_auc: 0.57844 |  0:10:31s\n",
      "epoch 81 | loss: 0.34193 | val_0_auc: 0.58566 |  0:10:39s\n",
      "epoch 82 | loss: 0.34192 | val_0_auc: 0.58129 |  0:10:47s\n",
      "epoch 83 | loss: 0.34149 | val_0_auc: 0.58446 |  0:10:55s\n",
      "epoch 84 | loss: 0.34166 | val_0_auc: 0.58614 |  0:11:03s\n",
      "epoch 85 | loss: 0.34117 | val_0_auc: 0.5738  |  0:11:11s\n",
      "epoch 86 | loss: 0.34371 | val_0_auc: 0.57984 |  0:11:20s\n",
      "epoch 87 | loss: 0.34215 | val_0_auc: 0.59052 |  0:11:29s\n",
      "epoch 88 | loss: 0.34336 | val_0_auc: 0.58979 |  0:11:35s\n",
      "epoch 89 | loss: 0.33906 | val_0_auc: 0.59635 |  0:11:43s\n",
      "epoch 90 | loss: 0.34275 | val_0_auc: 0.58977 |  0:11:50s\n",
      "epoch 91 | loss: 0.33903 | val_0_auc: 0.59377 |  0:11:57s\n",
      "epoch 92 | loss: 0.34071 | val_0_auc: 0.60185 |  0:12:04s\n",
      "epoch 93 | loss: 0.33989 | val_0_auc: 0.59321 |  0:12:12s\n",
      "epoch 94 | loss: 0.34067 | val_0_auc: 0.589   |  0:12:20s\n",
      "epoch 95 | loss: 0.3351  | val_0_auc: 0.60154 |  0:12:28s\n",
      "epoch 96 | loss: 0.33692 | val_0_auc: 0.60116 |  0:12:36s\n",
      "epoch 97 | loss: 0.33594 | val_0_auc: 0.59555 |  0:12:43s\n",
      "epoch 98 | loss: 0.34019 | val_0_auc: 0.60369 |  0:12:50s\n",
      "epoch 99 | loss: 0.33754 | val_0_auc: 0.59838 |  0:12:57s\n",
      "epoch 100| loss: 0.33853 | val_0_auc: 0.58397 |  0:13:05s\n",
      "epoch 101| loss: 0.33513 | val_0_auc: 0.58376 |  0:13:13s\n",
      "epoch 102| loss: 0.33878 | val_0_auc: 0.58553 |  0:13:21s\n",
      "epoch 103| loss: 0.33512 | val_0_auc: 0.59615 |  0:13:28s\n",
      "epoch 104| loss: 0.33736 | val_0_auc: 0.59479 |  0:13:36s\n",
      "epoch 105| loss: 0.33902 | val_0_auc: 0.58855 |  0:13:45s\n",
      "epoch 106| loss: 0.33524 | val_0_auc: 0.58109 |  0:13:53s\n",
      "epoch 107| loss: 0.33685 | val_0_auc: 0.59235 |  0:14:00s\n",
      "epoch 108| loss: 0.33793 | val_0_auc: 0.5859  |  0:14:06s\n",
      "epoch 109| loss: 0.33804 | val_0_auc: 0.58817 |  0:14:14s\n",
      "epoch 110| loss: 0.3376  | val_0_auc: 0.5978  |  0:14:22s\n",
      "epoch 111| loss: 0.33439 | val_0_auc: 0.59965 |  0:14:31s\n",
      "epoch 112| loss: 0.33371 | val_0_auc: 0.59523 |  0:14:40s\n",
      "epoch 113| loss: 0.33641 | val_0_auc: 0.61181 |  0:14:48s\n",
      "epoch 114| loss: 0.33773 | val_0_auc: 0.59847 |  0:14:56s\n",
      "epoch 115| loss: 0.33502 | val_0_auc: 0.61373 |  0:15:03s\n",
      "epoch 116| loss: 0.3342  | val_0_auc: 0.60849 |  0:15:11s\n",
      "epoch 117| loss: 0.33517 | val_0_auc: 0.61469 |  0:15:18s\n",
      "epoch 118| loss: 0.33485 | val_0_auc: 0.6166  |  0:15:25s\n",
      "epoch 119| loss: 0.33578 | val_0_auc: 0.60563 |  0:15:33s\n",
      "epoch 120| loss: 0.33587 | val_0_auc: 0.59795 |  0:15:42s\n",
      "epoch 121| loss: 0.33423 | val_0_auc: 0.62085 |  0:15:50s\n",
      "epoch 122| loss: 0.33411 | val_0_auc: 0.62348 |  0:15:57s\n",
      "epoch 123| loss: 0.33392 | val_0_auc: 0.62199 |  0:16:05s\n",
      "epoch 124| loss: 0.33431 | val_0_auc: 0.62222 |  0:16:13s\n",
      "epoch 125| loss: 0.33398 | val_0_auc: 0.60914 |  0:16:22s\n",
      "epoch 126| loss: 0.33371 | val_0_auc: 0.61064 |  0:16:30s\n",
      "epoch 127| loss: 0.33247 | val_0_auc: 0.61499 |  0:16:37s\n",
      "epoch 128| loss: 0.33433 | val_0_auc: 0.61889 |  0:16:45s\n",
      "epoch 129| loss: 0.33481 | val_0_auc: 0.61056 |  0:16:52s\n",
      "epoch 130| loss: 0.32976 | val_0_auc: 0.6104  |  0:16:59s\n",
      "epoch 131| loss: 0.33404 | val_0_auc: 0.60642 |  0:17:07s\n",
      "epoch 132| loss: 0.33124 | val_0_auc: 0.62593 |  0:17:15s\n",
      "epoch 133| loss: 0.33524 | val_0_auc: 0.61728 |  0:17:23s\n",
      "epoch 134| loss: 0.33413 | val_0_auc: 0.62736 |  0:17:31s\n",
      "epoch 135| loss: 0.33618 | val_0_auc: 0.63121 |  0:17:39s\n",
      "epoch 136| loss: 0.3354  | val_0_auc: 0.62542 |  0:17:47s\n",
      "epoch 137| loss: 0.33318 | val_0_auc: 0.62516 |  0:17:56s\n",
      "epoch 138| loss: 0.33369 | val_0_auc: 0.62391 |  0:18:03s\n",
      "epoch 139| loss: 0.33062 | val_0_auc: 0.61593 |  0:18:11s\n",
      "epoch 140| loss: 0.33317 | val_0_auc: 0.62678 |  0:18:19s\n",
      "epoch 141| loss: 0.33173 | val_0_auc: 0.62478 |  0:18:26s\n",
      "epoch 142| loss: 0.33207 | val_0_auc: 0.62805 |  0:18:33s\n",
      "epoch 143| loss: 0.33395 | val_0_auc: 0.62234 |  0:18:41s\n",
      "epoch 144| loss: 0.3346  | val_0_auc: 0.62322 |  0:18:48s\n",
      "epoch 145| loss: 0.33384 | val_0_auc: 0.61935 |  0:18:57s\n",
      "epoch 146| loss: 0.33293 | val_0_auc: 0.6261  |  0:19:06s\n",
      "epoch 147| loss: 0.33243 | val_0_auc: 0.61216 |  0:19:14s\n",
      "epoch 148| loss: 0.33223 | val_0_auc: 0.62082 |  0:19:22s\n",
      "epoch 149| loss: 0.33225 | val_0_auc: 0.62619 |  0:19:30s\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_0_auc = 0.63121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 00:29:11,224] Trial 19 finished with value: 0.6312126136849173 and parameters: {'n_d': 56, 'n_a': 48, 'n_steps': 5, 'gamma': 2.128129776025304, 'lambda_sparse': 0.00019662629730743286, 'lr': 0.0009319553301607022}. Best is trial 19 with value: 0.6312126136849173.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.7177  | val_0_auc: 0.47547 |  0:00:07s\n",
      "epoch 1  | loss: 0.67626 | val_0_auc: 0.47092 |  0:00:14s\n",
      "epoch 2  | loss: 0.6062  | val_0_auc: 0.46725 |  0:00:22s\n",
      "epoch 3  | loss: 0.58266 | val_0_auc: 0.47251 |  0:00:29s\n",
      "epoch 4  | loss: 0.56427 | val_0_auc: 0.4745  |  0:00:37s\n",
      "epoch 5  | loss: 0.54825 | val_0_auc: 0.47617 |  0:00:45s\n",
      "epoch 6  | loss: 0.52234 | val_0_auc: 0.47617 |  0:00:53s\n",
      "epoch 7  | loss: 0.52441 | val_0_auc: 0.46804 |  0:01:01s\n",
      "epoch 8  | loss: 0.49868 | val_0_auc: 0.4768  |  0:01:09s\n",
      "epoch 9  | loss: 0.48968 | val_0_auc: 0.48309 |  0:01:16s\n",
      "epoch 10 | loss: 0.47976 | val_0_auc: 0.49076 |  0:01:24s\n",
      "epoch 11 | loss: 0.4715  | val_0_auc: 0.49889 |  0:01:33s\n",
      "epoch 12 | loss: 0.45887 | val_0_auc: 0.50312 |  0:01:42s\n",
      "epoch 13 | loss: 0.46218 | val_0_auc: 0.50361 |  0:01:50s\n",
      "epoch 14 | loss: 0.43712 | val_0_auc: 0.51693 |  0:01:57s\n",
      "epoch 15 | loss: 0.43837 | val_0_auc: 0.51228 |  0:02:05s\n",
      "epoch 16 | loss: 0.42636 | val_0_auc: 0.51782 |  0:02:12s\n",
      "epoch 17 | loss: 0.41948 | val_0_auc: 0.54066 |  0:02:18s\n",
      "epoch 18 | loss: 0.40981 | val_0_auc: 0.53119 |  0:02:26s\n",
      "epoch 19 | loss: 0.42061 | val_0_auc: 0.5326  |  0:02:34s\n",
      "epoch 20 | loss: 0.4093  | val_0_auc: 0.53139 |  0:02:42s\n",
      "epoch 21 | loss: 0.39837 | val_0_auc: 0.53569 |  0:02:51s\n",
      "epoch 22 | loss: 0.39278 | val_0_auc: 0.54142 |  0:03:00s\n",
      "epoch 23 | loss: 0.39318 | val_0_auc: 0.53627 |  0:03:07s\n",
      "epoch 24 | loss: 0.39234 | val_0_auc: 0.53785 |  0:03:15s\n",
      "epoch 25 | loss: 0.38747 | val_0_auc: 0.53993 |  0:03:22s\n",
      "epoch 26 | loss: 0.381   | val_0_auc: 0.5505  |  0:03:29s\n",
      "epoch 27 | loss: 0.39063 | val_0_auc: 0.54208 |  0:03:37s\n",
      "epoch 28 | loss: 0.37588 | val_0_auc: 0.55656 |  0:03:44s\n",
      "epoch 29 | loss: 0.37612 | val_0_auc: 0.55348 |  0:03:53s\n",
      "epoch 30 | loss: 0.37723 | val_0_auc: 0.54139 |  0:04:02s\n",
      "epoch 31 | loss: 0.37511 | val_0_auc: 0.55626 |  0:04:09s\n",
      "epoch 32 | loss: 0.37377 | val_0_auc: 0.54956 |  0:04:16s\n",
      "epoch 33 | loss: 0.37882 | val_0_auc: 0.54774 |  0:04:24s\n",
      "epoch 34 | loss: 0.37186 | val_0_auc: 0.54818 |  0:04:32s\n",
      "epoch 35 | loss: 0.36882 | val_0_auc: 0.53882 |  0:04:40s\n",
      "epoch 36 | loss: 0.36751 | val_0_auc: 0.55121 |  0:04:48s\n",
      "epoch 37 | loss: 0.36629 | val_0_auc: 0.55154 |  0:04:57s\n",
      "epoch 38 | loss: 0.36822 | val_0_auc: 0.55877 |  0:05:05s\n",
      "epoch 39 | loss: 0.36431 | val_0_auc: 0.54931 |  0:05:14s\n",
      "epoch 40 | loss: 0.36708 | val_0_auc: 0.55584 |  0:05:21s\n",
      "epoch 41 | loss: 0.36477 | val_0_auc: 0.55008 |  0:05:29s\n",
      "epoch 42 | loss: 0.36126 | val_0_auc: 0.54594 |  0:05:36s\n",
      "epoch 43 | loss: 0.3621  | val_0_auc: 0.55619 |  0:05:43s\n",
      "epoch 44 | loss: 0.36185 | val_0_auc: 0.56581 |  0:05:50s\n",
      "epoch 45 | loss: 0.35483 | val_0_auc: 0.56425 |  0:05:58s\n",
      "epoch 46 | loss: 0.35939 | val_0_auc: 0.56097 |  0:06:06s\n",
      "epoch 47 | loss: 0.3619  | val_0_auc: 0.54043 |  0:06:15s\n",
      "epoch 48 | loss: 0.36071 | val_0_auc: 0.54628 |  0:06:23s\n",
      "epoch 49 | loss: 0.35218 | val_0_auc: 0.55103 |  0:06:30s\n",
      "epoch 50 | loss: 0.35547 | val_0_auc: 0.54543 |  0:06:38s\n",
      "epoch 51 | loss: 0.35925 | val_0_auc: 0.56519 |  0:06:46s\n",
      "epoch 52 | loss: 0.35689 | val_0_auc: 0.56652 |  0:06:53s\n",
      "epoch 53 | loss: 0.35375 | val_0_auc: 0.5597  |  0:07:00s\n",
      "epoch 54 | loss: 0.35115 | val_0_auc: 0.57362 |  0:07:09s\n",
      "epoch 55 | loss: 0.35273 | val_0_auc: 0.56915 |  0:07:17s\n",
      "epoch 56 | loss: 0.35096 | val_0_auc: 0.57387 |  0:07:25s\n",
      "epoch 57 | loss: 0.35061 | val_0_auc: 0.5572  |  0:07:33s\n",
      "epoch 58 | loss: 0.35279 | val_0_auc: 0.55875 |  0:07:40s\n",
      "epoch 59 | loss: 0.34632 | val_0_auc: 0.56174 |  0:07:48s\n",
      "epoch 60 | loss: 0.34804 | val_0_auc: 0.55568 |  0:07:56s\n",
      "epoch 61 | loss: 0.3523  | val_0_auc: 0.56588 |  0:08:04s\n",
      "epoch 62 | loss: 0.34888 | val_0_auc: 0.56145 |  0:08:12s\n",
      "epoch 63 | loss: 0.3486  | val_0_auc: 0.56695 |  0:08:21s\n",
      "epoch 64 | loss: 0.34488 | val_0_auc: 0.57768 |  0:08:29s\n",
      "epoch 65 | loss: 0.35032 | val_0_auc: 0.56751 |  0:08:36s\n",
      "epoch 66 | loss: 0.34062 | val_0_auc: 0.57261 |  0:08:44s\n",
      "epoch 67 | loss: 0.34653 | val_0_auc: 0.57656 |  0:08:51s\n",
      "epoch 68 | loss: 0.34464 | val_0_auc: 0.56709 |  0:08:58s\n",
      "epoch 69 | loss: 0.34584 | val_0_auc: 0.56951 |  0:09:04s\n",
      "epoch 70 | loss: 0.34278 | val_0_auc: 0.57181 |  0:09:12s\n",
      "epoch 71 | loss: 0.34566 | val_0_auc: 0.57135 |  0:09:20s\n",
      "epoch 72 | loss: 0.34617 | val_0_auc: 0.57475 |  0:09:28s\n",
      "epoch 73 | loss: 0.3432  | val_0_auc: 0.57886 |  0:09:36s\n",
      "epoch 74 | loss: 0.34424 | val_0_auc: 0.57005 |  0:09:45s\n",
      "epoch 75 | loss: 0.34413 | val_0_auc: 0.56853 |  0:09:52s\n",
      "epoch 76 | loss: 0.34649 | val_0_auc: 0.57041 |  0:10:00s\n",
      "epoch 77 | loss: 0.34197 | val_0_auc: 0.56059 |  0:10:06s\n",
      "epoch 78 | loss: 0.34374 | val_0_auc: 0.5765  |  0:10:13s\n",
      "epoch 79 | loss: 0.34315 | val_0_auc: 0.5701  |  0:10:21s\n",
      "epoch 80 | loss: 0.34447 | val_0_auc: 0.57572 |  0:10:29s\n",
      "epoch 81 | loss: 0.34033 | val_0_auc: 0.58904 |  0:10:38s\n",
      "epoch 82 | loss: 0.34085 | val_0_auc: 0.58916 |  0:10:46s\n",
      "epoch 83 | loss: 0.34497 | val_0_auc: 0.57952 |  0:10:55s\n",
      "epoch 84 | loss: 0.34417 | val_0_auc: 0.58672 |  0:11:03s\n",
      "epoch 85 | loss: 0.34219 | val_0_auc: 0.58956 |  0:11:11s\n",
      "epoch 86 | loss: 0.33938 | val_0_auc: 0.59136 |  0:11:19s\n",
      "epoch 87 | loss: 0.34131 | val_0_auc: 0.58418 |  0:11:26s\n",
      "epoch 88 | loss: 0.33889 | val_0_auc: 0.58723 |  0:11:34s\n",
      "epoch 89 | loss: 0.3414  | val_0_auc: 0.58575 |  0:11:42s\n",
      "epoch 90 | loss: 0.33941 | val_0_auc: 0.57796 |  0:11:51s\n",
      "epoch 91 | loss: 0.34109 | val_0_auc: 0.5754  |  0:11:59s\n",
      "epoch 92 | loss: 0.33876 | val_0_auc: 0.58104 |  0:12:07s\n",
      "epoch 93 | loss: 0.3394  | val_0_auc: 0.57675 |  0:12:15s\n",
      "epoch 94 | loss: 0.33691 | val_0_auc: 0.57811 |  0:12:21s\n",
      "epoch 95 | loss: 0.33893 | val_0_auc: 0.58815 |  0:12:28s\n",
      "epoch 96 | loss: 0.33794 | val_0_auc: 0.58293 |  0:12:35s\n",
      "epoch 97 | loss: 0.3372  | val_0_auc: 0.5875  |  0:12:43s\n",
      "epoch 98 | loss: 0.33758 | val_0_auc: 0.59459 |  0:12:50s\n",
      "epoch 99 | loss: 0.33896 | val_0_auc: 0.59331 |  0:12:58s\n",
      "epoch 100| loss: 0.33681 | val_0_auc: 0.58926 |  0:13:05s\n",
      "epoch 101| loss: 0.33744 | val_0_auc: 0.59799 |  0:13:12s\n",
      "epoch 102| loss: 0.33676 | val_0_auc: 0.59883 |  0:13:20s\n",
      "epoch 103| loss: 0.33773 | val_0_auc: 0.58958 |  0:13:28s\n",
      "epoch 104| loss: 0.33941 | val_0_auc: 0.59073 |  0:13:37s\n",
      "epoch 105| loss: 0.33885 | val_0_auc: 0.58267 |  0:13:45s\n",
      "epoch 106| loss: 0.33663 | val_0_auc: 0.58685 |  0:13:52s\n",
      "epoch 107| loss: 0.33394 | val_0_auc: 0.57931 |  0:13:59s\n",
      "epoch 108| loss: 0.33603 | val_0_auc: 0.58112 |  0:14:07s\n",
      "epoch 109| loss: 0.33774 | val_0_auc: 0.57886 |  0:14:14s\n",
      "epoch 110| loss: 0.33708 | val_0_auc: 0.57987 |  0:14:22s\n",
      "epoch 111| loss: 0.33726 | val_0_auc: 0.59191 |  0:14:31s\n",
      "epoch 112| loss: 0.33561 | val_0_auc: 0.5942  |  0:14:40s\n",
      "epoch 113| loss: 0.33594 | val_0_auc: 0.59451 |  0:14:48s\n",
      "epoch 114| loss: 0.33586 | val_0_auc: 0.59526 |  0:14:56s\n",
      "epoch 115| loss: 0.33581 | val_0_auc: 0.59244 |  0:15:04s\n",
      "epoch 116| loss: 0.33746 | val_0_auc: 0.59242 |  0:15:11s\n",
      "epoch 117| loss: 0.33454 | val_0_auc: 0.58681 |  0:15:18s\n",
      "\n",
      "Early stopping occurred at epoch 117 with best_epoch = 102 and best_val_0_auc = 0.59883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 00:44:33,784] Trial 20 finished with value: 0.5988311054141489 and parameters: {'n_d': 54, 'n_a': 48, 'n_steps': 5, 'gamma': 2.1200375316562154, 'lambda_sparse': 0.0002392949662305926, 'lr': 0.0007623746577577735}. Best is trial 19 with value: 0.6312126136849173.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.63774 | val_0_auc: 0.48891 |  0:00:09s\n",
      "epoch 1  | loss: 0.57877 | val_0_auc: 0.48687 |  0:00:18s\n",
      "epoch 2  | loss: 0.54943 | val_0_auc: 0.50214 |  0:00:26s\n",
      "epoch 3  | loss: 0.51039 | val_0_auc: 0.51234 |  0:00:34s\n",
      "epoch 4  | loss: 0.47481 | val_0_auc: 0.53047 |  0:00:42s\n",
      "epoch 5  | loss: 0.4537  | val_0_auc: 0.52539 |  0:00:50s\n",
      "epoch 6  | loss: 0.45335 | val_0_auc: 0.51449 |  0:00:58s\n",
      "epoch 7  | loss: 0.42938 | val_0_auc: 0.50935 |  0:01:09s\n",
      "epoch 8  | loss: 0.42289 | val_0_auc: 0.51169 |  0:01:18s\n",
      "epoch 9  | loss: 0.41548 | val_0_auc: 0.53959 |  0:01:28s\n",
      "epoch 10 | loss: 0.40835 | val_0_auc: 0.5434  |  0:01:36s\n",
      "epoch 11 | loss: 0.39319 | val_0_auc: 0.53292 |  0:01:45s\n",
      "epoch 12 | loss: 0.39769 | val_0_auc: 0.54378 |  0:01:53s\n",
      "epoch 13 | loss: 0.38821 | val_0_auc: 0.53418 |  0:02:01s\n",
      "epoch 14 | loss: 0.38688 | val_0_auc: 0.52498 |  0:02:10s\n",
      "epoch 15 | loss: 0.38932 | val_0_auc: 0.53557 |  0:02:19s\n",
      "epoch 16 | loss: 0.38794 | val_0_auc: 0.55068 |  0:02:27s\n",
      "epoch 17 | loss: 0.37656 | val_0_auc: 0.54832 |  0:02:36s\n",
      "epoch 18 | loss: 0.38036 | val_0_auc: 0.54023 |  0:02:45s\n",
      "epoch 19 | loss: 0.37785 | val_0_auc: 0.53398 |  0:02:54s\n",
      "epoch 20 | loss: 0.372   | val_0_auc: 0.52588 |  0:03:03s\n",
      "epoch 21 | loss: 0.37315 | val_0_auc: 0.54354 |  0:03:11s\n",
      "epoch 22 | loss: 0.3685  | val_0_auc: 0.55871 |  0:03:18s\n",
      "epoch 23 | loss: 0.36263 | val_0_auc: 0.55156 |  0:03:26s\n",
      "epoch 24 | loss: 0.36056 | val_0_auc: 0.56374 |  0:03:33s\n",
      "epoch 25 | loss: 0.36117 | val_0_auc: 0.55911 |  0:03:42s\n",
      "epoch 26 | loss: 0.3602  | val_0_auc: 0.56193 |  0:03:51s\n",
      "epoch 27 | loss: 0.3578  | val_0_auc: 0.56253 |  0:04:00s\n",
      "epoch 28 | loss: 0.36022 | val_0_auc: 0.5511  |  0:04:08s\n",
      "epoch 29 | loss: 0.35641 | val_0_auc: 0.55636 |  0:04:16s\n",
      "epoch 30 | loss: 0.35796 | val_0_auc: 0.54321 |  0:04:24s\n",
      "epoch 31 | loss: 0.35682 | val_0_auc: 0.54864 |  0:04:33s\n",
      "epoch 32 | loss: 0.3511  | val_0_auc: 0.55065 |  0:04:42s\n",
      "epoch 33 | loss: 0.34899 | val_0_auc: 0.57045 |  0:04:51s\n",
      "epoch 34 | loss: 0.34614 | val_0_auc: 0.56619 |  0:05:00s\n",
      "epoch 35 | loss: 0.35148 | val_0_auc: 0.56586 |  0:05:08s\n",
      "epoch 36 | loss: 0.35047 | val_0_auc: 0.56709 |  0:05:17s\n",
      "epoch 37 | loss: 0.34842 | val_0_auc: 0.57831 |  0:05:26s\n",
      "epoch 38 | loss: 0.3488  | val_0_auc: 0.574   |  0:05:34s\n",
      "epoch 39 | loss: 0.34723 | val_0_auc: 0.56668 |  0:05:42s\n",
      "epoch 40 | loss: 0.34677 | val_0_auc: 0.58167 |  0:05:50s\n",
      "epoch 41 | loss: 0.34772 | val_0_auc: 0.54947 |  0:05:59s\n",
      "epoch 42 | loss: 0.34696 | val_0_auc: 0.55217 |  0:06:08s\n",
      "epoch 43 | loss: 0.34536 | val_0_auc: 0.55777 |  0:06:16s\n",
      "epoch 44 | loss: 0.34507 | val_0_auc: 0.5688  |  0:06:24s\n",
      "epoch 45 | loss: 0.34108 | val_0_auc: 0.56545 |  0:06:33s\n",
      "epoch 46 | loss: 0.33987 | val_0_auc: 0.58448 |  0:06:41s\n",
      "epoch 47 | loss: 0.34624 | val_0_auc: 0.5679  |  0:06:51s\n",
      "epoch 48 | loss: 0.3453  | val_0_auc: 0.57657 |  0:06:59s\n",
      "epoch 49 | loss: 0.34392 | val_0_auc: 0.57276 |  0:07:07s\n",
      "epoch 50 | loss: 0.34092 | val_0_auc: 0.57751 |  0:07:15s\n",
      "epoch 51 | loss: 0.34196 | val_0_auc: 0.56942 |  0:07:23s\n",
      "epoch 52 | loss: 0.33907 | val_0_auc: 0.59358 |  0:07:31s\n",
      "epoch 53 | loss: 0.33993 | val_0_auc: 0.58166 |  0:07:40s\n",
      "epoch 54 | loss: 0.3383  | val_0_auc: 0.59217 |  0:07:49s\n",
      "epoch 55 | loss: 0.34244 | val_0_auc: 0.58841 |  0:07:58s\n",
      "epoch 56 | loss: 0.34156 | val_0_auc: 0.59573 |  0:08:07s\n",
      "epoch 57 | loss: 0.3386  | val_0_auc: 0.5802  |  0:08:15s\n",
      "epoch 58 | loss: 0.33862 | val_0_auc: 0.58657 |  0:08:23s\n",
      "epoch 59 | loss: 0.33619 | val_0_auc: 0.58798 |  0:08:31s\n",
      "epoch 60 | loss: 0.33877 | val_0_auc: 0.59103 |  0:08:39s\n",
      "epoch 61 | loss: 0.33656 | val_0_auc: 0.59962 |  0:08:47s\n",
      "epoch 62 | loss: 0.33634 | val_0_auc: 0.58829 |  0:08:56s\n",
      "epoch 63 | loss: 0.33694 | val_0_auc: 0.59189 |  0:09:06s\n",
      "epoch 64 | loss: 0.33615 | val_0_auc: 0.60318 |  0:09:15s\n",
      "epoch 65 | loss: 0.33672 | val_0_auc: 0.60645 |  0:09:23s\n",
      "epoch 66 | loss: 0.3361  | val_0_auc: 0.60066 |  0:09:31s\n",
      "epoch 67 | loss: 0.33512 | val_0_auc: 0.60738 |  0:09:39s\n",
      "epoch 68 | loss: 0.33245 | val_0_auc: 0.60803 |  0:09:48s\n",
      "epoch 69 | loss: 0.33642 | val_0_auc: 0.60239 |  0:09:57s\n",
      "epoch 70 | loss: 0.3324  | val_0_auc: 0.60134 |  0:10:06s\n",
      "epoch 71 | loss: 0.33749 | val_0_auc: 0.60381 |  0:10:15s\n",
      "epoch 72 | loss: 0.33801 | val_0_auc: 0.58932 |  0:10:23s\n",
      "epoch 73 | loss: 0.33484 | val_0_auc: 0.58688 |  0:10:32s\n",
      "epoch 74 | loss: 0.33365 | val_0_auc: 0.59254 |  0:10:40s\n",
      "epoch 75 | loss: 0.33218 | val_0_auc: 0.60307 |  0:10:48s\n",
      "epoch 76 | loss: 0.33239 | val_0_auc: 0.59517 |  0:10:57s\n",
      "epoch 77 | loss: 0.33376 | val_0_auc: 0.6012  |  0:11:06s\n",
      "epoch 78 | loss: 0.33478 | val_0_auc: 0.60121 |  0:11:15s\n",
      "epoch 79 | loss: 0.33257 | val_0_auc: 0.60028 |  0:11:24s\n",
      "epoch 80 | loss: 0.33086 | val_0_auc: 0.61048 |  0:11:31s\n",
      "epoch 81 | loss: 0.3296  | val_0_auc: 0.61052 |  0:11:39s\n",
      "epoch 82 | loss: 0.32978 | val_0_auc: 0.60863 |  0:11:48s\n",
      "epoch 83 | loss: 0.33467 | val_0_auc: 0.61501 |  0:11:57s\n",
      "epoch 84 | loss: 0.3331  | val_0_auc: 0.59486 |  0:12:07s\n",
      "epoch 85 | loss: 0.33465 | val_0_auc: 0.59777 |  0:12:16s\n",
      "epoch 86 | loss: 0.33124 | val_0_auc: 0.58443 |  0:12:24s\n",
      "epoch 87 | loss: 0.33429 | val_0_auc: 0.59382 |  0:12:31s\n",
      "epoch 88 | loss: 0.32951 | val_0_auc: 0.58567 |  0:12:39s\n",
      "epoch 89 | loss: 0.33192 | val_0_auc: 0.59679 |  0:12:48s\n",
      "epoch 90 | loss: 0.33203 | val_0_auc: 0.60775 |  0:12:56s\n",
      "epoch 91 | loss: 0.33242 | val_0_auc: 0.59819 |  0:13:05s\n",
      "epoch 92 | loss: 0.33298 | val_0_auc: 0.59457 |  0:13:14s\n",
      "epoch 93 | loss: 0.33279 | val_0_auc: 0.60607 |  0:13:23s\n",
      "epoch 94 | loss: 0.33035 | val_0_auc: 0.60621 |  0:13:32s\n",
      "epoch 95 | loss: 0.32884 | val_0_auc: 0.60949 |  0:13:41s\n",
      "epoch 96 | loss: 0.33098 | val_0_auc: 0.61522 |  0:13:49s\n",
      "epoch 97 | loss: 0.33168 | val_0_auc: 0.61743 |  0:13:57s\n",
      "epoch 98 | loss: 0.32924 | val_0_auc: 0.61863 |  0:14:05s\n",
      "epoch 99 | loss: 0.32871 | val_0_auc: 0.62207 |  0:14:12s\n",
      "epoch 100| loss: 0.32946 | val_0_auc: 0.62314 |  0:14:19s\n",
      "epoch 101| loss: 0.3311  | val_0_auc: 0.61236 |  0:14:29s\n",
      "epoch 102| loss: 0.3304  | val_0_auc: 0.61617 |  0:14:39s\n",
      "epoch 103| loss: 0.3274  | val_0_auc: 0.6142  |  0:14:49s\n",
      "epoch 104| loss: 0.32649 | val_0_auc: 0.6151  |  0:15:00s\n",
      "epoch 105| loss: 0.32942 | val_0_auc: 0.61366 |  0:15:11s\n",
      "epoch 106| loss: 0.32699 | val_0_auc: 0.61224 |  0:15:22s\n",
      "epoch 107| loss: 0.3279  | val_0_auc: 0.60459 |  0:15:33s\n",
      "epoch 108| loss: 0.32523 | val_0_auc: 0.61573 |  0:15:44s\n",
      "epoch 109| loss: 0.3297  | val_0_auc: 0.60782 |  0:15:55s\n",
      "epoch 110| loss: 0.32623 | val_0_auc: 0.61443 |  0:16:04s\n",
      "epoch 111| loss: 0.32778 | val_0_auc: 0.60887 |  0:16:13s\n",
      "epoch 112| loss: 0.32546 | val_0_auc: 0.60069 |  0:16:22s\n",
      "epoch 113| loss: 0.32595 | val_0_auc: 0.60886 |  0:16:32s\n",
      "epoch 114| loss: 0.32535 | val_0_auc: 0.602   |  0:16:40s\n",
      "epoch 115| loss: 0.32534 | val_0_auc: 0.60681 |  0:16:48s\n",
      "\n",
      "Early stopping occurred at epoch 115 with best_epoch = 100 and best_val_0_auc = 0.62314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 01:01:25,483] Trial 21 finished with value: 0.6231395857494557 and parameters: {'n_d': 64, 'n_a': 60, 'n_steps': 5, 'gamma': 2.3544419190193078, 'lambda_sparse': 7.474601862573292e-05, 'lr': 0.0012697149322133106}. Best is trial 19 with value: 0.6312126136849173.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.60328 | val_0_auc: 0.53794 |  0:00:07s\n",
      "epoch 1  | loss: 3.03742 | val_0_auc: 0.53609 |  0:00:15s\n",
      "epoch 2  | loss: 1.77745 | val_0_auc: 0.51474 |  0:00:24s\n",
      "epoch 3  | loss: 0.96404 | val_0_auc: 0.49486 |  0:00:32s\n",
      "epoch 4  | loss: 0.60049 | val_0_auc: 0.49952 |  0:00:42s\n",
      "epoch 5  | loss: 0.51993 | val_0_auc: 0.49473 |  0:00:50s\n",
      "epoch 6  | loss: 0.49097 | val_0_auc: 0.49126 |  0:00:58s\n",
      "epoch 7  | loss: 0.49319 | val_0_auc: 0.50863 |  0:01:06s\n",
      "epoch 8  | loss: 0.46565 | val_0_auc: 0.50582 |  0:01:13s\n",
      "epoch 9  | loss: 0.44325 | val_0_auc: 0.51629 |  0:01:20s\n",
      "epoch 10 | loss: 0.43218 | val_0_auc: 0.52467 |  0:01:28s\n",
      "epoch 11 | loss: 0.42892 | val_0_auc: 0.52695 |  0:01:36s\n",
      "epoch 12 | loss: 0.4202  | val_0_auc: 0.51295 |  0:01:44s\n",
      "epoch 13 | loss: 0.41482 | val_0_auc: 0.53024 |  0:01:53s\n",
      "epoch 14 | loss: 0.40884 | val_0_auc: 0.52352 |  0:02:02s\n",
      "epoch 15 | loss: 0.40039 | val_0_auc: 0.52382 |  0:02:10s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_0_auc = 0.53794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 01:03:39,304] Trial 22 finished with value: 0.5379382083053572 and parameters: {'n_d': 56, 'n_a': 59, 'n_steps': 5, 'gamma': 2.313506423951181, 'lambda_sparse': 5.497312328430202e-05, 'lr': 0.0014084787578780527}. Best is trial 19 with value: 0.6312126136849173.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.40617 | val_0_auc: 0.5128  |  0:00:05s\n",
      "epoch 1  | loss: 0.93508 | val_0_auc: 0.50447 |  0:00:11s\n",
      "epoch 2  | loss: 0.65862 | val_0_auc: 0.49215 |  0:00:18s\n",
      "epoch 3  | loss: 0.53933 | val_0_auc: 0.48899 |  0:00:24s\n",
      "epoch 4  | loss: 0.49953 | val_0_auc: 0.49527 |  0:00:31s\n",
      "epoch 5  | loss: 0.47199 | val_0_auc: 0.48357 |  0:00:39s\n",
      "epoch 6  | loss: 0.45637 | val_0_auc: 0.48258 |  0:00:47s\n",
      "epoch 7  | loss: 0.44144 | val_0_auc: 0.49618 |  0:00:54s\n",
      "epoch 8  | loss: 0.43262 | val_0_auc: 0.49614 |  0:01:02s\n",
      "epoch 9  | loss: 0.42325 | val_0_auc: 0.50019 |  0:01:09s\n",
      "epoch 10 | loss: 0.41735 | val_0_auc: 0.50685 |  0:01:16s\n",
      "epoch 11 | loss: 0.41172 | val_0_auc: 0.52243 |  0:01:22s\n",
      "epoch 12 | loss: 0.40977 | val_0_auc: 0.50713 |  0:01:29s\n",
      "epoch 13 | loss: 0.40281 | val_0_auc: 0.5148  |  0:01:35s\n",
      "epoch 14 | loss: 0.40145 | val_0_auc: 0.51951 |  0:01:41s\n",
      "epoch 15 | loss: 0.39048 | val_0_auc: 0.52352 |  0:01:47s\n",
      "epoch 16 | loss: 0.38622 | val_0_auc: 0.51904 |  0:01:55s\n",
      "epoch 17 | loss: 0.39241 | val_0_auc: 0.52704 |  0:02:01s\n",
      "epoch 18 | loss: 0.38734 | val_0_auc: 0.52897 |  0:02:08s\n",
      "epoch 19 | loss: 0.384   | val_0_auc: 0.52093 |  0:02:16s\n",
      "epoch 20 | loss: 0.3787  | val_0_auc: 0.52333 |  0:02:23s\n",
      "epoch 21 | loss: 0.37714 | val_0_auc: 0.52372 |  0:02:30s\n",
      "epoch 22 | loss: 0.38024 | val_0_auc: 0.52123 |  0:02:38s\n",
      "epoch 23 | loss: 0.37743 | val_0_auc: 0.51888 |  0:02:44s\n",
      "epoch 24 | loss: 0.37198 | val_0_auc: 0.51675 |  0:02:51s\n",
      "epoch 25 | loss: 0.37115 | val_0_auc: 0.52109 |  0:02:57s\n",
      "epoch 26 | loss: 0.37217 | val_0_auc: 0.53081 |  0:03:03s\n",
      "epoch 27 | loss: 0.3732  | val_0_auc: 0.5397  |  0:03:09s\n",
      "epoch 28 | loss: 0.36602 | val_0_auc: 0.53187 |  0:03:16s\n",
      "epoch 29 | loss: 0.36998 | val_0_auc: 0.52811 |  0:03:23s\n",
      "epoch 30 | loss: 0.36668 | val_0_auc: 0.52983 |  0:03:30s\n",
      "epoch 31 | loss: 0.36942 | val_0_auc: 0.54407 |  0:03:37s\n",
      "epoch 32 | loss: 0.36533 | val_0_auc: 0.54208 |  0:03:44s\n",
      "epoch 33 | loss: 0.36393 | val_0_auc: 0.5381  |  0:03:52s\n",
      "epoch 34 | loss: 0.36299 | val_0_auc: 0.5398  |  0:03:59s\n",
      "epoch 35 | loss: 0.36383 | val_0_auc: 0.55047 |  0:04:05s\n",
      "epoch 36 | loss: 0.35926 | val_0_auc: 0.55035 |  0:04:11s\n",
      "epoch 37 | loss: 0.36047 | val_0_auc: 0.55348 |  0:04:17s\n",
      "epoch 38 | loss: 0.3582  | val_0_auc: 0.55731 |  0:04:24s\n",
      "epoch 39 | loss: 0.35812 | val_0_auc: 0.55958 |  0:04:30s\n",
      "epoch 40 | loss: 0.35284 | val_0_auc: 0.56228 |  0:04:37s\n",
      "epoch 41 | loss: 0.35695 | val_0_auc: 0.55615 |  0:04:43s\n",
      "epoch 42 | loss: 0.35176 | val_0_auc: 0.56293 |  0:04:50s\n",
      "epoch 43 | loss: 0.35562 | val_0_auc: 0.57075 |  0:04:56s\n",
      "epoch 44 | loss: 0.35501 | val_0_auc: 0.55987 |  0:05:03s\n",
      "epoch 45 | loss: 0.3527  | val_0_auc: 0.56517 |  0:05:10s\n",
      "epoch 46 | loss: 0.35372 | val_0_auc: 0.55855 |  0:05:18s\n",
      "epoch 47 | loss: 0.35078 | val_0_auc: 0.56554 |  0:05:25s\n",
      "epoch 48 | loss: 0.3525  | val_0_auc: 0.55151 |  0:05:32s\n",
      "epoch 49 | loss: 0.34973 | val_0_auc: 0.5487  |  0:05:39s\n",
      "epoch 50 | loss: 0.35169 | val_0_auc: 0.55439 |  0:05:45s\n",
      "epoch 51 | loss: 0.35071 | val_0_auc: 0.55757 |  0:05:51s\n",
      "epoch 52 | loss: 0.34935 | val_0_auc: 0.56904 |  0:05:57s\n",
      "epoch 53 | loss: 0.34915 | val_0_auc: 0.55283 |  0:06:04s\n",
      "epoch 54 | loss: 0.34711 | val_0_auc: 0.57341 |  0:06:12s\n",
      "epoch 55 | loss: 0.35022 | val_0_auc: 0.56639 |  0:06:19s\n",
      "epoch 56 | loss: 0.351   | val_0_auc: 0.56485 |  0:06:26s\n",
      "epoch 57 | loss: 0.34684 | val_0_auc: 0.57112 |  0:06:33s\n",
      "epoch 58 | loss: 0.34818 | val_0_auc: 0.5706  |  0:06:40s\n",
      "epoch 59 | loss: 0.3459  | val_0_auc: 0.58691 |  0:06:47s\n",
      "epoch 60 | loss: 0.34886 | val_0_auc: 0.57573 |  0:06:53s\n",
      "epoch 61 | loss: 0.3432  | val_0_auc: 0.5749  |  0:06:59s\n",
      "epoch 62 | loss: 0.34484 | val_0_auc: 0.58389 |  0:07:04s\n",
      "epoch 63 | loss: 0.34415 | val_0_auc: 0.57409 |  0:07:11s\n",
      "epoch 64 | loss: 0.34665 | val_0_auc: 0.58078 |  0:07:17s\n",
      "epoch 65 | loss: 0.34753 | val_0_auc: 0.56898 |  0:07:24s\n",
      "epoch 66 | loss: 0.34566 | val_0_auc: 0.56531 |  0:07:31s\n",
      "epoch 67 | loss: 0.34448 | val_0_auc: 0.57258 |  0:07:39s\n",
      "epoch 68 | loss: 0.34569 | val_0_auc: 0.57485 |  0:07:46s\n",
      "epoch 69 | loss: 0.34297 | val_0_auc: 0.57955 |  0:07:54s\n",
      "epoch 70 | loss: 0.34149 | val_0_auc: 0.59099 |  0:08:01s\n",
      "epoch 71 | loss: 0.34392 | val_0_auc: 0.60047 |  0:08:07s\n",
      "epoch 72 | loss: 0.34301 | val_0_auc: 0.5927  |  0:08:13s\n",
      "epoch 73 | loss: 0.34081 | val_0_auc: 0.5902  |  0:08:19s\n",
      "epoch 74 | loss: 0.33948 | val_0_auc: 0.58663 |  0:08:25s\n",
      "epoch 75 | loss: 0.33875 | val_0_auc: 0.58388 |  0:08:32s\n",
      "epoch 76 | loss: 0.33928 | val_0_auc: 0.593   |  0:08:38s\n",
      "epoch 77 | loss: 0.33883 | val_0_auc: 0.58812 |  0:08:44s\n",
      "epoch 78 | loss: 0.33883 | val_0_auc: 0.58416 |  0:08:51s\n",
      "epoch 79 | loss: 0.33891 | val_0_auc: 0.57867 |  0:08:58s\n",
      "epoch 80 | loss: 0.33993 | val_0_auc: 0.56878 |  0:09:05s\n",
      "epoch 81 | loss: 0.339   | val_0_auc: 0.58433 |  0:09:13s\n",
      "epoch 82 | loss: 0.33823 | val_0_auc: 0.59008 |  0:09:20s\n",
      "epoch 83 | loss: 0.33863 | val_0_auc: 0.58771 |  0:09:28s\n",
      "epoch 84 | loss: 0.34071 | val_0_auc: 0.59344 |  0:09:34s\n",
      "epoch 85 | loss: 0.33646 | val_0_auc: 0.59424 |  0:09:41s\n",
      "epoch 86 | loss: 0.33574 | val_0_auc: 0.58641 |  0:09:47s\n",
      "\n",
      "Early stopping occurred at epoch 86 with best_epoch = 71 and best_val_0_auc = 0.60047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 01:13:29,420] Trial 23 finished with value: 0.6004698722252741 and parameters: {'n_d': 60, 'n_a': 52, 'n_steps': 4, 'gamma': 2.1382852053567194, 'lambda_sparse': 0.00023486535880957464, 'lr': 0.000887911721257968}. Best is trial 19 with value: 0.6312126136849173.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.81935 | val_0_auc: 0.46161 |  0:00:07s\n",
      "epoch 1  | loss: 0.60505 | val_0_auc: 0.47878 |  0:00:14s\n",
      "epoch 2  | loss: 0.56939 | val_0_auc: 0.48236 |  0:00:21s\n",
      "epoch 3  | loss: 0.54782 | val_0_auc: 0.47422 |  0:00:28s\n",
      "epoch 4  | loss: 0.51706 | val_0_auc: 0.49199 |  0:00:38s\n",
      "epoch 5  | loss: 0.49484 | val_0_auc: 0.4961  |  0:00:47s\n",
      "epoch 6  | loss: 0.48935 | val_0_auc: 0.49392 |  0:00:56s\n",
      "epoch 7  | loss: 0.47853 | val_0_auc: 0.50221 |  0:01:04s\n",
      "epoch 8  | loss: 0.45807 | val_0_auc: 0.50535 |  0:01:11s\n",
      "epoch 9  | loss: 0.4474  | val_0_auc: 0.50188 |  0:01:19s\n",
      "epoch 10 | loss: 0.4334  | val_0_auc: 0.49643 |  0:01:27s\n",
      "epoch 11 | loss: 0.42689 | val_0_auc: 0.50123 |  0:01:35s\n",
      "epoch 12 | loss: 0.42047 | val_0_auc: 0.51652 |  0:01:44s\n",
      "epoch 13 | loss: 0.42476 | val_0_auc: 0.51591 |  0:01:52s\n",
      "epoch 14 | loss: 0.41837 | val_0_auc: 0.5205  |  0:02:01s\n",
      "epoch 15 | loss: 0.41412 | val_0_auc: 0.51532 |  0:02:08s\n",
      "epoch 16 | loss: 0.41627 | val_0_auc: 0.51311 |  0:02:16s\n",
      "epoch 17 | loss: 0.40464 | val_0_auc: 0.51102 |  0:02:23s\n",
      "epoch 18 | loss: 0.39776 | val_0_auc: 0.5267  |  0:02:32s\n",
      "epoch 19 | loss: 0.39592 | val_0_auc: 0.53192 |  0:02:40s\n",
      "epoch 20 | loss: 0.38923 | val_0_auc: 0.52748 |  0:02:49s\n",
      "epoch 21 | loss: 0.39529 | val_0_auc: 0.53075 |  0:02:57s\n",
      "epoch 22 | loss: 0.39457 | val_0_auc: 0.53323 |  0:03:04s\n",
      "epoch 23 | loss: 0.3817  | val_0_auc: 0.53549 |  0:03:12s\n",
      "epoch 24 | loss: 0.38819 | val_0_auc: 0.53566 |  0:03:19s\n",
      "epoch 25 | loss: 0.38346 | val_0_auc: 0.55184 |  0:03:28s\n",
      "epoch 26 | loss: 0.37912 | val_0_auc: 0.55102 |  0:03:37s\n",
      "epoch 27 | loss: 0.37506 | val_0_auc: 0.55168 |  0:03:45s\n",
      "epoch 28 | loss: 0.3773  | val_0_auc: 0.55122 |  0:03:53s\n",
      "epoch 29 | loss: 0.37659 | val_0_auc: 0.55417 |  0:04:00s\n",
      "epoch 30 | loss: 0.37576 | val_0_auc: 0.57107 |  0:04:07s\n",
      "epoch 31 | loss: 0.36773 | val_0_auc: 0.56249 |  0:04:15s\n",
      "epoch 32 | loss: 0.36925 | val_0_auc: 0.55873 |  0:04:23s\n",
      "epoch 33 | loss: 0.36953 | val_0_auc: 0.56881 |  0:04:32s\n",
      "epoch 34 | loss: 0.36315 | val_0_auc: 0.56811 |  0:04:40s\n",
      "epoch 35 | loss: 0.3663  | val_0_auc: 0.55639 |  0:04:48s\n",
      "epoch 36 | loss: 0.36486 | val_0_auc: 0.56258 |  0:04:55s\n",
      "epoch 37 | loss: 0.3602  | val_0_auc: 0.55851 |  0:05:03s\n",
      "epoch 38 | loss: 0.35459 | val_0_auc: 0.54065 |  0:05:10s\n",
      "epoch 39 | loss: 0.35543 | val_0_auc: 0.54296 |  0:05:19s\n",
      "epoch 40 | loss: 0.35878 | val_0_auc: 0.53918 |  0:05:29s\n",
      "epoch 41 | loss: 0.36161 | val_0_auc: 0.55569 |  0:05:39s\n",
      "epoch 42 | loss: 0.35605 | val_0_auc: 0.54561 |  0:05:48s\n",
      "epoch 43 | loss: 0.35173 | val_0_auc: 0.55465 |  0:05:57s\n",
      "epoch 44 | loss: 0.35429 | val_0_auc: 0.55649 |  0:06:05s\n",
      "epoch 45 | loss: 0.35081 | val_0_auc: 0.56041 |  0:06:13s\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 30 and best_val_0_auc = 0.57107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 01:19:47,175] Trial 24 finished with value: 0.5710707491505422 and parameters: {'n_d': 62, 'n_a': 48, 'n_steps': 5, 'gamma': 2.338429529087072, 'lambda_sparse': 0.00011729179086134452, 'lr': 0.0011476373048387456}. Best is trial 19 with value: 0.6312126136849173.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.61167 | val_0_auc: 0.54663 |  0:00:08s\n",
      "epoch 1  | loss: 2.08247 | val_0_auc: 0.5392  |  0:00:17s\n",
      "epoch 2  | loss: 0.97265 | val_0_auc: 0.52031 |  0:00:25s\n",
      "epoch 3  | loss: 0.58021 | val_0_auc: 0.49882 |  0:00:33s\n",
      "epoch 4  | loss: 0.52239 | val_0_auc: 0.50171 |  0:00:40s\n",
      "epoch 5  | loss: 0.50764 | val_0_auc: 0.498   |  0:00:47s\n",
      "epoch 6  | loss: 0.48673 | val_0_auc: 0.5062  |  0:00:54s\n",
      "epoch 7  | loss: 0.47426 | val_0_auc: 0.528   |  0:01:01s\n",
      "epoch 8  | loss: 0.45081 | val_0_auc: 0.53388 |  0:01:09s\n",
      "epoch 9  | loss: 0.43047 | val_0_auc: 0.53821 |  0:01:17s\n",
      "epoch 10 | loss: 0.41445 | val_0_auc: 0.53272 |  0:01:26s\n",
      "epoch 11 | loss: 0.41901 | val_0_auc: 0.53194 |  0:01:34s\n",
      "epoch 12 | loss: 0.40607 | val_0_auc: 0.52336 |  0:01:43s\n",
      "epoch 13 | loss: 0.39426 | val_0_auc: 0.52783 |  0:01:52s\n",
      "epoch 14 | loss: 0.39973 | val_0_auc: 0.52908 |  0:01:59s\n",
      "epoch 15 | loss: 0.3918  | val_0_auc: 0.5275  |  0:02:06s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_0_auc = 0.54663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 01:21:57,173] Trial 25 finished with value: 0.546632434027142 and parameters: {'n_d': 46, 'n_a': 59, 'n_steps': 5, 'gamma': 1.823843403674699, 'lambda_sparse': 1.8955908909926063e-05, 'lr': 0.0017523872967851676}. Best is trial 19 with value: 0.6312126136849173.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.8769  | val_0_auc: 0.5155  |  0:00:05s\n",
      "epoch 1  | loss: 1.17328 | val_0_auc: 0.51954 |  0:00:11s\n",
      "epoch 2  | loss: 0.75264 | val_0_auc: 0.51906 |  0:00:17s\n",
      "epoch 3  | loss: 0.5287  | val_0_auc: 0.50402 |  0:00:23s\n",
      "epoch 4  | loss: 0.4572  | val_0_auc: 0.49723 |  0:00:30s\n",
      "epoch 5  | loss: 0.44443 | val_0_auc: 0.50632 |  0:00:37s\n",
      "epoch 6  | loss: 0.44674 | val_0_auc: 0.5106  |  0:00:44s\n",
      "epoch 7  | loss: 0.4269  | val_0_auc: 0.52046 |  0:00:50s\n",
      "epoch 8  | loss: 0.41474 | val_0_auc: 0.52641 |  0:00:57s\n",
      "epoch 9  | loss: 0.40462 | val_0_auc: 0.5296  |  0:01:04s\n",
      "epoch 10 | loss: 0.40366 | val_0_auc: 0.52154 |  0:01:10s\n",
      "epoch 11 | loss: 0.39576 | val_0_auc: 0.52982 |  0:01:16s\n",
      "epoch 12 | loss: 0.39156 | val_0_auc: 0.52401 |  0:01:21s\n",
      "epoch 13 | loss: 0.38205 | val_0_auc: 0.53348 |  0:01:27s\n",
      "epoch 14 | loss: 0.38074 | val_0_auc: 0.55364 |  0:01:34s\n",
      "epoch 15 | loss: 0.37848 | val_0_auc: 0.54618 |  0:01:40s\n",
      "epoch 16 | loss: 0.3746  | val_0_auc: 0.53473 |  0:01:47s\n",
      "epoch 17 | loss: 0.37773 | val_0_auc: 0.52437 |  0:01:53s\n",
      "epoch 18 | loss: 0.36945 | val_0_auc: 0.51995 |  0:01:59s\n",
      "epoch 19 | loss: 0.36904 | val_0_auc: 0.54025 |  0:02:06s\n",
      "epoch 20 | loss: 0.36706 | val_0_auc: 0.5409  |  0:02:12s\n",
      "epoch 21 | loss: 0.36748 | val_0_auc: 0.5409  |  0:02:18s\n",
      "epoch 22 | loss: 0.36245 | val_0_auc: 0.52737 |  0:02:24s\n",
      "epoch 23 | loss: 0.36468 | val_0_auc: 0.54544 |  0:02:30s\n",
      "epoch 24 | loss: 0.36484 | val_0_auc: 0.53814 |  0:02:36s\n",
      "epoch 25 | loss: 0.36255 | val_0_auc: 0.54331 |  0:02:42s\n",
      "epoch 26 | loss: 0.35947 | val_0_auc: 0.55163 |  0:02:49s\n",
      "epoch 27 | loss: 0.35742 | val_0_auc: 0.5446  |  0:02:56s\n",
      "epoch 28 | loss: 0.36063 | val_0_auc: 0.54209 |  0:03:04s\n",
      "epoch 29 | loss: 0.35291 | val_0_auc: 0.55599 |  0:03:10s\n",
      "epoch 30 | loss: 0.3573  | val_0_auc: 0.55871 |  0:03:17s\n",
      "epoch 31 | loss: 0.35403 | val_0_auc: 0.55637 |  0:03:23s\n",
      "epoch 32 | loss: 0.35112 | val_0_auc: 0.56632 |  0:03:28s\n",
      "epoch 33 | loss: 0.35059 | val_0_auc: 0.56069 |  0:03:35s\n",
      "epoch 34 | loss: 0.34935 | val_0_auc: 0.5568  |  0:03:41s\n",
      "epoch 35 | loss: 0.35156 | val_0_auc: 0.5617  |  0:03:47s\n",
      "epoch 36 | loss: 0.34808 | val_0_auc: 0.5653  |  0:03:54s\n",
      "epoch 37 | loss: 0.34833 | val_0_auc: 0.56885 |  0:04:00s\n",
      "epoch 38 | loss: 0.34872 | val_0_auc: 0.57682 |  0:04:06s\n",
      "epoch 39 | loss: 0.34882 | val_0_auc: 0.59386 |  0:04:12s\n",
      "epoch 40 | loss: 0.34951 | val_0_auc: 0.58429 |  0:04:18s\n",
      "epoch 41 | loss: 0.34829 | val_0_auc: 0.57101 |  0:04:25s\n",
      "epoch 42 | loss: 0.34853 | val_0_auc: 0.57693 |  0:04:32s\n",
      "epoch 43 | loss: 0.34595 | val_0_auc: 0.57896 |  0:04:39s\n",
      "epoch 44 | loss: 0.34699 | val_0_auc: 0.57142 |  0:04:46s\n",
      "epoch 45 | loss: 0.34226 | val_0_auc: 0.57928 |  0:04:53s\n",
      "epoch 46 | loss: 0.3451  | val_0_auc: 0.5756  |  0:05:00s\n",
      "epoch 47 | loss: 0.34625 | val_0_auc: 0.57153 |  0:05:06s\n",
      "epoch 48 | loss: 0.34498 | val_0_auc: 0.58009 |  0:05:12s\n",
      "epoch 49 | loss: 0.34392 | val_0_auc: 0.57669 |  0:05:18s\n",
      "epoch 50 | loss: 0.34253 | val_0_auc: 0.57748 |  0:05:24s\n",
      "epoch 51 | loss: 0.34067 | val_0_auc: 0.59602 |  0:05:30s\n",
      "epoch 52 | loss: 0.34268 | val_0_auc: 0.59381 |  0:05:36s\n",
      "epoch 53 | loss: 0.3412  | val_0_auc: 0.58259 |  0:05:43s\n",
      "epoch 54 | loss: 0.34259 | val_0_auc: 0.59337 |  0:05:50s\n",
      "epoch 55 | loss: 0.34133 | val_0_auc: 0.59641 |  0:05:56s\n",
      "epoch 56 | loss: 0.34059 | val_0_auc: 0.59668 |  0:06:02s\n",
      "epoch 57 | loss: 0.33987 | val_0_auc: 0.60254 |  0:06:09s\n",
      "epoch 58 | loss: 0.33981 | val_0_auc: 0.60238 |  0:06:15s\n",
      "epoch 59 | loss: 0.33841 | val_0_auc: 0.59514 |  0:06:21s\n",
      "epoch 60 | loss: 0.33956 | val_0_auc: 0.60736 |  0:06:27s\n",
      "epoch 61 | loss: 0.34176 | val_0_auc: 0.60765 |  0:06:33s\n",
      "epoch 62 | loss: 0.34142 | val_0_auc: 0.59858 |  0:06:39s\n",
      "epoch 63 | loss: 0.33568 | val_0_auc: 0.60259 |  0:06:46s\n",
      "epoch 64 | loss: 0.34129 | val_0_auc: 0.60755 |  0:06:52s\n",
      "epoch 65 | loss: 0.33762 | val_0_auc: 0.60775 |  0:06:59s\n",
      "epoch 66 | loss: 0.33864 | val_0_auc: 0.59759 |  0:07:06s\n",
      "epoch 67 | loss: 0.33833 | val_0_auc: 0.59103 |  0:07:13s\n",
      "epoch 68 | loss: 0.33779 | val_0_auc: 0.59662 |  0:07:20s\n",
      "epoch 69 | loss: 0.33471 | val_0_auc: 0.59352 |  0:07:26s\n",
      "epoch 70 | loss: 0.33558 | val_0_auc: 0.58739 |  0:07:32s\n",
      "epoch 71 | loss: 0.33712 | val_0_auc: 0.6068  |  0:07:38s\n",
      "epoch 72 | loss: 0.33756 | val_0_auc: 0.59734 |  0:07:44s\n",
      "epoch 73 | loss: 0.33542 | val_0_auc: 0.60619 |  0:07:50s\n",
      "epoch 74 | loss: 0.33579 | val_0_auc: 0.59232 |  0:07:57s\n",
      "epoch 75 | loss: 0.3351  | val_0_auc: 0.60319 |  0:08:03s\n",
      "epoch 76 | loss: 0.33719 | val_0_auc: 0.60885 |  0:08:10s\n",
      "epoch 77 | loss: 0.33351 | val_0_auc: 0.60921 |  0:08:17s\n",
      "epoch 78 | loss: 0.33477 | val_0_auc: 0.6054  |  0:08:24s\n",
      "epoch 79 | loss: 0.33208 | val_0_auc: 0.5968  |  0:08:31s\n",
      "epoch 80 | loss: 0.33281 | val_0_auc: 0.60055 |  0:08:37s\n",
      "epoch 81 | loss: 0.33278 | val_0_auc: 0.6026  |  0:08:43s\n",
      "epoch 82 | loss: 0.33151 | val_0_auc: 0.60146 |  0:08:49s\n",
      "epoch 83 | loss: 0.33278 | val_0_auc: 0.60724 |  0:08:55s\n",
      "epoch 84 | loss: 0.33321 | val_0_auc: 0.61589 |  0:09:01s\n",
      "epoch 85 | loss: 0.33176 | val_0_auc: 0.61735 |  0:09:08s\n",
      "epoch 86 | loss: 0.3332  | val_0_auc: 0.62144 |  0:09:14s\n",
      "epoch 87 | loss: 0.33371 | val_0_auc: 0.61197 |  0:09:21s\n",
      "epoch 88 | loss: 0.33299 | val_0_auc: 0.61501 |  0:09:28s\n",
      "epoch 89 | loss: 0.32998 | val_0_auc: 0.61768 |  0:09:35s\n",
      "epoch 90 | loss: 0.33369 | val_0_auc: 0.62353 |  0:09:42s\n",
      "epoch 91 | loss: 0.33226 | val_0_auc: 0.61287 |  0:09:48s\n",
      "epoch 92 | loss: 0.33195 | val_0_auc: 0.60946 |  0:09:54s\n",
      "epoch 93 | loss: 0.33217 | val_0_auc: 0.60855 |  0:09:59s\n",
      "epoch 94 | loss: 0.33074 | val_0_auc: 0.60647 |  0:10:06s\n",
      "epoch 95 | loss: 0.3307  | val_0_auc: 0.60095 |  0:10:12s\n",
      "epoch 96 | loss: 0.32944 | val_0_auc: 0.59937 |  0:10:18s\n",
      "epoch 97 | loss: 0.3289  | val_0_auc: 0.60556 |  0:10:24s\n",
      "epoch 98 | loss: 0.33016 | val_0_auc: 0.60814 |  0:10:31s\n",
      "epoch 99 | loss: 0.32998 | val_0_auc: 0.61415 |  0:10:38s\n",
      "epoch 100| loss: 0.33142 | val_0_auc: 0.60672 |  0:10:44s\n",
      "epoch 101| loss: 0.32934 | val_0_auc: 0.61619 |  0:10:51s\n",
      "epoch 102| loss: 0.32881 | val_0_auc: 0.62546 |  0:10:56s\n",
      "epoch 103| loss: 0.32881 | val_0_auc: 0.61009 |  0:11:02s\n",
      "epoch 104| loss: 0.32829 | val_0_auc: 0.6127  |  0:11:08s\n",
      "epoch 105| loss: 0.32948 | val_0_auc: 0.6233  |  0:11:14s\n",
      "epoch 106| loss: 0.32892 | val_0_auc: 0.61789 |  0:11:20s\n",
      "epoch 107| loss: 0.32892 | val_0_auc: 0.61984 |  0:11:27s\n",
      "epoch 108| loss: 0.32862 | val_0_auc: 0.62592 |  0:11:34s\n",
      "epoch 109| loss: 0.32823 | val_0_auc: 0.62923 |  0:11:39s\n",
      "epoch 110| loss: 0.32815 | val_0_auc: 0.62063 |  0:11:46s\n",
      "epoch 111| loss: 0.32782 | val_0_auc: 0.62008 |  0:11:52s\n",
      "epoch 112| loss: 0.32935 | val_0_auc: 0.62427 |  0:11:58s\n",
      "epoch 113| loss: 0.32777 | val_0_auc: 0.62985 |  0:12:04s\n",
      "epoch 114| loss: 0.32748 | val_0_auc: 0.62968 |  0:12:10s\n",
      "epoch 115| loss: 0.32792 | val_0_auc: 0.62818 |  0:12:17s\n",
      "epoch 116| loss: 0.32922 | val_0_auc: 0.63651 |  0:12:24s\n",
      "epoch 117| loss: 0.32724 | val_0_auc: 0.63445 |  0:12:31s\n",
      "epoch 118| loss: 0.32706 | val_0_auc: 0.63186 |  0:12:37s\n",
      "epoch 119| loss: 0.32662 | val_0_auc: 0.63305 |  0:12:43s\n",
      "epoch 120| loss: 0.32603 | val_0_auc: 0.6287  |  0:12:50s\n",
      "epoch 121| loss: 0.328   | val_0_auc: 0.62498 |  0:12:57s\n",
      "epoch 122| loss: 0.32671 | val_0_auc: 0.62209 |  0:13:05s\n",
      "epoch 123| loss: 0.32556 | val_0_auc: 0.63121 |  0:13:11s\n",
      "epoch 124| loss: 0.32546 | val_0_auc: 0.63487 |  0:13:17s\n",
      "epoch 125| loss: 0.32654 | val_0_auc: 0.62959 |  0:13:22s\n",
      "epoch 126| loss: 0.32473 | val_0_auc: 0.63343 |  0:13:28s\n",
      "epoch 127| loss: 0.32603 | val_0_auc: 0.63527 |  0:13:34s\n",
      "epoch 128| loss: 0.32405 | val_0_auc: 0.63763 |  0:13:41s\n",
      "epoch 129| loss: 0.32354 | val_0_auc: 0.63442 |  0:13:48s\n",
      "epoch 130| loss: 0.32416 | val_0_auc: 0.6377  |  0:13:55s\n",
      "epoch 131| loss: 0.32577 | val_0_auc: 0.64301 |  0:14:01s\n",
      "epoch 132| loss: 0.32314 | val_0_auc: 0.63943 |  0:14:07s\n",
      "epoch 133| loss: 0.3238  | val_0_auc: 0.6372  |  0:14:13s\n",
      "epoch 134| loss: 0.32339 | val_0_auc: 0.63411 |  0:14:20s\n",
      "epoch 135| loss: 0.32281 | val_0_auc: 0.63584 |  0:14:26s\n",
      "epoch 136| loss: 0.32338 | val_0_auc: 0.64076 |  0:14:31s\n",
      "epoch 137| loss: 0.32301 | val_0_auc: 0.63712 |  0:14:37s\n",
      "epoch 138| loss: 0.32417 | val_0_auc: 0.63819 |  0:14:43s\n",
      "epoch 139| loss: 0.32325 | val_0_auc: 0.64258 |  0:14:50s\n",
      "epoch 140| loss: 0.32043 | val_0_auc: 0.64391 |  0:14:57s\n",
      "epoch 141| loss: 0.32336 | val_0_auc: 0.63592 |  0:15:04s\n",
      "epoch 142| loss: 0.32024 | val_0_auc: 0.6319  |  0:15:12s\n",
      "epoch 143| loss: 0.32217 | val_0_auc: 0.63299 |  0:15:18s\n",
      "epoch 144| loss: 0.32188 | val_0_auc: 0.63719 |  0:15:24s\n",
      "epoch 145| loss: 0.32129 | val_0_auc: 0.63776 |  0:15:30s\n",
      "epoch 146| loss: 0.32263 | val_0_auc: 0.63077 |  0:15:35s\n",
      "epoch 147| loss: 0.32079 | val_0_auc: 0.62546 |  0:15:41s\n",
      "epoch 148| loss: 0.32219 | val_0_auc: 0.62861 |  0:15:47s\n",
      "epoch 149| loss: 0.32186 | val_0_auc: 0.63255 |  0:15:53s\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 140 and best_val_0_auc = 0.64391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 01:37:54,209] Trial 26 finished with value: 0.6439057763128447 and parameters: {'n_d': 36, 'n_a': 61, 'n_steps': 4, 'gamma': 2.3822502122916203, 'lambda_sparse': 0.00023153575603720476, 'lr': 0.0014550987086816701}. Best is trial 26 with value: 0.6439057763128447.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69144 | val_0_auc: 0.47782 |  0:00:06s\n",
      "epoch 1  | loss: 0.50699 | val_0_auc: 0.46836 |  0:00:12s\n",
      "epoch 2  | loss: 0.47585 | val_0_auc: 0.48971 |  0:00:18s\n",
      "epoch 3  | loss: 0.44992 | val_0_auc: 0.49872 |  0:00:25s\n",
      "epoch 4  | loss: 0.43483 | val_0_auc: 0.50112 |  0:00:30s\n",
      "epoch 5  | loss: 0.4099  | val_0_auc: 0.51072 |  0:00:36s\n",
      "epoch 6  | loss: 0.40739 | val_0_auc: 0.50874 |  0:00:41s\n",
      "epoch 7  | loss: 0.39632 | val_0_auc: 0.51027 |  0:00:46s\n",
      "epoch 8  | loss: 0.38955 | val_0_auc: 0.51595 |  0:00:51s\n",
      "epoch 9  | loss: 0.38056 | val_0_auc: 0.52392 |  0:00:57s\n",
      "epoch 10 | loss: 0.3826  | val_0_auc: 0.52784 |  0:01:02s\n",
      "epoch 11 | loss: 0.3729  | val_0_auc: 0.54687 |  0:01:08s\n",
      "epoch 12 | loss: 0.36921 | val_0_auc: 0.5471  |  0:01:14s\n",
      "epoch 13 | loss: 0.36941 | val_0_auc: 0.54099 |  0:01:21s\n",
      "epoch 14 | loss: 0.36202 | val_0_auc: 0.54188 |  0:01:27s\n",
      "epoch 15 | loss: 0.35935 | val_0_auc: 0.54652 |  0:01:32s\n",
      "epoch 16 | loss: 0.35905 | val_0_auc: 0.55903 |  0:01:38s\n",
      "epoch 17 | loss: 0.35875 | val_0_auc: 0.56031 |  0:01:44s\n",
      "epoch 18 | loss: 0.35256 | val_0_auc: 0.57509 |  0:01:50s\n",
      "epoch 19 | loss: 0.35547 | val_0_auc: 0.57901 |  0:01:57s\n",
      "epoch 20 | loss: 0.35356 | val_0_auc: 0.57291 |  0:02:02s\n",
      "epoch 21 | loss: 0.34888 | val_0_auc: 0.57691 |  0:02:08s\n",
      "epoch 22 | loss: 0.35001 | val_0_auc: 0.57784 |  0:02:13s\n",
      "epoch 23 | loss: 0.35014 | val_0_auc: 0.57253 |  0:02:18s\n",
      "epoch 24 | loss: 0.34185 | val_0_auc: 0.58459 |  0:02:24s\n",
      "epoch 25 | loss: 0.34672 | val_0_auc: 0.58691 |  0:02:30s\n",
      "epoch 26 | loss: 0.34577 | val_0_auc: 0.59198 |  0:02:36s\n",
      "epoch 27 | loss: 0.35032 | val_0_auc: 0.59127 |  0:02:43s\n",
      "epoch 28 | loss: 0.3431  | val_0_auc: 0.57456 |  0:02:49s\n",
      "epoch 29 | loss: 0.34412 | val_0_auc: 0.57283 |  0:02:55s\n",
      "epoch 30 | loss: 0.34125 | val_0_auc: 0.57348 |  0:03:00s\n",
      "epoch 31 | loss: 0.33719 | val_0_auc: 0.58009 |  0:03:06s\n",
      "epoch 32 | loss: 0.34162 | val_0_auc: 0.582   |  0:03:11s\n",
      "epoch 33 | loss: 0.33983 | val_0_auc: 0.59866 |  0:03:16s\n",
      "epoch 34 | loss: 0.33389 | val_0_auc: 0.59085 |  0:03:22s\n",
      "epoch 35 | loss: 0.3386  | val_0_auc: 0.59186 |  0:03:28s\n",
      "epoch 36 | loss: 0.33764 | val_0_auc: 0.59126 |  0:03:34s\n",
      "epoch 37 | loss: 0.33683 | val_0_auc: 0.59623 |  0:03:40s\n",
      "epoch 38 | loss: 0.33485 | val_0_auc: 0.60179 |  0:03:46s\n",
      "epoch 39 | loss: 0.33397 | val_0_auc: 0.5917  |  0:03:53s\n",
      "epoch 40 | loss: 0.33533 | val_0_auc: 0.59358 |  0:03:58s\n",
      "epoch 41 | loss: 0.33703 | val_0_auc: 0.59353 |  0:04:04s\n",
      "epoch 42 | loss: 0.33759 | val_0_auc: 0.60099 |  0:04:10s\n",
      "epoch 43 | loss: 0.334   | val_0_auc: 0.60727 |  0:04:14s\n",
      "epoch 44 | loss: 0.33344 | val_0_auc: 0.5974  |  0:04:20s\n",
      "epoch 45 | loss: 0.33406 | val_0_auc: 0.59663 |  0:04:25s\n",
      "epoch 46 | loss: 0.33614 | val_0_auc: 0.6078  |  0:04:31s\n",
      "epoch 47 | loss: 0.33259 | val_0_auc: 0.60007 |  0:04:36s\n",
      "epoch 48 | loss: 0.33522 | val_0_auc: 0.60971 |  0:04:41s\n",
      "epoch 49 | loss: 0.33179 | val_0_auc: 0.6123  |  0:04:47s\n",
      "epoch 50 | loss: 0.33259 | val_0_auc: 0.61015 |  0:04:53s\n",
      "epoch 51 | loss: 0.33292 | val_0_auc: 0.6208  |  0:05:00s\n",
      "epoch 52 | loss: 0.33104 | val_0_auc: 0.6141  |  0:05:06s\n",
      "epoch 53 | loss: 0.33056 | val_0_auc: 0.61354 |  0:05:12s\n",
      "epoch 54 | loss: 0.33046 | val_0_auc: 0.60741 |  0:05:18s\n",
      "epoch 55 | loss: 0.33159 | val_0_auc: 0.60469 |  0:05:24s\n",
      "epoch 56 | loss: 0.3295  | val_0_auc: 0.60828 |  0:05:29s\n",
      "epoch 57 | loss: 0.3297  | val_0_auc: 0.60704 |  0:05:34s\n",
      "epoch 58 | loss: 0.32903 | val_0_auc: 0.61163 |  0:05:40s\n",
      "epoch 59 | loss: 0.33105 | val_0_auc: 0.61523 |  0:05:45s\n",
      "epoch 60 | loss: 0.32887 | val_0_auc: 0.61834 |  0:05:51s\n",
      "epoch 61 | loss: 0.33123 | val_0_auc: 0.61227 |  0:05:56s\n",
      "epoch 62 | loss: 0.33064 | val_0_auc: 0.62222 |  0:06:01s\n",
      "epoch 63 | loss: 0.33158 | val_0_auc: 0.62106 |  0:06:07s\n",
      "epoch 64 | loss: 0.32989 | val_0_auc: 0.63042 |  0:06:13s\n",
      "epoch 65 | loss: 0.32846 | val_0_auc: 0.62285 |  0:06:19s\n",
      "epoch 66 | loss: 0.32933 | val_0_auc: 0.61919 |  0:06:26s\n",
      "epoch 67 | loss: 0.3304  | val_0_auc: 0.62236 |  0:06:32s\n",
      "epoch 68 | loss: 0.32985 | val_0_auc: 0.63057 |  0:06:39s\n",
      "epoch 69 | loss: 0.32948 | val_0_auc: 0.62968 |  0:06:45s\n",
      "epoch 70 | loss: 0.327   | val_0_auc: 0.62372 |  0:06:51s\n",
      "epoch 71 | loss: 0.32885 | val_0_auc: 0.62318 |  0:06:58s\n",
      "epoch 72 | loss: 0.3274  | val_0_auc: 0.62542 |  0:07:04s\n",
      "epoch 73 | loss: 0.32683 | val_0_auc: 0.62698 |  0:07:09s\n",
      "epoch 74 | loss: 0.32852 | val_0_auc: 0.63372 |  0:07:15s\n",
      "epoch 75 | loss: 0.32812 | val_0_auc: 0.62799 |  0:07:20s\n",
      "epoch 76 | loss: 0.32677 | val_0_auc: 0.63135 |  0:07:26s\n",
      "epoch 77 | loss: 0.3242  | val_0_auc: 0.63077 |  0:07:31s\n",
      "epoch 78 | loss: 0.32722 | val_0_auc: 0.6287  |  0:07:37s\n",
      "epoch 79 | loss: 0.32656 | val_0_auc: 0.62717 |  0:07:43s\n",
      "epoch 80 | loss: 0.32574 | val_0_auc: 0.63475 |  0:07:49s\n",
      "epoch 81 | loss: 0.32561 | val_0_auc: 0.63746 |  0:07:56s\n",
      "epoch 82 | loss: 0.32521 | val_0_auc: 0.64443 |  0:08:02s\n",
      "epoch 83 | loss: 0.32576 | val_0_auc: 0.64778 |  0:08:08s\n",
      "epoch 84 | loss: 0.3244  | val_0_auc: 0.64716 |  0:08:14s\n",
      "epoch 85 | loss: 0.32473 | val_0_auc: 0.6375  |  0:08:20s\n",
      "epoch 86 | loss: 0.32473 | val_0_auc: 0.63393 |  0:08:25s\n",
      "epoch 87 | loss: 0.32526 | val_0_auc: 0.64136 |  0:08:30s\n",
      "epoch 88 | loss: 0.32546 | val_0_auc: 0.63541 |  0:08:36s\n",
      "epoch 89 | loss: 0.32611 | val_0_auc: 0.63974 |  0:08:41s\n",
      "epoch 90 | loss: 0.32497 | val_0_auc: 0.63572 |  0:08:46s\n",
      "epoch 91 | loss: 0.32567 | val_0_auc: 0.63655 |  0:08:52s\n",
      "epoch 92 | loss: 0.32617 | val_0_auc: 0.62873 |  0:08:58s\n",
      "epoch 93 | loss: 0.32316 | val_0_auc: 0.63183 |  0:09:04s\n",
      "epoch 94 | loss: 0.3238  | val_0_auc: 0.63121 |  0:09:11s\n",
      "epoch 95 | loss: 0.32287 | val_0_auc: 0.6316  |  0:09:16s\n",
      "epoch 96 | loss: 0.32272 | val_0_auc: 0.63114 |  0:09:21s\n",
      "epoch 97 | loss: 0.32506 | val_0_auc: 0.62938 |  0:09:26s\n",
      "epoch 98 | loss: 0.32553 | val_0_auc: 0.62977 |  0:09:32s\n",
      "\n",
      "Early stopping occurred at epoch 98 with best_epoch = 83 and best_val_0_auc = 0.64778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 01:47:29,224] Trial 27 finished with value: 0.6477814909764187 and parameters: {'n_d': 36, 'n_a': 37, 'n_steps': 4, 'gamma': 2.0710944796800606, 'lambda_sparse': 0.00026629055145732644, 'lr': 0.0021613813807179944}. Best is trial 27 with value: 0.6477814909764187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.63022 | val_0_auc: 0.46854 |  0:00:05s\n",
      "epoch 1  | loss: 0.55979 | val_0_auc: 0.45848 |  0:00:11s\n",
      "epoch 2  | loss: 0.51286 | val_0_auc: 0.46856 |  0:00:18s\n",
      "epoch 3  | loss: 0.49895 | val_0_auc: 0.45504 |  0:00:24s\n",
      "epoch 4  | loss: 0.47713 | val_0_auc: 0.45849 |  0:00:30s\n",
      "epoch 5  | loss: 0.47105 | val_0_auc: 0.46899 |  0:00:36s\n",
      "epoch 6  | loss: 0.46666 | val_0_auc: 0.47027 |  0:00:42s\n",
      "epoch 7  | loss: 0.45047 | val_0_auc: 0.46574 |  0:00:47s\n",
      "epoch 8  | loss: 0.4426  | val_0_auc: 0.47716 |  0:00:52s\n",
      "epoch 9  | loss: 0.44096 | val_0_auc: 0.47279 |  0:00:57s\n",
      "epoch 10 | loss: 0.43511 | val_0_auc: 0.48871 |  0:01:02s\n",
      "epoch 11 | loss: 0.41955 | val_0_auc: 0.4927  |  0:01:07s\n",
      "epoch 12 | loss: 0.4272  | val_0_auc: 0.49069 |  0:01:14s\n",
      "epoch 13 | loss: 0.42156 | val_0_auc: 0.49188 |  0:01:20s\n",
      "epoch 14 | loss: 0.41235 | val_0_auc: 0.48745 |  0:01:26s\n",
      "epoch 15 | loss: 0.41034 | val_0_auc: 0.49662 |  0:01:33s\n",
      "epoch 16 | loss: 0.40151 | val_0_auc: 0.49474 |  0:01:40s\n",
      "epoch 17 | loss: 0.39523 | val_0_auc: 0.50981 |  0:01:45s\n",
      "epoch 18 | loss: 0.39266 | val_0_auc: 0.50535 |  0:01:51s\n",
      "epoch 19 | loss: 0.3934  | val_0_auc: 0.50426 |  0:01:56s\n",
      "epoch 20 | loss: 0.39072 | val_0_auc: 0.51098 |  0:02:01s\n",
      "epoch 21 | loss: 0.38863 | val_0_auc: 0.51819 |  0:02:07s\n",
      "epoch 22 | loss: 0.38103 | val_0_auc: 0.52589 |  0:02:12s\n",
      "epoch 23 | loss: 0.37619 | val_0_auc: 0.52557 |  0:02:18s\n",
      "epoch 24 | loss: 0.38052 | val_0_auc: 0.52217 |  0:02:24s\n",
      "epoch 25 | loss: 0.37196 | val_0_auc: 0.52439 |  0:02:30s\n",
      "epoch 26 | loss: 0.37858 | val_0_auc: 0.5281  |  0:02:36s\n",
      "epoch 27 | loss: 0.37451 | val_0_auc: 0.52461 |  0:02:42s\n",
      "epoch 28 | loss: 0.36789 | val_0_auc: 0.51986 |  0:02:48s\n",
      "epoch 29 | loss: 0.36922 | val_0_auc: 0.52192 |  0:02:54s\n",
      "epoch 30 | loss: 0.36802 | val_0_auc: 0.52842 |  0:03:00s\n",
      "epoch 31 | loss: 0.36556 | val_0_auc: 0.53209 |  0:03:06s\n",
      "epoch 32 | loss: 0.36677 | val_0_auc: 0.54223 |  0:03:12s\n",
      "epoch 33 | loss: 0.36543 | val_0_auc: 0.53851 |  0:03:17s\n",
      "epoch 34 | loss: 0.36215 | val_0_auc: 0.54233 |  0:03:22s\n",
      "epoch 35 | loss: 0.36447 | val_0_auc: 0.54579 |  0:03:27s\n",
      "epoch 36 | loss: 0.36536 | val_0_auc: 0.538   |  0:03:33s\n",
      "epoch 37 | loss: 0.3613  | val_0_auc: 0.54063 |  0:03:39s\n",
      "epoch 38 | loss: 0.36015 | val_0_auc: 0.54034 |  0:03:44s\n",
      "epoch 39 | loss: 0.35708 | val_0_auc: 0.54189 |  0:03:50s\n",
      "epoch 40 | loss: 0.35638 | val_0_auc: 0.54577 |  0:03:56s\n",
      "epoch 41 | loss: 0.35712 | val_0_auc: 0.5475  |  0:04:02s\n",
      "epoch 42 | loss: 0.35577 | val_0_auc: 0.55451 |  0:04:08s\n",
      "epoch 43 | loss: 0.35072 | val_0_auc: 0.55401 |  0:04:15s\n",
      "epoch 44 | loss: 0.35697 | val_0_auc: 0.55863 |  0:04:21s\n",
      "epoch 45 | loss: 0.35443 | val_0_auc: 0.55335 |  0:04:27s\n",
      "epoch 46 | loss: 0.35281 | val_0_auc: 0.55972 |  0:04:32s\n",
      "epoch 47 | loss: 0.35407 | val_0_auc: 0.56223 |  0:04:39s\n",
      "epoch 48 | loss: 0.35277 | val_0_auc: 0.55666 |  0:04:45s\n",
      "epoch 49 | loss: 0.34715 | val_0_auc: 0.56426 |  0:04:50s\n",
      "epoch 50 | loss: 0.34793 | val_0_auc: 0.55735 |  0:04:55s\n",
      "epoch 51 | loss: 0.34814 | val_0_auc: 0.56027 |  0:05:00s\n",
      "epoch 52 | loss: 0.34899 | val_0_auc: 0.5621  |  0:05:05s\n",
      "epoch 53 | loss: 0.34395 | val_0_auc: 0.56323 |  0:05:10s\n",
      "epoch 54 | loss: 0.34667 | val_0_auc: 0.56578 |  0:05:16s\n",
      "epoch 55 | loss: 0.34647 | val_0_auc: 0.56252 |  0:05:21s\n",
      "epoch 56 | loss: 0.34525 | val_0_auc: 0.56073 |  0:05:27s\n",
      "epoch 57 | loss: 0.34372 | val_0_auc: 0.56407 |  0:05:33s\n",
      "epoch 58 | loss: 0.34626 | val_0_auc: 0.5616  |  0:05:40s\n",
      "epoch 59 | loss: 0.34624 | val_0_auc: 0.56005 |  0:05:46s\n",
      "epoch 60 | loss: 0.34238 | val_0_auc: 0.56215 |  0:05:52s\n",
      "epoch 61 | loss: 0.342   | val_0_auc: 0.56753 |  0:05:58s\n",
      "epoch 62 | loss: 0.34273 | val_0_auc: 0.55994 |  0:06:03s\n",
      "epoch 63 | loss: 0.34414 | val_0_auc: 0.56344 |  0:06:08s\n",
      "epoch 64 | loss: 0.34236 | val_0_auc: 0.56892 |  0:06:13s\n",
      "epoch 65 | loss: 0.3413  | val_0_auc: 0.56425 |  0:06:18s\n",
      "epoch 66 | loss: 0.34324 | val_0_auc: 0.56608 |  0:06:23s\n",
      "epoch 67 | loss: 0.34356 | val_0_auc: 0.57439 |  0:06:28s\n",
      "epoch 68 | loss: 0.33928 | val_0_auc: 0.57781 |  0:06:34s\n",
      "epoch 69 | loss: 0.33919 | val_0_auc: 0.58303 |  0:06:40s\n",
      "epoch 70 | loss: 0.34051 | val_0_auc: 0.5861  |  0:06:46s\n",
      "epoch 71 | loss: 0.3419  | val_0_auc: 0.57328 |  0:06:53s\n",
      "epoch 72 | loss: 0.33857 | val_0_auc: 0.58154 |  0:06:59s\n",
      "epoch 73 | loss: 0.33462 | val_0_auc: 0.58239 |  0:07:05s\n",
      "epoch 74 | loss: 0.34207 | val_0_auc: 0.57871 |  0:07:12s\n",
      "epoch 75 | loss: 0.33902 | val_0_auc: 0.58235 |  0:07:18s\n",
      "epoch 76 | loss: 0.3399  | val_0_auc: 0.58611 |  0:07:23s\n",
      "epoch 77 | loss: 0.34026 | val_0_auc: 0.58377 |  0:07:28s\n",
      "epoch 78 | loss: 0.33714 | val_0_auc: 0.57945 |  0:07:34s\n",
      "epoch 79 | loss: 0.33847 | val_0_auc: 0.58352 |  0:07:39s\n",
      "epoch 80 | loss: 0.33844 | val_0_auc: 0.57566 |  0:07:44s\n",
      "epoch 81 | loss: 0.33652 | val_0_auc: 0.57735 |  0:07:50s\n",
      "epoch 82 | loss: 0.33774 | val_0_auc: 0.57977 |  0:07:56s\n",
      "epoch 83 | loss: 0.34011 | val_0_auc: 0.57308 |  0:08:02s\n",
      "epoch 84 | loss: 0.33795 | val_0_auc: 0.57623 |  0:08:08s\n",
      "epoch 85 | loss: 0.33681 | val_0_auc: 0.57444 |  0:08:14s\n",
      "epoch 86 | loss: 0.33649 | val_0_auc: 0.57454 |  0:08:20s\n",
      "epoch 87 | loss: 0.33714 | val_0_auc: 0.58163 |  0:08:26s\n",
      "epoch 88 | loss: 0.3368  | val_0_auc: 0.57881 |  0:08:31s\n",
      "epoch 89 | loss: 0.33818 | val_0_auc: 0.58366 |  0:08:36s\n",
      "epoch 90 | loss: 0.33721 | val_0_auc: 0.5876  |  0:08:41s\n",
      "epoch 91 | loss: 0.33356 | val_0_auc: 0.57981 |  0:08:47s\n",
      "epoch 92 | loss: 0.33487 | val_0_auc: 0.57769 |  0:08:53s\n",
      "epoch 93 | loss: 0.3342  | val_0_auc: 0.58285 |  0:08:58s\n",
      "epoch 94 | loss: 0.33533 | val_0_auc: 0.58387 |  0:09:04s\n",
      "epoch 95 | loss: 0.3345  | val_0_auc: 0.58806 |  0:09:09s\n",
      "epoch 96 | loss: 0.33384 | val_0_auc: 0.58773 |  0:09:15s\n",
      "epoch 97 | loss: 0.33381 | val_0_auc: 0.59202 |  0:09:20s\n",
      "epoch 98 | loss: 0.33361 | val_0_auc: 0.60111 |  0:09:27s\n",
      "epoch 99 | loss: 0.33403 | val_0_auc: 0.5975  |  0:09:33s\n",
      "epoch 100| loss: 0.33353 | val_0_auc: 0.59187 |  0:09:40s\n",
      "epoch 101| loss: 0.33643 | val_0_auc: 0.59015 |  0:09:46s\n",
      "epoch 102| loss: 0.33428 | val_0_auc: 0.5882  |  0:09:53s\n",
      "epoch 103| loss: 0.33145 | val_0_auc: 0.59515 |  0:09:58s\n",
      "epoch 104| loss: 0.33453 | val_0_auc: 0.5863  |  0:10:04s\n",
      "epoch 105| loss: 0.33436 | val_0_auc: 0.58523 |  0:10:09s\n",
      "epoch 106| loss: 0.33404 | val_0_auc: 0.59034 |  0:10:14s\n",
      "epoch 107| loss: 0.33291 | val_0_auc: 0.59186 |  0:10:19s\n",
      "epoch 108| loss: 0.33188 | val_0_auc: 0.59683 |  0:10:24s\n",
      "epoch 109| loss: 0.33532 | val_0_auc: 0.58437 |  0:10:30s\n",
      "epoch 110| loss: 0.33373 | val_0_auc: 0.58242 |  0:10:36s\n",
      "epoch 111| loss: 0.33436 | val_0_auc: 0.5928  |  0:10:42s\n",
      "epoch 112| loss: 0.33255 | val_0_auc: 0.58303 |  0:10:48s\n",
      "epoch 113| loss: 0.33364 | val_0_auc: 0.58265 |  0:10:54s\n",
      "\n",
      "Early stopping occurred at epoch 113 with best_epoch = 98 and best_val_0_auc = 0.60111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 01:58:26,117] Trial 28 finished with value: 0.6011060754033654 and parameters: {'n_d': 36, 'n_a': 34, 'n_steps': 4, 'gamma': 2.013362324568173, 'lambda_sparse': 0.00035397175196197437, 'lr': 0.0006876061461543996}. Best is trial 27 with value: 0.6477814909764187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6952  | val_0_auc: 0.48243 |  0:00:04s\n",
      "epoch 1  | loss: 0.54592 | val_0_auc: 0.46354 |  0:00:09s\n",
      "epoch 2  | loss: 0.50276 | val_0_auc: 0.47412 |  0:00:14s\n",
      "epoch 3  | loss: 0.49947 | val_0_auc: 0.4789  |  0:00:19s\n",
      "epoch 4  | loss: 0.48019 | val_0_auc: 0.47942 |  0:00:23s\n",
      "epoch 5  | loss: 0.45678 | val_0_auc: 0.47935 |  0:00:28s\n",
      "epoch 6  | loss: 0.43542 | val_0_auc: 0.48881 |  0:00:33s\n",
      "epoch 7  | loss: 0.42134 | val_0_auc: 0.4996  |  0:00:39s\n",
      "epoch 8  | loss: 0.42177 | val_0_auc: 0.50974 |  0:00:44s\n",
      "epoch 9  | loss: 0.40875 | val_0_auc: 0.51934 |  0:00:50s\n",
      "epoch 10 | loss: 0.40738 | val_0_auc: 0.52716 |  0:00:56s\n",
      "epoch 11 | loss: 0.39365 | val_0_auc: 0.53053 |  0:01:03s\n",
      "epoch 12 | loss: 0.38954 | val_0_auc: 0.53529 |  0:01:08s\n",
      "epoch 13 | loss: 0.38076 | val_0_auc: 0.52029 |  0:01:13s\n",
      "epoch 14 | loss: 0.38583 | val_0_auc: 0.54009 |  0:01:18s\n",
      "epoch 15 | loss: 0.3743  | val_0_auc: 0.54456 |  0:01:23s\n",
      "epoch 16 | loss: 0.37118 | val_0_auc: 0.55441 |  0:01:28s\n",
      "epoch 17 | loss: 0.37139 | val_0_auc: 0.55331 |  0:01:33s\n",
      "epoch 18 | loss: 0.37243 | val_0_auc: 0.5567  |  0:01:37s\n",
      "epoch 19 | loss: 0.36406 | val_0_auc: 0.55746 |  0:01:42s\n",
      "epoch 20 | loss: 0.36012 | val_0_auc: 0.57452 |  0:01:47s\n",
      "epoch 21 | loss: 0.3581  | val_0_auc: 0.56256 |  0:01:52s\n",
      "epoch 22 | loss: 0.3611  | val_0_auc: 0.56069 |  0:01:57s\n",
      "epoch 23 | loss: 0.35278 | val_0_auc: 0.55541 |  0:02:03s\n",
      "epoch 24 | loss: 0.35394 | val_0_auc: 0.55824 |  0:02:09s\n",
      "epoch 25 | loss: 0.3573  | val_0_auc: 0.55499 |  0:02:14s\n",
      "epoch 26 | loss: 0.35043 | val_0_auc: 0.56437 |  0:02:20s\n",
      "epoch 27 | loss: 0.34938 | val_0_auc: 0.56012 |  0:02:26s\n",
      "epoch 28 | loss: 0.34619 | val_0_auc: 0.55653 |  0:02:32s\n",
      "epoch 29 | loss: 0.35076 | val_0_auc: 0.5578  |  0:02:37s\n",
      "epoch 30 | loss: 0.34773 | val_0_auc: 0.54859 |  0:02:43s\n",
      "epoch 31 | loss: 0.34813 | val_0_auc: 0.56474 |  0:02:48s\n",
      "epoch 32 | loss: 0.34593 | val_0_auc: 0.5713  |  0:02:53s\n",
      "epoch 33 | loss: 0.34739 | val_0_auc: 0.56443 |  0:02:58s\n",
      "epoch 34 | loss: 0.3439  | val_0_auc: 0.57375 |  0:03:02s\n",
      "epoch 35 | loss: 0.34639 | val_0_auc: 0.58377 |  0:03:07s\n",
      "epoch 36 | loss: 0.34491 | val_0_auc: 0.58189 |  0:03:12s\n",
      "epoch 37 | loss: 0.34242 | val_0_auc: 0.59272 |  0:03:17s\n",
      "epoch 38 | loss: 0.34192 | val_0_auc: 0.58716 |  0:03:23s\n",
      "epoch 39 | loss: 0.34222 | val_0_auc: 0.57975 |  0:03:29s\n",
      "epoch 40 | loss: 0.33866 | val_0_auc: 0.57863 |  0:03:35s\n",
      "epoch 41 | loss: 0.3439  | val_0_auc: 0.59239 |  0:03:40s\n",
      "epoch 42 | loss: 0.33814 | val_0_auc: 0.58911 |  0:03:45s\n",
      "epoch 43 | loss: 0.3384  | val_0_auc: 0.59827 |  0:03:50s\n",
      "epoch 44 | loss: 0.34045 | val_0_auc: 0.59788 |  0:03:55s\n",
      "epoch 45 | loss: 0.33894 | val_0_auc: 0.58683 |  0:03:59s\n",
      "epoch 46 | loss: 0.33944 | val_0_auc: 0.57841 |  0:04:04s\n",
      "epoch 47 | loss: 0.34028 | val_0_auc: 0.58438 |  0:04:09s\n",
      "epoch 48 | loss: 0.3373  | val_0_auc: 0.58527 |  0:04:14s\n",
      "epoch 49 | loss: 0.33917 | val_0_auc: 0.59174 |  0:04:19s\n",
      "epoch 50 | loss: 0.33761 | val_0_auc: 0.57662 |  0:04:25s\n",
      "epoch 51 | loss: 0.33982 | val_0_auc: 0.59522 |  0:04:31s\n",
      "epoch 52 | loss: 0.3367  | val_0_auc: 0.59433 |  0:04:37s\n",
      "epoch 53 | loss: 0.33668 | val_0_auc: 0.59106 |  0:04:42s\n",
      "epoch 54 | loss: 0.33756 | val_0_auc: 0.5864  |  0:04:48s\n",
      "epoch 55 | loss: 0.3359  | val_0_auc: 0.59019 |  0:04:53s\n",
      "epoch 56 | loss: 0.33345 | val_0_auc: 0.59342 |  0:04:57s\n",
      "epoch 57 | loss: 0.33732 | val_0_auc: 0.60125 |  0:05:02s\n",
      "epoch 58 | loss: 0.33422 | val_0_auc: 0.60532 |  0:05:07s\n",
      "epoch 59 | loss: 0.33597 | val_0_auc: 0.59824 |  0:05:11s\n",
      "epoch 60 | loss: 0.33402 | val_0_auc: 0.59251 |  0:05:16s\n",
      "epoch 61 | loss: 0.33241 | val_0_auc: 0.59922 |  0:05:22s\n",
      "epoch 62 | loss: 0.33551 | val_0_auc: 0.6081  |  0:05:27s\n",
      "epoch 63 | loss: 0.33509 | val_0_auc: 0.60753 |  0:05:32s\n",
      "epoch 64 | loss: 0.33419 | val_0_auc: 0.59915 |  0:05:37s\n",
      "epoch 65 | loss: 0.33311 | val_0_auc: 0.60322 |  0:05:42s\n",
      "epoch 66 | loss: 0.33249 | val_0_auc: 0.59776 |  0:05:48s\n",
      "epoch 67 | loss: 0.33332 | val_0_auc: 0.60733 |  0:05:54s\n",
      "epoch 68 | loss: 0.33479 | val_0_auc: 0.61501 |  0:06:00s\n",
      "epoch 69 | loss: 0.33199 | val_0_auc: 0.60504 |  0:06:06s\n",
      "epoch 70 | loss: 0.332   | val_0_auc: 0.60091 |  0:06:12s\n",
      "epoch 71 | loss: 0.32962 | val_0_auc: 0.60538 |  0:06:17s\n",
      "epoch 72 | loss: 0.33152 | val_0_auc: 0.59835 |  0:06:22s\n",
      "epoch 73 | loss: 0.33018 | val_0_auc: 0.60153 |  0:06:27s\n",
      "epoch 74 | loss: 0.33188 | val_0_auc: 0.60414 |  0:06:32s\n",
      "epoch 75 | loss: 0.32943 | val_0_auc: 0.60499 |  0:06:37s\n",
      "epoch 76 | loss: 0.33036 | val_0_auc: 0.60057 |  0:06:42s\n",
      "epoch 77 | loss: 0.33261 | val_0_auc: 0.59657 |  0:06:48s\n",
      "epoch 78 | loss: 0.32865 | val_0_auc: 0.60465 |  0:06:53s\n",
      "epoch 79 | loss: 0.33227 | val_0_auc: 0.59785 |  0:06:59s\n",
      "epoch 80 | loss: 0.33004 | val_0_auc: 0.58922 |  0:07:05s\n",
      "epoch 81 | loss: 0.33301 | val_0_auc: 0.60108 |  0:07:11s\n",
      "epoch 82 | loss: 0.32994 | val_0_auc: 0.60947 |  0:07:16s\n",
      "epoch 83 | loss: 0.32931 | val_0_auc: 0.61648 |  0:07:21s\n",
      "epoch 84 | loss: 0.32843 | val_0_auc: 0.61624 |  0:07:27s\n",
      "epoch 85 | loss: 0.33154 | val_0_auc: 0.62026 |  0:07:32s\n",
      "epoch 86 | loss: 0.33053 | val_0_auc: 0.61387 |  0:07:37s\n",
      "epoch 87 | loss: 0.33081 | val_0_auc: 0.60851 |  0:07:41s\n",
      "epoch 88 | loss: 0.32989 | val_0_auc: 0.60663 |  0:07:46s\n",
      "epoch 89 | loss: 0.32791 | val_0_auc: 0.61474 |  0:07:51s\n",
      "epoch 90 | loss: 0.3274  | val_0_auc: 0.61088 |  0:07:57s\n",
      "epoch 91 | loss: 0.32841 | val_0_auc: 0.61316 |  0:08:02s\n",
      "epoch 92 | loss: 0.32625 | val_0_auc: 0.61754 |  0:08:07s\n",
      "epoch 93 | loss: 0.32812 | val_0_auc: 0.61883 |  0:08:12s\n",
      "epoch 94 | loss: 0.32748 | val_0_auc: 0.62768 |  0:08:17s\n",
      "epoch 95 | loss: 0.32657 | val_0_auc: 0.62221 |  0:08:22s\n",
      "epoch 96 | loss: 0.32585 | val_0_auc: 0.61703 |  0:08:27s\n",
      "epoch 97 | loss: 0.32894 | val_0_auc: 0.61808 |  0:08:33s\n",
      "epoch 98 | loss: 0.32586 | val_0_auc: 0.61086 |  0:08:38s\n",
      "epoch 99 | loss: 0.32781 | val_0_auc: 0.61055 |  0:08:43s\n",
      "epoch 100| loss: 0.32554 | val_0_auc: 0.61743 |  0:08:48s\n",
      "epoch 101| loss: 0.32582 | val_0_auc: 0.61871 |  0:08:53s\n",
      "epoch 102| loss: 0.3248  | val_0_auc: 0.6187  |  0:08:58s\n",
      "epoch 103| loss: 0.32561 | val_0_auc: 0.61826 |  0:09:03s\n",
      "epoch 104| loss: 0.32781 | val_0_auc: 0.61713 |  0:09:09s\n",
      "epoch 105| loss: 0.32504 | val_0_auc: 0.61105 |  0:09:15s\n",
      "epoch 106| loss: 0.32593 | val_0_auc: 0.61328 |  0:09:20s\n",
      "epoch 107| loss: 0.32579 | val_0_auc: 0.61762 |  0:09:25s\n",
      "epoch 108| loss: 0.32572 | val_0_auc: 0.61493 |  0:09:30s\n",
      "epoch 109| loss: 0.32585 | val_0_auc: 0.62236 |  0:09:35s\n",
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 94 and best_val_0_auc = 0.62768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 02:08:03,802] Trial 29 finished with value: 0.6276812600459827 and parameters: {'n_d': 36, 'n_a': 25, 'n_steps': 4, 'gamma': 1.5054859574801902, 'lambda_sparse': 0.0009759765027053578, 'lr': 0.001615616167051018}. Best is trial 27 with value: 0.6477814909764187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67282 | val_0_auc: 0.50936 |  0:00:04s\n",
      "epoch 1  | loss: 0.55098 | val_0_auc: 0.49965 |  0:00:09s\n",
      "epoch 2  | loss: 0.48819 | val_0_auc: 0.50787 |  0:00:13s\n",
      "epoch 3  | loss: 0.47714 | val_0_auc: 0.49878 |  0:00:18s\n",
      "epoch 4  | loss: 0.43829 | val_0_auc: 0.50275 |  0:00:23s\n",
      "epoch 5  | loss: 0.42361 | val_0_auc: 0.50526 |  0:00:29s\n",
      "epoch 6  | loss: 0.41351 | val_0_auc: 0.50945 |  0:00:33s\n",
      "epoch 7  | loss: 0.4147  | val_0_auc: 0.51788 |  0:00:38s\n",
      "epoch 8  | loss: 0.40404 | val_0_auc: 0.52737 |  0:00:42s\n",
      "epoch 9  | loss: 0.40107 | val_0_auc: 0.53577 |  0:00:46s\n",
      "epoch 10 | loss: 0.39537 | val_0_auc: 0.54055 |  0:00:50s\n",
      "epoch 11 | loss: 0.38356 | val_0_auc: 0.54499 |  0:00:55s\n",
      "epoch 12 | loss: 0.38229 | val_0_auc: 0.5477  |  0:01:00s\n",
      "epoch 13 | loss: 0.37927 | val_0_auc: 0.54946 |  0:01:05s\n",
      "epoch 14 | loss: 0.37404 | val_0_auc: 0.55891 |  0:01:09s\n",
      "epoch 15 | loss: 0.37364 | val_0_auc: 0.56155 |  0:01:15s\n",
      "epoch 16 | loss: 0.37057 | val_0_auc: 0.56041 |  0:01:20s\n",
      "epoch 17 | loss: 0.36348 | val_0_auc: 0.56717 |  0:01:24s\n",
      "epoch 18 | loss: 0.36108 | val_0_auc: 0.56907 |  0:01:28s\n",
      "epoch 19 | loss: 0.36165 | val_0_auc: 0.57237 |  0:01:33s\n",
      "epoch 20 | loss: 0.3577  | val_0_auc: 0.56776 |  0:01:37s\n",
      "epoch 21 | loss: 0.35466 | val_0_auc: 0.56808 |  0:01:41s\n",
      "epoch 22 | loss: 0.35216 | val_0_auc: 0.57947 |  0:01:46s\n",
      "epoch 23 | loss: 0.3536  | val_0_auc: 0.58359 |  0:01:49s\n",
      "epoch 24 | loss: 0.35291 | val_0_auc: 0.57563 |  0:01:54s\n",
      "epoch 25 | loss: 0.34934 | val_0_auc: 0.57665 |  0:01:59s\n",
      "epoch 26 | loss: 0.3524  | val_0_auc: 0.57391 |  0:02:04s\n",
      "epoch 27 | loss: 0.34714 | val_0_auc: 0.58115 |  0:02:08s\n",
      "epoch 28 | loss: 0.34909 | val_0_auc: 0.58496 |  0:02:13s\n",
      "epoch 29 | loss: 0.34473 | val_0_auc: 0.58518 |  0:02:18s\n",
      "epoch 30 | loss: 0.3447  | val_0_auc: 0.58445 |  0:02:23s\n",
      "epoch 31 | loss: 0.34212 | val_0_auc: 0.58753 |  0:02:27s\n",
      "epoch 32 | loss: 0.34369 | val_0_auc: 0.5927  |  0:02:32s\n",
      "epoch 33 | loss: 0.3417  | val_0_auc: 0.59517 |  0:02:37s\n",
      "epoch 34 | loss: 0.34034 | val_0_auc: 0.59528 |  0:02:41s\n",
      "epoch 35 | loss: 0.33769 | val_0_auc: 0.59482 |  0:02:45s\n",
      "epoch 36 | loss: 0.33908 | val_0_auc: 0.6004  |  0:02:50s\n",
      "epoch 37 | loss: 0.33847 | val_0_auc: 0.60221 |  0:02:55s\n",
      "epoch 38 | loss: 0.33894 | val_0_auc: 0.60766 |  0:02:59s\n",
      "epoch 39 | loss: 0.33795 | val_0_auc: 0.60857 |  0:03:03s\n",
      "epoch 40 | loss: 0.3371  | val_0_auc: 0.60508 |  0:03:08s\n",
      "epoch 41 | loss: 0.33596 | val_0_auc: 0.60272 |  0:03:12s\n",
      "epoch 42 | loss: 0.3351  | val_0_auc: 0.60671 |  0:03:17s\n",
      "epoch 43 | loss: 0.33371 | val_0_auc: 0.6085  |  0:03:22s\n",
      "epoch 44 | loss: 0.33226 | val_0_auc: 0.61143 |  0:03:26s\n",
      "epoch 45 | loss: 0.33504 | val_0_auc: 0.61478 |  0:03:31s\n",
      "epoch 46 | loss: 0.33516 | val_0_auc: 0.6106  |  0:03:36s\n",
      "epoch 47 | loss: 0.33437 | val_0_auc: 0.6162  |  0:03:41s\n",
      "epoch 48 | loss: 0.3304  | val_0_auc: 0.62005 |  0:03:46s\n",
      "epoch 49 | loss: 0.33158 | val_0_auc: 0.62445 |  0:03:50s\n",
      "epoch 50 | loss: 0.33078 | val_0_auc: 0.62844 |  0:03:54s\n",
      "epoch 51 | loss: 0.3319  | val_0_auc: 0.62547 |  0:03:58s\n",
      "epoch 52 | loss: 0.33069 | val_0_auc: 0.62775 |  0:04:03s\n",
      "epoch 53 | loss: 0.32727 | val_0_auc: 0.62521 |  0:04:07s\n",
      "epoch 54 | loss: 0.32584 | val_0_auc: 0.61836 |  0:04:12s\n",
      "epoch 55 | loss: 0.32998 | val_0_auc: 0.6237  |  0:04:17s\n",
      "epoch 56 | loss: 0.3283  | val_0_auc: 0.62709 |  0:04:22s\n",
      "epoch 57 | loss: 0.32938 | val_0_auc: 0.62346 |  0:04:27s\n",
      "epoch 58 | loss: 0.32793 | val_0_auc: 0.62185 |  0:04:32s\n",
      "epoch 59 | loss: 0.32723 | val_0_auc: 0.62286 |  0:04:37s\n",
      "epoch 60 | loss: 0.327   | val_0_auc: 0.63128 |  0:04:42s\n",
      "epoch 61 | loss: 0.32603 | val_0_auc: 0.6356  |  0:04:46s\n",
      "epoch 62 | loss: 0.3254  | val_0_auc: 0.63067 |  0:04:51s\n",
      "epoch 63 | loss: 0.32805 | val_0_auc: 0.62968 |  0:04:56s\n",
      "epoch 64 | loss: 0.32604 | val_0_auc: 0.62928 |  0:05:01s\n",
      "epoch 65 | loss: 0.32692 | val_0_auc: 0.62883 |  0:05:06s\n",
      "epoch 66 | loss: 0.32816 | val_0_auc: 0.62938 |  0:05:11s\n",
      "epoch 67 | loss: 0.32648 | val_0_auc: 0.62588 |  0:05:15s\n",
      "epoch 68 | loss: 0.32906 | val_0_auc: 0.62022 |  0:05:20s\n",
      "epoch 69 | loss: 0.32724 | val_0_auc: 0.61857 |  0:05:24s\n",
      "epoch 70 | loss: 0.32534 | val_0_auc: 0.61902 |  0:05:28s\n",
      "epoch 71 | loss: 0.32752 | val_0_auc: 0.61721 |  0:05:32s\n",
      "epoch 72 | loss: 0.32665 | val_0_auc: 0.62125 |  0:05:37s\n",
      "epoch 73 | loss: 0.32444 | val_0_auc: 0.62922 |  0:05:41s\n",
      "epoch 74 | loss: 0.32631 | val_0_auc: 0.63483 |  0:05:45s\n",
      "epoch 75 | loss: 0.32462 | val_0_auc: 0.63122 |  0:05:50s\n",
      "epoch 76 | loss: 0.32212 | val_0_auc: 0.63088 |  0:05:55s\n",
      "\n",
      "Early stopping occurred at epoch 76 with best_epoch = 61 and best_val_0_auc = 0.6356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 02:14:01,157] Trial 30 finished with value: 0.6356004191336547 and parameters: {'n_d': 32, 'n_a': 38, 'n_steps': 3, 'gamma': 1.5774178500763443, 'lambda_sparse': 0.0009982305229730263, 'lr': 0.0009530278188333653}. Best is trial 27 with value: 0.6477814909764187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.09359 | val_0_auc: 0.51579 |  0:00:05s\n",
      "epoch 1  | loss: 0.85147 | val_0_auc: 0.52326 |  0:00:10s\n",
      "epoch 2  | loss: 0.68101 | val_0_auc: 0.51819 |  0:00:16s\n",
      "epoch 3  | loss: 0.57191 | val_0_auc: 0.50984 |  0:00:21s\n",
      "epoch 4  | loss: 0.48807 | val_0_auc: 0.50924 |  0:00:26s\n",
      "epoch 5  | loss: 0.44343 | val_0_auc: 0.50482 |  0:00:31s\n",
      "epoch 6  | loss: 0.42541 | val_0_auc: 0.50917 |  0:00:36s\n",
      "epoch 7  | loss: 0.40876 | val_0_auc: 0.50986 |  0:00:40s\n",
      "epoch 8  | loss: 0.39531 | val_0_auc: 0.5084  |  0:00:44s\n",
      "epoch 9  | loss: 0.39439 | val_0_auc: 0.50941 |  0:00:49s\n",
      "epoch 10 | loss: 0.38465 | val_0_auc: 0.51959 |  0:00:53s\n",
      "epoch 11 | loss: 0.37964 | val_0_auc: 0.52202 |  0:00:57s\n",
      "epoch 12 | loss: 0.37594 | val_0_auc: 0.52759 |  0:01:01s\n",
      "epoch 13 | loss: 0.36689 | val_0_auc: 0.52416 |  0:01:06s\n",
      "epoch 14 | loss: 0.36746 | val_0_auc: 0.52536 |  0:01:10s\n",
      "epoch 15 | loss: 0.36579 | val_0_auc: 0.52572 |  0:01:14s\n",
      "epoch 16 | loss: 0.36345 | val_0_auc: 0.53766 |  0:01:18s\n",
      "epoch 17 | loss: 0.35651 | val_0_auc: 0.54514 |  0:01:23s\n",
      "epoch 18 | loss: 0.36022 | val_0_auc: 0.5569  |  0:01:28s\n",
      "epoch 19 | loss: 0.35537 | val_0_auc: 0.55762 |  0:01:33s\n",
      "epoch 20 | loss: 0.35349 | val_0_auc: 0.57332 |  0:01:38s\n",
      "epoch 21 | loss: 0.35328 | val_0_auc: 0.57571 |  0:01:43s\n",
      "epoch 22 | loss: 0.35185 | val_0_auc: 0.57067 |  0:01:47s\n",
      "epoch 23 | loss: 0.35305 | val_0_auc: 0.56312 |  0:01:52s\n",
      "epoch 24 | loss: 0.35033 | val_0_auc: 0.56274 |  0:01:56s\n",
      "epoch 25 | loss: 0.34596 | val_0_auc: 0.56709 |  0:02:01s\n",
      "epoch 26 | loss: 0.34786 | val_0_auc: 0.5665  |  0:02:05s\n",
      "epoch 27 | loss: 0.34439 | val_0_auc: 0.56874 |  0:02:09s\n",
      "epoch 28 | loss: 0.34452 | val_0_auc: 0.56836 |  0:02:13s\n",
      "epoch 29 | loss: 0.34375 | val_0_auc: 0.57326 |  0:02:18s\n",
      "epoch 30 | loss: 0.34332 | val_0_auc: 0.57789 |  0:02:23s\n",
      "epoch 31 | loss: 0.33916 | val_0_auc: 0.57549 |  0:02:28s\n",
      "epoch 32 | loss: 0.34264 | val_0_auc: 0.57939 |  0:02:32s\n",
      "epoch 33 | loss: 0.33988 | val_0_auc: 0.58705 |  0:02:38s\n",
      "epoch 34 | loss: 0.34038 | val_0_auc: 0.59038 |  0:02:43s\n",
      "epoch 35 | loss: 0.33661 | val_0_auc: 0.59135 |  0:02:48s\n",
      "epoch 36 | loss: 0.34029 | val_0_auc: 0.59636 |  0:02:53s\n",
      "epoch 37 | loss: 0.33678 | val_0_auc: 0.59077 |  0:02:57s\n",
      "epoch 38 | loss: 0.34149 | val_0_auc: 0.59552 |  0:03:02s\n",
      "epoch 39 | loss: 0.33775 | val_0_auc: 0.58961 |  0:03:06s\n",
      "epoch 40 | loss: 0.33846 | val_0_auc: 0.59544 |  0:03:10s\n",
      "epoch 41 | loss: 0.33709 | val_0_auc: 0.60037 |  0:03:14s\n",
      "epoch 42 | loss: 0.33916 | val_0_auc: 0.59772 |  0:03:18s\n",
      "epoch 43 | loss: 0.33612 | val_0_auc: 0.60305 |  0:03:22s\n",
      "epoch 44 | loss: 0.33481 | val_0_auc: 0.6094  |  0:03:27s\n",
      "epoch 45 | loss: 0.33489 | val_0_auc: 0.61413 |  0:03:31s\n",
      "epoch 46 | loss: 0.33471 | val_0_auc: 0.61332 |  0:03:36s\n",
      "epoch 47 | loss: 0.3313  | val_0_auc: 0.61351 |  0:03:41s\n",
      "epoch 48 | loss: 0.33387 | val_0_auc: 0.61124 |  0:03:46s\n",
      "epoch 49 | loss: 0.33435 | val_0_auc: 0.60967 |  0:03:51s\n",
      "epoch 50 | loss: 0.33402 | val_0_auc: 0.60775 |  0:03:56s\n",
      "epoch 51 | loss: 0.33295 | val_0_auc: 0.60599 |  0:04:00s\n",
      "epoch 52 | loss: 0.33358 | val_0_auc: 0.6159  |  0:04:04s\n",
      "epoch 53 | loss: 0.33257 | val_0_auc: 0.61891 |  0:04:08s\n",
      "epoch 54 | loss: 0.33151 | val_0_auc: 0.61895 |  0:04:13s\n",
      "epoch 55 | loss: 0.33156 | val_0_auc: 0.62147 |  0:04:17s\n",
      "epoch 56 | loss: 0.33298 | val_0_auc: 0.61757 |  0:04:21s\n",
      "epoch 57 | loss: 0.33267 | val_0_auc: 0.6213  |  0:04:25s\n",
      "epoch 58 | loss: 0.33026 | val_0_auc: 0.61953 |  0:04:30s\n",
      "epoch 59 | loss: 0.33026 | val_0_auc: 0.62174 |  0:04:35s\n",
      "epoch 60 | loss: 0.33011 | val_0_auc: 0.62644 |  0:04:39s\n",
      "epoch 61 | loss: 0.33303 | val_0_auc: 0.62093 |  0:04:44s\n",
      "epoch 62 | loss: 0.32922 | val_0_auc: 0.62161 |  0:04:49s\n",
      "epoch 63 | loss: 0.33123 | val_0_auc: 0.62053 |  0:04:54s\n",
      "epoch 64 | loss: 0.3324  | val_0_auc: 0.62262 |  0:04:59s\n",
      "epoch 65 | loss: 0.32949 | val_0_auc: 0.62255 |  0:05:04s\n",
      "epoch 66 | loss: 0.33071 | val_0_auc: 0.62171 |  0:05:09s\n",
      "epoch 67 | loss: 0.33051 | val_0_auc: 0.62169 |  0:05:13s\n",
      "epoch 68 | loss: 0.32869 | val_0_auc: 0.61753 |  0:05:17s\n",
      "epoch 69 | loss: 0.3269  | val_0_auc: 0.61835 |  0:05:21s\n",
      "epoch 70 | loss: 0.32544 | val_0_auc: 0.623   |  0:05:26s\n",
      "epoch 71 | loss: 0.32849 | val_0_auc: 0.62143 |  0:05:30s\n",
      "epoch 72 | loss: 0.32574 | val_0_auc: 0.62273 |  0:05:35s\n",
      "epoch 73 | loss: 0.3285  | val_0_auc: 0.61822 |  0:05:39s\n",
      "epoch 74 | loss: 0.32378 | val_0_auc: 0.6198  |  0:05:45s\n",
      "epoch 75 | loss: 0.32576 | val_0_auc: 0.62388 |  0:05:50s\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 60 and best_val_0_auc = 0.62644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 02:19:54,467] Trial 31 finished with value: 0.6264383558159882 and parameters: {'n_d': 30, 'n_a': 37, 'n_steps': 3, 'gamma': 1.5334132213222986, 'lambda_sparse': 0.0008037202566755063, 'lr': 0.00098052619434122}. Best is trial 27 with value: 0.6477814909764187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73676 | val_0_auc: 0.51372 |  0:00:04s\n",
      "epoch 1  | loss: 0.49662 | val_0_auc: 0.49274 |  0:00:09s\n",
      "epoch 2  | loss: 0.43967 | val_0_auc: 0.49506 |  0:00:13s\n",
      "epoch 3  | loss: 0.42184 | val_0_auc: 0.50218 |  0:00:17s\n",
      "epoch 4  | loss: 0.40452 | val_0_auc: 0.51467 |  0:00:22s\n",
      "epoch 5  | loss: 0.3897  | val_0_auc: 0.5175  |  0:00:26s\n",
      "epoch 6  | loss: 0.37802 | val_0_auc: 0.5205  |  0:00:31s\n",
      "epoch 7  | loss: 0.36751 | val_0_auc: 0.52587 |  0:00:36s\n",
      "epoch 8  | loss: 0.36584 | val_0_auc: 0.52671 |  0:00:41s\n",
      "epoch 9  | loss: 0.35663 | val_0_auc: 0.53159 |  0:00:46s\n",
      "epoch 10 | loss: 0.35709 | val_0_auc: 0.53583 |  0:00:50s\n",
      "epoch 11 | loss: 0.35123 | val_0_auc: 0.54472 |  0:00:55s\n",
      "epoch 12 | loss: 0.34649 | val_0_auc: 0.54939 |  0:00:59s\n",
      "epoch 13 | loss: 0.34494 | val_0_auc: 0.55443 |  0:01:04s\n",
      "epoch 14 | loss: 0.34394 | val_0_auc: 0.56145 |  0:01:09s\n",
      "epoch 15 | loss: 0.34085 | val_0_auc: 0.56266 |  0:01:14s\n",
      "epoch 16 | loss: 0.34006 | val_0_auc: 0.57297 |  0:01:19s\n",
      "epoch 17 | loss: 0.34092 | val_0_auc: 0.58839 |  0:01:24s\n",
      "epoch 18 | loss: 0.34067 | val_0_auc: 0.58869 |  0:01:29s\n",
      "epoch 19 | loss: 0.33993 | val_0_auc: 0.59797 |  0:01:35s\n",
      "epoch 20 | loss: 0.33807 | val_0_auc: 0.60009 |  0:01:40s\n",
      "epoch 21 | loss: 0.33592 | val_0_auc: 0.60627 |  0:01:44s\n",
      "epoch 22 | loss: 0.33463 | val_0_auc: 0.60195 |  0:01:49s\n",
      "epoch 23 | loss: 0.33416 | val_0_auc: 0.59936 |  0:01:53s\n",
      "epoch 24 | loss: 0.33212 | val_0_auc: 0.59885 |  0:01:57s\n",
      "epoch 25 | loss: 0.33004 | val_0_auc: 0.5978  |  0:02:01s\n",
      "epoch 26 | loss: 0.33196 | val_0_auc: 0.59958 |  0:02:05s\n",
      "epoch 27 | loss: 0.33299 | val_0_auc: 0.6006  |  0:02:09s\n",
      "epoch 28 | loss: 0.32962 | val_0_auc: 0.60067 |  0:02:14s\n",
      "epoch 29 | loss: 0.33101 | val_0_auc: 0.60856 |  0:02:19s\n",
      "epoch 30 | loss: 0.33223 | val_0_auc: 0.60194 |  0:02:24s\n",
      "epoch 31 | loss: 0.32948 | val_0_auc: 0.60052 |  0:02:28s\n",
      "epoch 32 | loss: 0.32825 | val_0_auc: 0.60325 |  0:02:34s\n",
      "epoch 33 | loss: 0.3288  | val_0_auc: 0.60745 |  0:02:39s\n",
      "epoch 34 | loss: 0.32619 | val_0_auc: 0.59852 |  0:02:44s\n",
      "epoch 35 | loss: 0.32668 | val_0_auc: 0.60118 |  0:02:49s\n",
      "epoch 36 | loss: 0.32613 | val_0_auc: 0.59639 |  0:02:53s\n",
      "epoch 37 | loss: 0.3247  | val_0_auc: 0.59739 |  0:02:58s\n",
      "epoch 38 | loss: 0.32566 | val_0_auc: 0.60235 |  0:03:02s\n",
      "epoch 39 | loss: 0.324   | val_0_auc: 0.60096 |  0:03:06s\n",
      "epoch 40 | loss: 0.32227 | val_0_auc: 0.60463 |  0:03:10s\n",
      "epoch 41 | loss: 0.32245 | val_0_auc: 0.60175 |  0:03:14s\n",
      "epoch 42 | loss: 0.32189 | val_0_auc: 0.60089 |  0:03:18s\n",
      "epoch 43 | loss: 0.32248 | val_0_auc: 0.60779 |  0:03:23s\n",
      "epoch 44 | loss: 0.32189 | val_0_auc: 0.6134  |  0:03:27s\n",
      "epoch 45 | loss: 0.32003 | val_0_auc: 0.61801 |  0:03:32s\n",
      "epoch 46 | loss: 0.31978 | val_0_auc: 0.61714 |  0:03:37s\n",
      "epoch 47 | loss: 0.31817 | val_0_auc: 0.61598 |  0:03:42s\n",
      "epoch 48 | loss: 0.31622 | val_0_auc: 0.61875 |  0:03:48s\n",
      "epoch 49 | loss: 0.31716 | val_0_auc: 0.6185  |  0:03:53s\n",
      "epoch 50 | loss: 0.3167  | val_0_auc: 0.6159  |  0:03:58s\n",
      "epoch 51 | loss: 0.31561 | val_0_auc: 0.62487 |  0:04:03s\n",
      "epoch 52 | loss: 0.31609 | val_0_auc: 0.6308  |  0:04:07s\n",
      "epoch 53 | loss: 0.31357 | val_0_auc: 0.62825 |  0:04:13s\n",
      "epoch 54 | loss: 0.31633 | val_0_auc: 0.63098 |  0:04:18s\n",
      "epoch 55 | loss: 0.31735 | val_0_auc: 0.63643 |  0:04:23s\n",
      "epoch 56 | loss: 0.31509 | val_0_auc: 0.63244 |  0:04:27s\n",
      "epoch 57 | loss: 0.31504 | val_0_auc: 0.63221 |  0:04:31s\n",
      "epoch 58 | loss: 0.31663 | val_0_auc: 0.63428 |  0:04:36s\n",
      "epoch 59 | loss: 0.31193 | val_0_auc: 0.62695 |  0:04:40s\n",
      "epoch 60 | loss: 0.31228 | val_0_auc: 0.62375 |  0:04:44s\n",
      "epoch 61 | loss: 0.31026 | val_0_auc: 0.63058 |  0:04:49s\n",
      "epoch 62 | loss: 0.3106  | val_0_auc: 0.63453 |  0:04:54s\n",
      "epoch 63 | loss: 0.31105 | val_0_auc: 0.63091 |  0:04:59s\n",
      "epoch 64 | loss: 0.30919 | val_0_auc: 0.62831 |  0:05:05s\n",
      "epoch 65 | loss: 0.31061 | val_0_auc: 0.62914 |  0:05:10s\n",
      "epoch 66 | loss: 0.30756 | val_0_auc: 0.63457 |  0:05:14s\n",
      "epoch 67 | loss: 0.30905 | val_0_auc: 0.63434 |  0:05:19s\n",
      "epoch 68 | loss: 0.30866 | val_0_auc: 0.62678 |  0:05:23s\n",
      "epoch 69 | loss: 0.31006 | val_0_auc: 0.62564 |  0:05:27s\n",
      "epoch 70 | loss: 0.30831 | val_0_auc: 0.62538 |  0:05:32s\n",
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 55 and best_val_0_auc = 0.63643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 02:25:28,618] Trial 32 finished with value: 0.6364257156808887 and parameters: {'n_d': 31, 'n_a': 44, 'n_steps': 3, 'gamma': 1.6152181008092008, 'lambda_sparse': 0.00027186565322942155, 'lr': 0.002118615939937679}. Best is trial 27 with value: 0.6477814909764187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6076  | val_0_auc: 0.46661 |  0:00:03s\n",
      "epoch 1  | loss: 0.47178 | val_0_auc: 0.4521  |  0:00:07s\n",
      "epoch 2  | loss: 0.44031 | val_0_auc: 0.46485 |  0:00:11s\n",
      "epoch 3  | loss: 0.42294 | val_0_auc: 0.48643 |  0:00:15s\n",
      "epoch 4  | loss: 0.40393 | val_0_auc: 0.50349 |  0:00:19s\n",
      "epoch 5  | loss: 0.38922 | val_0_auc: 0.5166  |  0:00:23s\n",
      "epoch 6  | loss: 0.38407 | val_0_auc: 0.52453 |  0:00:27s\n",
      "epoch 7  | loss: 0.37429 | val_0_auc: 0.53208 |  0:00:31s\n",
      "epoch 8  | loss: 0.364   | val_0_auc: 0.54314 |  0:00:36s\n",
      "epoch 9  | loss: 0.36255 | val_0_auc: 0.5471  |  0:00:40s\n",
      "epoch 10 | loss: 0.35572 | val_0_auc: 0.55074 |  0:00:45s\n",
      "epoch 11 | loss: 0.35432 | val_0_auc: 0.55094 |  0:00:50s\n",
      "epoch 12 | loss: 0.35188 | val_0_auc: 0.55544 |  0:00:55s\n",
      "epoch 13 | loss: 0.34826 | val_0_auc: 0.56059 |  0:00:59s\n",
      "epoch 14 | loss: 0.35012 | val_0_auc: 0.56446 |  0:01:03s\n",
      "epoch 15 | loss: 0.34638 | val_0_auc: 0.57008 |  0:01:07s\n",
      "epoch 16 | loss: 0.34606 | val_0_auc: 0.57148 |  0:01:11s\n",
      "epoch 17 | loss: 0.34581 | val_0_auc: 0.5759  |  0:01:14s\n",
      "epoch 18 | loss: 0.34386 | val_0_auc: 0.57524 |  0:01:18s\n",
      "epoch 19 | loss: 0.34054 | val_0_auc: 0.58387 |  0:01:22s\n",
      "epoch 20 | loss: 0.33956 | val_0_auc: 0.59007 |  0:01:25s\n",
      "epoch 21 | loss: 0.33901 | val_0_auc: 0.58299 |  0:01:29s\n",
      "epoch 22 | loss: 0.33824 | val_0_auc: 0.58572 |  0:01:33s\n",
      "epoch 23 | loss: 0.3363  | val_0_auc: 0.59087 |  0:01:38s\n",
      "epoch 24 | loss: 0.33709 | val_0_auc: 0.59122 |  0:01:42s\n",
      "epoch 25 | loss: 0.33522 | val_0_auc: 0.58773 |  0:01:47s\n",
      "epoch 26 | loss: 0.33399 | val_0_auc: 0.58957 |  0:01:51s\n",
      "epoch 27 | loss: 0.33438 | val_0_auc: 0.58936 |  0:01:56s\n",
      "epoch 28 | loss: 0.33376 | val_0_auc: 0.59511 |  0:02:00s\n",
      "epoch 29 | loss: 0.33162 | val_0_auc: 0.60065 |  0:02:04s\n",
      "epoch 30 | loss: 0.32902 | val_0_auc: 0.59573 |  0:02:08s\n",
      "epoch 31 | loss: 0.32818 | val_0_auc: 0.59041 |  0:02:12s\n",
      "epoch 32 | loss: 0.32721 | val_0_auc: 0.59471 |  0:02:15s\n",
      "epoch 33 | loss: 0.33068 | val_0_auc: 0.59771 |  0:02:19s\n",
      "epoch 34 | loss: 0.32718 | val_0_auc: 0.60254 |  0:02:23s\n",
      "epoch 35 | loss: 0.32583 | val_0_auc: 0.60416 |  0:02:27s\n",
      "epoch 36 | loss: 0.32825 | val_0_auc: 0.61008 |  0:02:31s\n",
      "epoch 37 | loss: 0.32575 | val_0_auc: 0.61015 |  0:02:35s\n",
      "epoch 38 | loss: 0.32737 | val_0_auc: 0.60569 |  0:02:40s\n",
      "epoch 39 | loss: 0.32428 | val_0_auc: 0.6145  |  0:02:44s\n",
      "epoch 40 | loss: 0.32303 | val_0_auc: 0.61096 |  0:02:48s\n",
      "epoch 41 | loss: 0.32138 | val_0_auc: 0.6202  |  0:02:53s\n",
      "epoch 42 | loss: 0.32294 | val_0_auc: 0.61429 |  0:02:58s\n",
      "epoch 43 | loss: 0.32292 | val_0_auc: 0.61206 |  0:03:03s\n",
      "epoch 44 | loss: 0.3242  | val_0_auc: 0.61569 |  0:03:07s\n",
      "epoch 45 | loss: 0.32021 | val_0_auc: 0.61479 |  0:03:12s\n",
      "epoch 46 | loss: 0.32179 | val_0_auc: 0.62332 |  0:03:16s\n",
      "epoch 47 | loss: 0.3235  | val_0_auc: 0.62652 |  0:03:20s\n",
      "epoch 48 | loss: 0.32194 | val_0_auc: 0.6307  |  0:03:24s\n",
      "epoch 49 | loss: 0.32109 | val_0_auc: 0.62674 |  0:03:28s\n",
      "epoch 50 | loss: 0.32197 | val_0_auc: 0.62696 |  0:03:32s\n",
      "epoch 51 | loss: 0.31896 | val_0_auc: 0.62718 |  0:03:35s\n",
      "epoch 52 | loss: 0.32019 | val_0_auc: 0.62652 |  0:03:39s\n",
      "epoch 53 | loss: 0.32105 | val_0_auc: 0.62529 |  0:03:43s\n",
      "epoch 54 | loss: 0.31863 | val_0_auc: 0.62327 |  0:03:47s\n",
      "epoch 55 | loss: 0.31853 | val_0_auc: 0.61865 |  0:03:51s\n",
      "epoch 56 | loss: 0.31936 | val_0_auc: 0.62671 |  0:03:55s\n",
      "epoch 57 | loss: 0.31769 | val_0_auc: 0.6342  |  0:03:59s\n",
      "epoch 58 | loss: 0.31697 | val_0_auc: 0.63565 |  0:04:04s\n",
      "epoch 59 | loss: 0.31749 | val_0_auc: 0.62836 |  0:04:09s\n",
      "epoch 60 | loss: 0.31824 | val_0_auc: 0.62289 |  0:04:14s\n",
      "epoch 61 | loss: 0.31671 | val_0_auc: 0.62561 |  0:04:19s\n",
      "epoch 62 | loss: 0.31655 | val_0_auc: 0.63128 |  0:04:23s\n",
      "epoch 63 | loss: 0.31269 | val_0_auc: 0.6327  |  0:04:27s\n",
      "epoch 64 | loss: 0.31569 | val_0_auc: 0.63422 |  0:04:31s\n",
      "epoch 65 | loss: 0.31415 | val_0_auc: 0.63503 |  0:04:35s\n",
      "epoch 66 | loss: 0.31576 | val_0_auc: 0.63266 |  0:04:38s\n",
      "epoch 67 | loss: 0.31378 | val_0_auc: 0.63024 |  0:04:42s\n",
      "epoch 68 | loss: 0.31358 | val_0_auc: 0.62715 |  0:04:46s\n",
      "epoch 69 | loss: 0.31082 | val_0_auc: 0.63343 |  0:04:50s\n",
      "epoch 70 | loss: 0.31238 | val_0_auc: 0.63253 |  0:04:54s\n",
      "epoch 71 | loss: 0.3132  | val_0_auc: 0.6351  |  0:04:58s\n",
      "epoch 72 | loss: 0.31392 | val_0_auc: 0.63703 |  0:05:03s\n",
      "epoch 73 | loss: 0.31158 | val_0_auc: 0.63416 |  0:05:08s\n",
      "epoch 74 | loss: 0.31247 | val_0_auc: 0.63385 |  0:05:12s\n",
      "epoch 75 | loss: 0.31068 | val_0_auc: 0.6312  |  0:05:17s\n",
      "epoch 76 | loss: 0.31055 | val_0_auc: 0.63033 |  0:05:21s\n",
      "epoch 77 | loss: 0.31003 | val_0_auc: 0.62914 |  0:05:26s\n",
      "epoch 78 | loss: 0.31159 | val_0_auc: 0.6324  |  0:05:30s\n",
      "epoch 79 | loss: 0.30763 | val_0_auc: 0.62949 |  0:05:35s\n",
      "epoch 80 | loss: 0.30932 | val_0_auc: 0.63279 |  0:05:38s\n",
      "epoch 81 | loss: 0.30666 | val_0_auc: 0.63813 |  0:05:42s\n",
      "epoch 82 | loss: 0.30861 | val_0_auc: 0.63465 |  0:05:46s\n",
      "epoch 83 | loss: 0.30834 | val_0_auc: 0.63026 |  0:05:50s\n",
      "epoch 84 | loss: 0.31058 | val_0_auc: 0.63193 |  0:05:54s\n",
      "epoch 85 | loss: 0.30618 | val_0_auc: 0.6339  |  0:05:58s\n",
      "epoch 86 | loss: 0.30636 | val_0_auc: 0.63736 |  0:06:02s\n",
      "epoch 87 | loss: 0.30658 | val_0_auc: 0.63304 |  0:06:07s\n",
      "epoch 88 | loss: 0.3064  | val_0_auc: 0.63613 |  0:06:11s\n",
      "epoch 89 | loss: 0.30734 | val_0_auc: 0.63832 |  0:06:16s\n",
      "epoch 90 | loss: 0.30536 | val_0_auc: 0.63684 |  0:06:19s\n",
      "epoch 91 | loss: 0.30651 | val_0_auc: 0.63298 |  0:06:23s\n",
      "epoch 92 | loss: 0.30572 | val_0_auc: 0.63787 |  0:06:27s\n",
      "epoch 93 | loss: 0.30595 | val_0_auc: 0.63939 |  0:06:31s\n",
      "epoch 94 | loss: 0.30473 | val_0_auc: 0.64038 |  0:06:36s\n",
      "epoch 95 | loss: 0.30366 | val_0_auc: 0.64172 |  0:06:40s\n",
      "epoch 96 | loss: 0.30444 | val_0_auc: 0.64342 |  0:06:44s\n",
      "epoch 97 | loss: 0.30451 | val_0_auc: 0.64372 |  0:06:48s\n",
      "epoch 98 | loss: 0.30484 | val_0_auc: 0.64254 |  0:06:53s\n",
      "epoch 99 | loss: 0.30239 | val_0_auc: 0.64154 |  0:06:58s\n",
      "epoch 100| loss: 0.30314 | val_0_auc: 0.63981 |  0:07:02s\n",
      "epoch 101| loss: 0.30181 | val_0_auc: 0.63711 |  0:07:07s\n",
      "epoch 102| loss: 0.30201 | val_0_auc: 0.63883 |  0:07:12s\n",
      "epoch 103| loss: 0.29977 | val_0_auc: 0.63951 |  0:07:17s\n",
      "epoch 104| loss: 0.30051 | val_0_auc: 0.63761 |  0:07:21s\n",
      "epoch 105| loss: 0.30213 | val_0_auc: 0.64434 |  0:07:26s\n",
      "epoch 106| loss: 0.29939 | val_0_auc: 0.64126 |  0:07:29s\n",
      "epoch 107| loss: 0.3006  | val_0_auc: 0.64201 |  0:07:33s\n",
      "epoch 108| loss: 0.30025 | val_0_auc: 0.6444  |  0:07:37s\n",
      "epoch 109| loss: 0.29962 | val_0_auc: 0.64757 |  0:07:40s\n",
      "epoch 110| loss: 0.29712 | val_0_auc: 0.64859 |  0:07:44s\n",
      "epoch 111| loss: 0.29929 | val_0_auc: 0.64408 |  0:07:48s\n",
      "epoch 112| loss: 0.29847 | val_0_auc: 0.63979 |  0:07:52s\n",
      "epoch 113| loss: 0.29599 | val_0_auc: 0.64206 |  0:07:56s\n",
      "epoch 114| loss: 0.2942  | val_0_auc: 0.64011 |  0:08:00s\n",
      "epoch 115| loss: 0.29828 | val_0_auc: 0.64176 |  0:08:05s\n",
      "epoch 116| loss: 0.29393 | val_0_auc: 0.64122 |  0:08:09s\n",
      "epoch 117| loss: 0.29593 | val_0_auc: 0.63796 |  0:08:14s\n",
      "epoch 118| loss: 0.29515 | val_0_auc: 0.63572 |  0:08:19s\n",
      "epoch 119| loss: 0.29433 | val_0_auc: 0.63816 |  0:08:23s\n",
      "epoch 120| loss: 0.29634 | val_0_auc: 0.63795 |  0:08:28s\n",
      "epoch 121| loss: 0.29543 | val_0_auc: 0.63938 |  0:08:32s\n",
      "epoch 122| loss: 0.2964  | val_0_auc: 0.63899 |  0:08:36s\n",
      "epoch 123| loss: 0.29504 | val_0_auc: 0.63358 |  0:08:40s\n",
      "epoch 124| loss: 0.29357 | val_0_auc: 0.63518 |  0:08:44s\n",
      "epoch 125| loss: 0.29374 | val_0_auc: 0.63424 |  0:08:48s\n",
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 110 and best_val_0_auc = 0.64859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 02:34:18,723] Trial 33 finished with value: 0.648586186901056 and parameters: {'n_d': 31, 'n_a': 27, 'n_steps': 3, 'gamma': 1.2894075904618, 'lambda_sparse': 0.0003330209998517352, 'lr': 0.0019789527951969126}. Best is trial 33 with value: 0.648586186901056.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.4992  | val_0_auc: 0.4592  |  0:00:04s\n",
      "epoch 1  | loss: 0.44342 | val_0_auc: 0.48262 |  0:00:08s\n",
      "epoch 2  | loss: 0.42427 | val_0_auc: 0.4949  |  0:00:12s\n",
      "epoch 3  | loss: 0.40267 | val_0_auc: 0.5108  |  0:00:17s\n",
      "epoch 4  | loss: 0.38229 | val_0_auc: 0.50581 |  0:00:22s\n",
      "epoch 5  | loss: 0.37369 | val_0_auc: 0.509   |  0:00:27s\n",
      "epoch 6  | loss: 0.36445 | val_0_auc: 0.52224 |  0:00:32s\n",
      "epoch 7  | loss: 0.35916 | val_0_auc: 0.52357 |  0:00:37s\n",
      "epoch 8  | loss: 0.35378 | val_0_auc: 0.53445 |  0:00:42s\n",
      "epoch 9  | loss: 0.34742 | val_0_auc: 0.54525 |  0:00:47s\n",
      "epoch 10 | loss: 0.34559 | val_0_auc: 0.53931 |  0:00:52s\n",
      "epoch 11 | loss: 0.34461 | val_0_auc: 0.54749 |  0:00:57s\n",
      "epoch 12 | loss: 0.34409 | val_0_auc: 0.56371 |  0:01:01s\n",
      "epoch 13 | loss: 0.34017 | val_0_auc: 0.57002 |  0:01:06s\n",
      "epoch 14 | loss: 0.33847 | val_0_auc: 0.58215 |  0:01:10s\n",
      "epoch 15 | loss: 0.33183 | val_0_auc: 0.59388 |  0:01:14s\n",
      "epoch 16 | loss: 0.33313 | val_0_auc: 0.60596 |  0:01:18s\n",
      "epoch 17 | loss: 0.3321  | val_0_auc: 0.60954 |  0:01:22s\n",
      "epoch 18 | loss: 0.33539 | val_0_auc: 0.60215 |  0:01:26s\n",
      "epoch 19 | loss: 0.33258 | val_0_auc: 0.59933 |  0:01:30s\n",
      "epoch 20 | loss: 0.33603 | val_0_auc: 0.59471 |  0:01:35s\n",
      "epoch 21 | loss: 0.32979 | val_0_auc: 0.60178 |  0:01:39s\n",
      "epoch 22 | loss: 0.33056 | val_0_auc: 0.60833 |  0:01:44s\n",
      "epoch 23 | loss: 0.33096 | val_0_auc: 0.61716 |  0:01:48s\n",
      "epoch 24 | loss: 0.32883 | val_0_auc: 0.61721 |  0:01:53s\n",
      "epoch 25 | loss: 0.32977 | val_0_auc: 0.62299 |  0:01:58s\n",
      "epoch 26 | loss: 0.32848 | val_0_auc: 0.62663 |  0:02:04s\n",
      "epoch 27 | loss: 0.33069 | val_0_auc: 0.62899 |  0:02:08s\n",
      "epoch 28 | loss: 0.32946 | val_0_auc: 0.63629 |  0:02:13s\n",
      "epoch 29 | loss: 0.32601 | val_0_auc: 0.63003 |  0:02:17s\n",
      "epoch 30 | loss: 0.32574 | val_0_auc: 0.63429 |  0:02:21s\n",
      "epoch 31 | loss: 0.32591 | val_0_auc: 0.63313 |  0:02:25s\n",
      "epoch 32 | loss: 0.32354 | val_0_auc: 0.63313 |  0:02:29s\n",
      "epoch 33 | loss: 0.32551 | val_0_auc: 0.62945 |  0:02:34s\n",
      "epoch 34 | loss: 0.3231  | val_0_auc: 0.6253  |  0:02:38s\n",
      "epoch 35 | loss: 0.32087 | val_0_auc: 0.63293 |  0:02:42s\n",
      "epoch 36 | loss: 0.32057 | val_0_auc: 0.63075 |  0:02:47s\n",
      "epoch 37 | loss: 0.321   | val_0_auc: 0.63246 |  0:02:51s\n",
      "epoch 38 | loss: 0.32017 | val_0_auc: 0.63187 |  0:02:56s\n",
      "epoch 39 | loss: 0.31954 | val_0_auc: 0.63326 |  0:03:01s\n",
      "epoch 40 | loss: 0.32013 | val_0_auc: 0.63616 |  0:03:06s\n",
      "epoch 41 | loss: 0.32162 | val_0_auc: 0.63507 |  0:03:11s\n",
      "epoch 42 | loss: 0.31929 | val_0_auc: 0.64038 |  0:03:16s\n",
      "epoch 43 | loss: 0.31927 | val_0_auc: 0.63641 |  0:03:21s\n",
      "epoch 44 | loss: 0.31891 | val_0_auc: 0.63596 |  0:03:25s\n",
      "epoch 45 | loss: 0.31657 | val_0_auc: 0.6392  |  0:03:29s\n",
      "epoch 46 | loss: 0.31574 | val_0_auc: 0.64315 |  0:03:34s\n",
      "epoch 47 | loss: 0.31458 | val_0_auc: 0.64494 |  0:03:38s\n",
      "epoch 48 | loss: 0.31494 | val_0_auc: 0.6502  |  0:03:42s\n",
      "epoch 49 | loss: 0.31612 | val_0_auc: 0.64955 |  0:03:46s\n",
      "epoch 50 | loss: 0.31786 | val_0_auc: 0.65226 |  0:03:51s\n",
      "epoch 51 | loss: 0.31403 | val_0_auc: 0.6467  |  0:03:55s\n",
      "epoch 52 | loss: 0.31415 | val_0_auc: 0.64527 |  0:04:00s\n",
      "epoch 53 | loss: 0.3143  | val_0_auc: 0.64023 |  0:04:05s\n",
      "epoch 54 | loss: 0.31313 | val_0_auc: 0.64026 |  0:04:09s\n",
      "epoch 55 | loss: 0.31389 | val_0_auc: 0.64453 |  0:04:13s\n",
      "epoch 56 | loss: 0.31302 | val_0_auc: 0.64723 |  0:04:17s\n",
      "epoch 57 | loss: 0.31367 | val_0_auc: 0.64782 |  0:04:21s\n",
      "epoch 58 | loss: 0.31078 | val_0_auc: 0.65002 |  0:04:26s\n",
      "epoch 59 | loss: 0.30919 | val_0_auc: 0.65013 |  0:04:31s\n",
      "epoch 60 | loss: 0.31169 | val_0_auc: 0.65153 |  0:04:35s\n",
      "epoch 61 | loss: 0.31147 | val_0_auc: 0.65988 |  0:04:40s\n",
      "epoch 62 | loss: 0.31181 | val_0_auc: 0.65726 |  0:04:44s\n",
      "epoch 63 | loss: 0.30851 | val_0_auc: 0.6509  |  0:04:48s\n",
      "epoch 64 | loss: 0.30754 | val_0_auc: 0.649   |  0:04:53s\n",
      "epoch 65 | loss: 0.3076  | val_0_auc: 0.65085 |  0:04:57s\n",
      "epoch 66 | loss: 0.30655 | val_0_auc: 0.6497  |  0:05:02s\n",
      "epoch 67 | loss: 0.3069  | val_0_auc: 0.65093 |  0:05:07s\n",
      "epoch 68 | loss: 0.30649 | val_0_auc: 0.65094 |  0:05:11s\n",
      "epoch 69 | loss: 0.30676 | val_0_auc: 0.64707 |  0:05:15s\n",
      "epoch 70 | loss: 0.30506 | val_0_auc: 0.64884 |  0:05:19s\n",
      "epoch 71 | loss: 0.30628 | val_0_auc: 0.64661 |  0:05:24s\n",
      "epoch 72 | loss: 0.30527 | val_0_auc: 0.64708 |  0:05:29s\n",
      "epoch 73 | loss: 0.30569 | val_0_auc: 0.64208 |  0:05:34s\n",
      "epoch 74 | loss: 0.30559 | val_0_auc: 0.64002 |  0:05:39s\n",
      "epoch 75 | loss: 0.30595 | val_0_auc: 0.64026 |  0:05:44s\n",
      "epoch 76 | loss: 0.30458 | val_0_auc: 0.6431  |  0:05:49s\n",
      "\n",
      "Early stopping occurred at epoch 76 with best_epoch = 61 and best_val_0_auc = 0.65988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 02:40:11,007] Trial 34 finished with value: 0.6598847382449285 and parameters: {'n_d': 39, 'n_a': 27, 'n_steps': 3, 'gamma': 1.0103641343886474, 'lambda_sparse': 0.0003184394635095508, 'lr': 0.00212323970394761}. Best is trial 34 with value: 0.6598847382449285.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.01959 | val_0_auc: 0.46901 |  0:00:06s\n",
      "epoch 1  | loss: 0.57129 | val_0_auc: 0.48157 |  0:00:11s\n",
      "epoch 2  | loss: 0.50409 | val_0_auc: 0.48225 |  0:00:17s\n",
      "epoch 3  | loss: 0.48418 | val_0_auc: 0.48011 |  0:00:22s\n",
      "epoch 4  | loss: 0.45015 | val_0_auc: 0.49665 |  0:00:27s\n",
      "epoch 5  | loss: 0.43177 | val_0_auc: 0.50603 |  0:00:32s\n",
      "epoch 6  | loss: 0.41702 | val_0_auc: 0.51357 |  0:00:38s\n",
      "epoch 7  | loss: 0.41556 | val_0_auc: 0.52803 |  0:00:44s\n",
      "epoch 8  | loss: 0.39625 | val_0_auc: 0.53237 |  0:00:51s\n",
      "epoch 9  | loss: 0.39534 | val_0_auc: 0.54587 |  0:00:57s\n",
      "epoch 10 | loss: 0.38857 | val_0_auc: 0.53832 |  0:01:03s\n",
      "epoch 11 | loss: 0.3873  | val_0_auc: 0.54117 |  0:01:10s\n",
      "epoch 12 | loss: 0.37988 | val_0_auc: 0.55121 |  0:01:16s\n",
      "epoch 13 | loss: 0.38113 | val_0_auc: 0.55021 |  0:01:21s\n",
      "epoch 14 | loss: 0.37714 | val_0_auc: 0.54913 |  0:01:26s\n",
      "epoch 15 | loss: 0.37434 | val_0_auc: 0.54797 |  0:01:31s\n",
      "epoch 16 | loss: 0.3755  | val_0_auc: 0.54043 |  0:01:36s\n",
      "epoch 17 | loss: 0.37057 | val_0_auc: 0.54893 |  0:01:41s\n",
      "epoch 18 | loss: 0.37146 | val_0_auc: 0.56232 |  0:01:47s\n",
      "epoch 19 | loss: 0.36533 | val_0_auc: 0.56219 |  0:01:52s\n",
      "epoch 20 | loss: 0.36209 | val_0_auc: 0.55163 |  0:01:58s\n",
      "epoch 21 | loss: 0.36229 | val_0_auc: 0.55638 |  0:02:04s\n",
      "epoch 22 | loss: 0.36165 | val_0_auc: 0.56914 |  0:02:10s\n",
      "epoch 23 | loss: 0.3607  | val_0_auc: 0.56219 |  0:02:16s\n",
      "epoch 24 | loss: 0.35731 | val_0_auc: 0.56731 |  0:02:21s\n",
      "epoch 25 | loss: 0.35453 | val_0_auc: 0.58479 |  0:02:27s\n",
      "epoch 26 | loss: 0.35388 | val_0_auc: 0.58252 |  0:02:32s\n",
      "epoch 27 | loss: 0.35037 | val_0_auc: 0.57121 |  0:02:37s\n",
      "epoch 28 | loss: 0.35273 | val_0_auc: 0.56732 |  0:02:42s\n",
      "epoch 29 | loss: 0.35056 | val_0_auc: 0.58142 |  0:02:47s\n",
      "epoch 30 | loss: 0.34885 | val_0_auc: 0.58061 |  0:02:53s\n",
      "epoch 31 | loss: 0.34597 | val_0_auc: 0.58506 |  0:02:58s\n",
      "epoch 32 | loss: 0.35015 | val_0_auc: 0.59275 |  0:03:04s\n",
      "epoch 33 | loss: 0.34769 | val_0_auc: 0.59599 |  0:03:10s\n",
      "epoch 34 | loss: 0.34455 | val_0_auc: 0.60335 |  0:03:16s\n",
      "epoch 35 | loss: 0.34516 | val_0_auc: 0.59734 |  0:03:22s\n",
      "epoch 36 | loss: 0.34236 | val_0_auc: 0.60163 |  0:03:28s\n",
      "epoch 37 | loss: 0.34023 | val_0_auc: 0.6042  |  0:03:34s\n",
      "epoch 38 | loss: 0.34194 | val_0_auc: 0.60945 |  0:03:39s\n",
      "epoch 39 | loss: 0.34034 | val_0_auc: 0.60179 |  0:03:45s\n",
      "epoch 40 | loss: 0.3372  | val_0_auc: 0.59845 |  0:03:50s\n",
      "epoch 41 | loss: 0.33686 | val_0_auc: 0.60162 |  0:03:56s\n",
      "epoch 42 | loss: 0.34006 | val_0_auc: 0.61605 |  0:04:02s\n",
      "epoch 43 | loss: 0.33515 | val_0_auc: 0.62    |  0:04:07s\n",
      "epoch 44 | loss: 0.33693 | val_0_auc: 0.61929 |  0:04:13s\n",
      "epoch 45 | loss: 0.33298 | val_0_auc: 0.62202 |  0:04:20s\n",
      "epoch 46 | loss: 0.33399 | val_0_auc: 0.63075 |  0:04:26s\n",
      "epoch 47 | loss: 0.33596 | val_0_auc: 0.62853 |  0:04:32s\n",
      "epoch 48 | loss: 0.33452 | val_0_auc: 0.62583 |  0:04:39s\n",
      "epoch 49 | loss: 0.33049 | val_0_auc: 0.6279  |  0:04:45s\n",
      "epoch 50 | loss: 0.33026 | val_0_auc: 0.63151 |  0:04:52s\n",
      "epoch 51 | loss: 0.33188 | val_0_auc: 0.63931 |  0:04:58s\n",
      "epoch 52 | loss: 0.32967 | val_0_auc: 0.63607 |  0:05:05s\n",
      "epoch 53 | loss: 0.32981 | val_0_auc: 0.6341  |  0:05:10s\n",
      "epoch 54 | loss: 0.32928 | val_0_auc: 0.6238  |  0:05:16s\n",
      "epoch 55 | loss: 0.32872 | val_0_auc: 0.63151 |  0:05:21s\n",
      "epoch 56 | loss: 0.32712 | val_0_auc: 0.62995 |  0:05:26s\n",
      "epoch 57 | loss: 0.32789 | val_0_auc: 0.63753 |  0:05:31s\n",
      "epoch 58 | loss: 0.32687 | val_0_auc: 0.63821 |  0:05:36s\n",
      "epoch 59 | loss: 0.32782 | val_0_auc: 0.64762 |  0:05:42s\n",
      "epoch 60 | loss: 0.32646 | val_0_auc: 0.64427 |  0:05:47s\n",
      "epoch 61 | loss: 0.32488 | val_0_auc: 0.64307 |  0:05:52s\n",
      "epoch 62 | loss: 0.32554 | val_0_auc: 0.64792 |  0:05:58s\n",
      "epoch 63 | loss: 0.32611 | val_0_auc: 0.64762 |  0:06:03s\n",
      "epoch 64 | loss: 0.32568 | val_0_auc: 0.64688 |  0:06:09s\n",
      "epoch 65 | loss: 0.3267  | val_0_auc: 0.63434 |  0:06:15s\n",
      "epoch 66 | loss: 0.32879 | val_0_auc: 0.64048 |  0:06:21s\n",
      "epoch 67 | loss: 0.32498 | val_0_auc: 0.6411  |  0:06:27s\n",
      "epoch 68 | loss: 0.32613 | val_0_auc: 0.63715 |  0:06:33s\n",
      "epoch 69 | loss: 0.32437 | val_0_auc: 0.63957 |  0:06:38s\n",
      "epoch 70 | loss: 0.32482 | val_0_auc: 0.64672 |  0:06:43s\n",
      "epoch 71 | loss: 0.32379 | val_0_auc: 0.65175 |  0:06:48s\n",
      "epoch 72 | loss: 0.32487 | val_0_auc: 0.64513 |  0:06:54s\n",
      "epoch 73 | loss: 0.32214 | val_0_auc: 0.64902 |  0:07:00s\n",
      "epoch 74 | loss: 0.32216 | val_0_auc: 0.64905 |  0:07:06s\n",
      "epoch 75 | loss: 0.32361 | val_0_auc: 0.65177 |  0:07:13s\n",
      "epoch 76 | loss: 0.32153 | val_0_auc: 0.64447 |  0:07:18s\n",
      "epoch 77 | loss: 0.32542 | val_0_auc: 0.64526 |  0:07:24s\n",
      "epoch 78 | loss: 0.32223 | val_0_auc: 0.64029 |  0:07:29s\n",
      "epoch 79 | loss: 0.32243 | val_0_auc: 0.64548 |  0:07:34s\n",
      "epoch 80 | loss: 0.32207 | val_0_auc: 0.64878 |  0:07:39s\n",
      "epoch 81 | loss: 0.32224 | val_0_auc: 0.6479  |  0:07:44s\n",
      "epoch 82 | loss: 0.32395 | val_0_auc: 0.64431 |  0:07:49s\n",
      "epoch 83 | loss: 0.32297 | val_0_auc: 0.64311 |  0:07:55s\n",
      "epoch 84 | loss: 0.32039 | val_0_auc: 0.64556 |  0:08:01s\n",
      "epoch 85 | loss: 0.32108 | val_0_auc: 0.65581 |  0:08:07s\n",
      "epoch 86 | loss: 0.3203  | val_0_auc: 0.65056 |  0:08:14s\n",
      "epoch 87 | loss: 0.32138 | val_0_auc: 0.65287 |  0:08:20s\n",
      "epoch 88 | loss: 0.31866 | val_0_auc: 0.6507  |  0:08:26s\n",
      "epoch 89 | loss: 0.31852 | val_0_auc: 0.6484  |  0:08:32s\n",
      "epoch 90 | loss: 0.31823 | val_0_auc: 0.64964 |  0:08:37s\n",
      "epoch 91 | loss: 0.31833 | val_0_auc: 0.65839 |  0:08:42s\n",
      "epoch 92 | loss: 0.31893 | val_0_auc: 0.65868 |  0:08:47s\n",
      "epoch 93 | loss: 0.31779 | val_0_auc: 0.65505 |  0:08:53s\n",
      "epoch 94 | loss: 0.31823 | val_0_auc: 0.65173 |  0:08:58s\n",
      "epoch 95 | loss: 0.3184  | val_0_auc: 0.65419 |  0:09:04s\n",
      "epoch 96 | loss: 0.31816 | val_0_auc: 0.6585  |  0:09:10s\n",
      "epoch 97 | loss: 0.31643 | val_0_auc: 0.65254 |  0:09:16s\n",
      "epoch 98 | loss: 0.31803 | val_0_auc: 0.64943 |  0:09:22s\n",
      "epoch 99 | loss: 0.31568 | val_0_auc: 0.64605 |  0:09:29s\n",
      "epoch 100| loss: 0.31674 | val_0_auc: 0.64923 |  0:09:35s\n",
      "epoch 101| loss: 0.31569 | val_0_auc: 0.64938 |  0:09:40s\n",
      "epoch 102| loss: 0.31939 | val_0_auc: 0.65403 |  0:09:45s\n",
      "epoch 103| loss: 0.31499 | val_0_auc: 0.65038 |  0:09:51s\n",
      "epoch 104| loss: 0.31492 | val_0_auc: 0.65341 |  0:09:55s\n",
      "epoch 105| loss: 0.31905 | val_0_auc: 0.65668 |  0:10:00s\n",
      "epoch 106| loss: 0.31596 | val_0_auc: 0.65689 |  0:10:06s\n",
      "epoch 107| loss: 0.31604 | val_0_auc: 0.65189 |  0:10:11s\n",
      "\n",
      "Early stopping occurred at epoch 107 with best_epoch = 92 and best_val_0_auc = 0.65868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 02:50:25,311] Trial 35 finished with value: 0.6586766770432766 and parameters: {'n_d': 40, 'n_a': 27, 'n_steps': 4, 'gamma': 1.0090767759649073, 'lambda_sparse': 0.0003889511907962204, 'lr': 0.002053331544763867}. Best is trial 34 with value: 0.6598847382449285.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.89577 | val_0_auc: 0.49589 |  0:00:05s\n",
      "epoch 1  | loss: 0.51108 | val_0_auc: 0.48672 |  0:00:09s\n",
      "epoch 2  | loss: 0.4537  | val_0_auc: 0.49564 |  0:00:14s\n",
      "epoch 3  | loss: 0.44304 | val_0_auc: 0.49855 |  0:00:19s\n",
      "epoch 4  | loss: 0.40772 | val_0_auc: 0.52657 |  0:00:25s\n",
      "epoch 5  | loss: 0.39683 | val_0_auc: 0.53085 |  0:00:30s\n",
      "epoch 6  | loss: 0.3891  | val_0_auc: 0.53852 |  0:00:34s\n",
      "epoch 7  | loss: 0.38138 | val_0_auc: 0.53497 |  0:00:39s\n",
      "epoch 8  | loss: 0.37597 | val_0_auc: 0.558   |  0:00:43s\n",
      "epoch 9  | loss: 0.36613 | val_0_auc: 0.56378 |  0:00:47s\n",
      "epoch 10 | loss: 0.36441 | val_0_auc: 0.57003 |  0:00:52s\n",
      "epoch 11 | loss: 0.36239 | val_0_auc: 0.58553 |  0:00:56s\n",
      "epoch 12 | loss: 0.35485 | val_0_auc: 0.57616 |  0:01:00s\n",
      "epoch 13 | loss: 0.35385 | val_0_auc: 0.56755 |  0:01:05s\n",
      "epoch 14 | loss: 0.35639 | val_0_auc: 0.56911 |  0:01:09s\n",
      "epoch 15 | loss: 0.34876 | val_0_auc: 0.57321 |  0:01:13s\n",
      "epoch 16 | loss: 0.35298 | val_0_auc: 0.57689 |  0:01:17s\n",
      "epoch 17 | loss: 0.35101 | val_0_auc: 0.56859 |  0:01:22s\n",
      "epoch 18 | loss: 0.34637 | val_0_auc: 0.59336 |  0:01:27s\n",
      "epoch 19 | loss: 0.34423 | val_0_auc: 0.59604 |  0:01:32s\n",
      "epoch 20 | loss: 0.3464  | val_0_auc: 0.59981 |  0:01:37s\n",
      "epoch 21 | loss: 0.33837 | val_0_auc: 0.59382 |  0:01:42s\n",
      "epoch 22 | loss: 0.33713 | val_0_auc: 0.60343 |  0:01:48s\n",
      "epoch 23 | loss: 0.3393  | val_0_auc: 0.60943 |  0:01:52s\n",
      "epoch 24 | loss: 0.33643 | val_0_auc: 0.6093  |  0:01:57s\n",
      "epoch 25 | loss: 0.33697 | val_0_auc: 0.60852 |  0:02:01s\n",
      "epoch 26 | loss: 0.33708 | val_0_auc: 0.61286 |  0:02:05s\n",
      "epoch 27 | loss: 0.33436 | val_0_auc: 0.60372 |  0:02:09s\n",
      "epoch 28 | loss: 0.3376  | val_0_auc: 0.60968 |  0:02:13s\n",
      "epoch 29 | loss: 0.33252 | val_0_auc: 0.60819 |  0:02:17s\n",
      "epoch 30 | loss: 0.3341  | val_0_auc: 0.61234 |  0:02:22s\n",
      "epoch 31 | loss: 0.33348 | val_0_auc: 0.61433 |  0:02:26s\n",
      "epoch 32 | loss: 0.33236 | val_0_auc: 0.61516 |  0:02:31s\n",
      "epoch 33 | loss: 0.32858 | val_0_auc: 0.61725 |  0:02:36s\n",
      "epoch 34 | loss: 0.32881 | val_0_auc: 0.62162 |  0:02:41s\n",
      "epoch 35 | loss: 0.32954 | val_0_auc: 0.62429 |  0:02:46s\n",
      "epoch 36 | loss: 0.32682 | val_0_auc: 0.62861 |  0:02:51s\n",
      "epoch 37 | loss: 0.32639 | val_0_auc: 0.63031 |  0:02:55s\n",
      "epoch 38 | loss: 0.32727 | val_0_auc: 0.63751 |  0:03:00s\n",
      "epoch 39 | loss: 0.32436 | val_0_auc: 0.63951 |  0:03:04s\n",
      "epoch 40 | loss: 0.32329 | val_0_auc: 0.63527 |  0:03:09s\n",
      "epoch 41 | loss: 0.32533 | val_0_auc: 0.64152 |  0:03:14s\n",
      "epoch 42 | loss: 0.3246  | val_0_auc: 0.64105 |  0:03:18s\n",
      "epoch 43 | loss: 0.32447 | val_0_auc: 0.63863 |  0:03:22s\n",
      "epoch 44 | loss: 0.32236 | val_0_auc: 0.63373 |  0:03:26s\n",
      "epoch 45 | loss: 0.32458 | val_0_auc: 0.63575 |  0:03:31s\n",
      "epoch 46 | loss: 0.32431 | val_0_auc: 0.63634 |  0:03:35s\n",
      "epoch 47 | loss: 0.32092 | val_0_auc: 0.64531 |  0:03:39s\n",
      "epoch 48 | loss: 0.32027 | val_0_auc: 0.64162 |  0:03:44s\n",
      "epoch 49 | loss: 0.31987 | val_0_auc: 0.64314 |  0:03:49s\n",
      "epoch 50 | loss: 0.32081 | val_0_auc: 0.64888 |  0:03:53s\n",
      "epoch 51 | loss: 0.31952 | val_0_auc: 0.65365 |  0:03:58s\n",
      "epoch 52 | loss: 0.3182  | val_0_auc: 0.65631 |  0:04:03s\n",
      "epoch 53 | loss: 0.31797 | val_0_auc: 0.66247 |  0:04:08s\n",
      "epoch 54 | loss: 0.31886 | val_0_auc: 0.66531 |  0:04:13s\n",
      "epoch 55 | loss: 0.31492 | val_0_auc: 0.66357 |  0:04:17s\n",
      "epoch 56 | loss: 0.32058 | val_0_auc: 0.65868 |  0:04:22s\n",
      "epoch 57 | loss: 0.31591 | val_0_auc: 0.65848 |  0:04:27s\n",
      "epoch 58 | loss: 0.31773 | val_0_auc: 0.65376 |  0:04:31s\n",
      "epoch 59 | loss: 0.31518 | val_0_auc: 0.64529 |  0:04:35s\n",
      "epoch 60 | loss: 0.31236 | val_0_auc: 0.64927 |  0:04:39s\n",
      "epoch 61 | loss: 0.31606 | val_0_auc: 0.65965 |  0:04:43s\n",
      "epoch 62 | loss: 0.31703 | val_0_auc: 0.65915 |  0:04:48s\n",
      "epoch 63 | loss: 0.31624 | val_0_auc: 0.66114 |  0:04:53s\n",
      "epoch 64 | loss: 0.31277 | val_0_auc: 0.66606 |  0:04:58s\n",
      "epoch 65 | loss: 0.3117  | val_0_auc: 0.66531 |  0:05:03s\n",
      "epoch 66 | loss: 0.3107  | val_0_auc: 0.66301 |  0:05:08s\n",
      "epoch 67 | loss: 0.31327 | val_0_auc: 0.65914 |  0:05:13s\n",
      "epoch 68 | loss: 0.31255 | val_0_auc: 0.65516 |  0:05:18s\n",
      "epoch 69 | loss: 0.31133 | val_0_auc: 0.65625 |  0:05:23s\n",
      "epoch 70 | loss: 0.31151 | val_0_auc: 0.65486 |  0:05:28s\n",
      "epoch 71 | loss: 0.31052 | val_0_auc: 0.65808 |  0:05:32s\n",
      "epoch 72 | loss: 0.3116  | val_0_auc: 0.65449 |  0:05:36s\n",
      "epoch 73 | loss: 0.31261 | val_0_auc: 0.65353 |  0:05:41s\n",
      "epoch 74 | loss: 0.30988 | val_0_auc: 0.65847 |  0:05:45s\n",
      "epoch 75 | loss: 0.30801 | val_0_auc: 0.65457 |  0:05:50s\n",
      "epoch 76 | loss: 0.30944 | val_0_auc: 0.65123 |  0:05:55s\n",
      "epoch 77 | loss: 0.31119 | val_0_auc: 0.65561 |  0:06:00s\n",
      "epoch 78 | loss: 0.30835 | val_0_auc: 0.65566 |  0:06:05s\n",
      "epoch 79 | loss: 0.3086  | val_0_auc: 0.65587 |  0:06:10s\n",
      "\n",
      "Early stopping occurred at epoch 79 with best_epoch = 64 and best_val_0_auc = 0.66606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 02:56:37,811] Trial 36 finished with value: 0.6660550062056196 and parameters: {'n_d': 43, 'n_a': 26, 'n_steps': 3, 'gamma': 1.0167676577817777, 'lambda_sparse': 0.0003870983872813724, 'lr': 0.0022631373167367017}. Best is trial 36 with value: 0.6660550062056196.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.84011 | val_0_auc: 0.47241 |  0:00:04s\n",
      "epoch 1  | loss: 0.48291 | val_0_auc: 0.47742 |  0:00:08s\n",
      "epoch 2  | loss: 0.44915 | val_0_auc: 0.48317 |  0:00:13s\n",
      "epoch 3  | loss: 0.41172 | val_0_auc: 0.49943 |  0:00:17s\n",
      "epoch 4  | loss: 0.39372 | val_0_auc: 0.51446 |  0:00:21s\n",
      "epoch 5  | loss: 0.38386 | val_0_auc: 0.52737 |  0:00:26s\n",
      "epoch 6  | loss: 0.37576 | val_0_auc: 0.5288  |  0:00:31s\n",
      "epoch 7  | loss: 0.36797 | val_0_auc: 0.54607 |  0:00:35s\n",
      "epoch 8  | loss: 0.36299 | val_0_auc: 0.56372 |  0:00:40s\n",
      "epoch 9  | loss: 0.36309 | val_0_auc: 0.55485 |  0:00:44s\n",
      "epoch 10 | loss: 0.35338 | val_0_auc: 0.54523 |  0:00:48s\n",
      "epoch 11 | loss: 0.35065 | val_0_auc: 0.5605  |  0:00:52s\n",
      "epoch 12 | loss: 0.35406 | val_0_auc: 0.57741 |  0:00:56s\n",
      "epoch 13 | loss: 0.35149 | val_0_auc: 0.57569 |  0:01:01s\n",
      "epoch 14 | loss: 0.34814 | val_0_auc: 0.582   |  0:01:05s\n",
      "epoch 15 | loss: 0.34145 | val_0_auc: 0.60502 |  0:01:10s\n",
      "epoch 16 | loss: 0.3484  | val_0_auc: 0.60394 |  0:01:14s\n",
      "epoch 17 | loss: 0.33731 | val_0_auc: 0.60042 |  0:01:19s\n",
      "epoch 18 | loss: 0.33592 | val_0_auc: 0.6091  |  0:01:24s\n",
      "epoch 19 | loss: 0.33866 | val_0_auc: 0.61478 |  0:01:29s\n",
      "epoch 20 | loss: 0.33612 | val_0_auc: 0.61972 |  0:01:34s\n",
      "epoch 21 | loss: 0.33316 | val_0_auc: 0.62183 |  0:01:39s\n",
      "epoch 22 | loss: 0.33232 | val_0_auc: 0.62256 |  0:01:44s\n",
      "epoch 23 | loss: 0.33161 | val_0_auc: 0.62172 |  0:01:48s\n",
      "epoch 24 | loss: 0.32913 | val_0_auc: 0.6257  |  0:01:52s\n",
      "epoch 25 | loss: 0.32755 | val_0_auc: 0.62906 |  0:01:56s\n",
      "epoch 26 | loss: 0.32764 | val_0_auc: 0.62861 |  0:02:00s\n",
      "epoch 27 | loss: 0.32781 | val_0_auc: 0.6297  |  0:02:04s\n",
      "epoch 28 | loss: 0.32969 | val_0_auc: 0.6284  |  0:02:08s\n",
      "epoch 29 | loss: 0.32451 | val_0_auc: 0.62881 |  0:02:13s\n",
      "epoch 30 | loss: 0.32486 | val_0_auc: 0.63596 |  0:02:17s\n",
      "epoch 31 | loss: 0.32356 | val_0_auc: 0.63923 |  0:02:22s\n",
      "epoch 32 | loss: 0.32081 | val_0_auc: 0.63577 |  0:02:27s\n",
      "epoch 33 | loss: 0.32088 | val_0_auc: 0.63753 |  0:02:31s\n",
      "epoch 34 | loss: 0.32001 | val_0_auc: 0.64343 |  0:02:35s\n",
      "epoch 35 | loss: 0.32156 | val_0_auc: 0.644   |  0:02:39s\n",
      "epoch 36 | loss: 0.32017 | val_0_auc: 0.64641 |  0:02:44s\n",
      "epoch 37 | loss: 0.31983 | val_0_auc: 0.65457 |  0:02:49s\n",
      "epoch 38 | loss: 0.31844 | val_0_auc: 0.65473 |  0:02:53s\n",
      "epoch 39 | loss: 0.31825 | val_0_auc: 0.65055 |  0:02:58s\n",
      "epoch 40 | loss: 0.31755 | val_0_auc: 0.65231 |  0:03:03s\n",
      "epoch 41 | loss: 0.31773 | val_0_auc: 0.65664 |  0:03:09s\n",
      "epoch 42 | loss: 0.3185  | val_0_auc: 0.65151 |  0:03:14s\n",
      "epoch 43 | loss: 0.31556 | val_0_auc: 0.64394 |  0:03:19s\n",
      "epoch 44 | loss: 0.31684 | val_0_auc: 0.64207 |  0:03:23s\n",
      "epoch 45 | loss: 0.31541 | val_0_auc: 0.6379  |  0:03:27s\n",
      "epoch 46 | loss: 0.31698 | val_0_auc: 0.64063 |  0:03:31s\n",
      "epoch 47 | loss: 0.31568 | val_0_auc: 0.63785 |  0:03:36s\n",
      "epoch 48 | loss: 0.31226 | val_0_auc: 0.63736 |  0:03:40s\n",
      "epoch 49 | loss: 0.313   | val_0_auc: 0.64168 |  0:03:45s\n",
      "epoch 50 | loss: 0.31272 | val_0_auc: 0.64927 |  0:03:49s\n",
      "epoch 51 | loss: 0.31576 | val_0_auc: 0.645   |  0:03:54s\n",
      "epoch 52 | loss: 0.31167 | val_0_auc: 0.64443 |  0:04:00s\n",
      "epoch 53 | loss: 0.31037 | val_0_auc: 0.64472 |  0:04:05s\n",
      "epoch 54 | loss: 0.3112  | val_0_auc: 0.64333 |  0:04:10s\n",
      "epoch 55 | loss: 0.30968 | val_0_auc: 0.64705 |  0:04:15s\n",
      "epoch 56 | loss: 0.31162 | val_0_auc: 0.64919 |  0:04:19s\n",
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 41 and best_val_0_auc = 0.65664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 03:00:59,186] Trial 37 finished with value: 0.6566428106370424 and parameters: {'n_d': 43, 'n_a': 26, 'n_steps': 3, 'gamma': 1.01338999537851, 'lambda_sparse': 0.0004489704414174315, 'lr': 0.0029808822690002373}. Best is trial 36 with value: 0.6660550062056196.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.59557 | val_0_auc: 0.46469 |  0:00:04s\n",
      "epoch 1  | loss: 0.4617  | val_0_auc: 0.46914 |  0:00:08s\n",
      "epoch 2  | loss: 0.43962 | val_0_auc: 0.48723 |  0:00:12s\n",
      "epoch 3  | loss: 0.4058  | val_0_auc: 0.49664 |  0:00:17s\n",
      "epoch 4  | loss: 0.39179 | val_0_auc: 0.51021 |  0:00:21s\n",
      "epoch 5  | loss: 0.38259 | val_0_auc: 0.51496 |  0:00:25s\n",
      "epoch 6  | loss: 0.37538 | val_0_auc: 0.51674 |  0:00:29s\n",
      "epoch 7  | loss: 0.36696 | val_0_auc: 0.52705 |  0:00:33s\n",
      "epoch 8  | loss: 0.36355 | val_0_auc: 0.53222 |  0:00:38s\n",
      "epoch 9  | loss: 0.35865 | val_0_auc: 0.53816 |  0:00:43s\n",
      "epoch 10 | loss: 0.35447 | val_0_auc: 0.55772 |  0:00:47s\n",
      "epoch 11 | loss: 0.34752 | val_0_auc: 0.56719 |  0:00:52s\n",
      "epoch 12 | loss: 0.35096 | val_0_auc: 0.57011 |  0:00:58s\n",
      "epoch 13 | loss: 0.34687 | val_0_auc: 0.58387 |  0:01:03s\n",
      "epoch 14 | loss: 0.34069 | val_0_auc: 0.5874  |  0:01:08s\n",
      "epoch 15 | loss: 0.3414  | val_0_auc: 0.59066 |  0:01:12s\n",
      "epoch 16 | loss: 0.33996 | val_0_auc: 0.5795  |  0:01:16s\n",
      "epoch 17 | loss: 0.33728 | val_0_auc: 0.58129 |  0:01:20s\n",
      "epoch 18 | loss: 0.3364  | val_0_auc: 0.58152 |  0:01:25s\n",
      "epoch 19 | loss: 0.3354  | val_0_auc: 0.58239 |  0:01:29s\n",
      "epoch 20 | loss: 0.3357  | val_0_auc: 0.57897 |  0:01:33s\n",
      "epoch 21 | loss: 0.33262 | val_0_auc: 0.58586 |  0:01:37s\n",
      "epoch 22 | loss: 0.33523 | val_0_auc: 0.59519 |  0:01:41s\n",
      "epoch 23 | loss: 0.33373 | val_0_auc: 0.59892 |  0:01:46s\n",
      "epoch 24 | loss: 0.33092 | val_0_auc: 0.59814 |  0:01:50s\n",
      "epoch 25 | loss: 0.33174 | val_0_auc: 0.59702 |  0:01:55s\n",
      "epoch 26 | loss: 0.33135 | val_0_auc: 0.59913 |  0:02:01s\n",
      "epoch 27 | loss: 0.32977 | val_0_auc: 0.59797 |  0:02:06s\n",
      "epoch 28 | loss: 0.32848 | val_0_auc: 0.61025 |  0:02:10s\n",
      "epoch 29 | loss: 0.32545 | val_0_auc: 0.60369 |  0:02:15s\n",
      "epoch 30 | loss: 0.3235  | val_0_auc: 0.6099  |  0:02:19s\n",
      "epoch 31 | loss: 0.32306 | val_0_auc: 0.61208 |  0:02:24s\n",
      "epoch 32 | loss: 0.32388 | val_0_auc: 0.61412 |  0:02:29s\n",
      "epoch 33 | loss: 0.32392 | val_0_auc: 0.61152 |  0:02:33s\n",
      "epoch 34 | loss: 0.32255 | val_0_auc: 0.60671 |  0:02:37s\n",
      "epoch 35 | loss: 0.32207 | val_0_auc: 0.61234 |  0:02:41s\n",
      "epoch 36 | loss: 0.32155 | val_0_auc: 0.61145 |  0:02:45s\n",
      "epoch 37 | loss: 0.32126 | val_0_auc: 0.61191 |  0:02:49s\n",
      "epoch 38 | loss: 0.31941 | val_0_auc: 0.61598 |  0:02:54s\n",
      "epoch 39 | loss: 0.31905 | val_0_auc: 0.60955 |  0:02:59s\n",
      "epoch 40 | loss: 0.31931 | val_0_auc: 0.61224 |  0:03:04s\n",
      "epoch 41 | loss: 0.31766 | val_0_auc: 0.61962 |  0:03:09s\n",
      "epoch 42 | loss: 0.31584 | val_0_auc: 0.62805 |  0:03:14s\n",
      "epoch 43 | loss: 0.31842 | val_0_auc: 0.63084 |  0:03:20s\n",
      "epoch 44 | loss: 0.31629 | val_0_auc: 0.63088 |  0:03:24s\n",
      "epoch 45 | loss: 0.31654 | val_0_auc: 0.63635 |  0:03:28s\n",
      "epoch 46 | loss: 0.31528 | val_0_auc: 0.63204 |  0:03:32s\n",
      "epoch 47 | loss: 0.31507 | val_0_auc: 0.63042 |  0:03:36s\n",
      "epoch 48 | loss: 0.31578 | val_0_auc: 0.63099 |  0:03:41s\n",
      "epoch 49 | loss: 0.31496 | val_0_auc: 0.62816 |  0:03:46s\n",
      "epoch 50 | loss: 0.31435 | val_0_auc: 0.63156 |  0:03:51s\n",
      "epoch 51 | loss: 0.31249 | val_0_auc: 0.62814 |  0:03:57s\n",
      "epoch 52 | loss: 0.31341 | val_0_auc: 0.63191 |  0:04:02s\n",
      "epoch 53 | loss: 0.31192 | val_0_auc: 0.63439 |  0:04:07s\n",
      "epoch 54 | loss: 0.3112  | val_0_auc: 0.63586 |  0:04:11s\n",
      "epoch 55 | loss: 0.3122  | val_0_auc: 0.63001 |  0:04:16s\n",
      "epoch 56 | loss: 0.31203 | val_0_auc: 0.62236 |  0:04:20s\n",
      "epoch 57 | loss: 0.31016 | val_0_auc: 0.62171 |  0:04:25s\n",
      "epoch 58 | loss: 0.31081 | val_0_auc: 0.62853 |  0:04:29s\n",
      "epoch 59 | loss: 0.31293 | val_0_auc: 0.62406 |  0:04:33s\n",
      "epoch 60 | loss: 0.30935 | val_0_auc: 0.62874 |  0:04:38s\n",
      "\n",
      "Early stopping occurred at epoch 60 with best_epoch = 45 and best_val_0_auc = 0.63635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 03:05:39,603] Trial 38 finished with value: 0.6363515788724085 and parameters: {'n_d': 44, 'n_a': 23, 'n_steps': 3, 'gamma': 1.0034109562048865, 'lambda_sparse': 0.0005749221474807429, 'lr': 0.002531899112685773}. Best is trial 36 with value: 0.6660550062056196.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.62862 | val_0_auc: 0.46198 |  0:00:04s\n",
      "epoch 1  | loss: 0.58395 | val_0_auc: 0.45701 |  0:00:08s\n",
      "epoch 2  | loss: 0.54264 | val_0_auc: 0.45336 |  0:00:13s\n",
      "epoch 3  | loss: 0.52214 | val_0_auc: 0.45181 |  0:00:18s\n",
      "epoch 4  | loss: 0.49415 | val_0_auc: 0.45392 |  0:00:22s\n",
      "epoch 5  | loss: 0.48106 | val_0_auc: 0.44767 |  0:00:27s\n",
      "epoch 6  | loss: 0.47722 | val_0_auc: 0.44555 |  0:00:33s\n",
      "epoch 7  | loss: 0.48204 | val_0_auc: 0.45231 |  0:00:38s\n",
      "epoch 8  | loss: 0.47133 | val_0_auc: 0.45425 |  0:00:43s\n",
      "epoch 9  | loss: 0.4547  | val_0_auc: 0.45526 |  0:00:47s\n",
      "epoch 10 | loss: 0.44562 | val_0_auc: 0.45833 |  0:00:52s\n",
      "epoch 11 | loss: 0.44171 | val_0_auc: 0.45754 |  0:00:57s\n",
      "epoch 12 | loss: 0.43816 | val_0_auc: 0.46226 |  0:01:02s\n",
      "epoch 13 | loss: 0.42974 | val_0_auc: 0.46504 |  0:01:07s\n",
      "epoch 14 | loss: 0.42618 | val_0_auc: 0.45771 |  0:01:12s\n",
      "epoch 15 | loss: 0.42195 | val_0_auc: 0.45867 |  0:01:16s\n",
      "epoch 16 | loss: 0.42039 | val_0_auc: 0.45991 |  0:01:21s\n",
      "epoch 17 | loss: 0.41389 | val_0_auc: 0.46617 |  0:01:25s\n",
      "epoch 18 | loss: 0.41077 | val_0_auc: 0.46751 |  0:01:29s\n",
      "epoch 19 | loss: 0.41183 | val_0_auc: 0.4703  |  0:01:33s\n",
      "epoch 20 | loss: 0.40596 | val_0_auc: 0.46862 |  0:01:38s\n",
      "epoch 21 | loss: 0.40678 | val_0_auc: 0.47221 |  0:01:43s\n",
      "epoch 22 | loss: 0.39826 | val_0_auc: 0.47905 |  0:01:48s\n",
      "epoch 23 | loss: 0.39703 | val_0_auc: 0.4766  |  0:01:52s\n",
      "epoch 24 | loss: 0.40055 | val_0_auc: 0.48111 |  0:01:56s\n",
      "epoch 25 | loss: 0.3968  | val_0_auc: 0.48776 |  0:02:00s\n",
      "epoch 26 | loss: 0.40206 | val_0_auc: 0.48818 |  0:02:05s\n",
      "epoch 27 | loss: 0.39321 | val_0_auc: 0.48715 |  0:02:09s\n",
      "epoch 28 | loss: 0.39148 | val_0_auc: 0.48937 |  0:02:14s\n",
      "epoch 29 | loss: 0.39141 | val_0_auc: 0.48829 |  0:02:19s\n",
      "epoch 30 | loss: 0.39235 | val_0_auc: 0.48817 |  0:02:23s\n",
      "epoch 31 | loss: 0.38385 | val_0_auc: 0.49151 |  0:02:27s\n",
      "epoch 32 | loss: 0.38246 | val_0_auc: 0.48892 |  0:02:31s\n",
      "epoch 33 | loss: 0.3841  | val_0_auc: 0.4935  |  0:02:36s\n",
      "epoch 34 | loss: 0.38015 | val_0_auc: 0.49328 |  0:02:41s\n",
      "epoch 35 | loss: 0.3777  | val_0_auc: 0.49621 |  0:02:46s\n",
      "epoch 36 | loss: 0.38226 | val_0_auc: 0.49692 |  0:02:51s\n",
      "epoch 37 | loss: 0.37524 | val_0_auc: 0.49649 |  0:02:56s\n",
      "epoch 38 | loss: 0.37608 | val_0_auc: 0.49671 |  0:03:01s\n",
      "epoch 39 | loss: 0.37355 | val_0_auc: 0.49346 |  0:03:06s\n",
      "epoch 40 | loss: 0.37411 | val_0_auc: 0.50048 |  0:03:11s\n",
      "epoch 41 | loss: 0.37875 | val_0_auc: 0.50445 |  0:03:15s\n",
      "epoch 42 | loss: 0.37285 | val_0_auc: 0.50752 |  0:03:19s\n",
      "epoch 43 | loss: 0.36751 | val_0_auc: 0.5095  |  0:03:23s\n",
      "epoch 44 | loss: 0.37308 | val_0_auc: 0.51455 |  0:03:28s\n",
      "epoch 45 | loss: 0.36604 | val_0_auc: 0.51171 |  0:03:32s\n",
      "epoch 46 | loss: 0.36538 | val_0_auc: 0.51107 |  0:03:36s\n",
      "epoch 47 | loss: 0.36714 | val_0_auc: 0.51311 |  0:03:40s\n",
      "epoch 48 | loss: 0.36067 | val_0_auc: 0.51547 |  0:03:45s\n",
      "epoch 49 | loss: 0.3627  | val_0_auc: 0.52243 |  0:03:49s\n",
      "epoch 50 | loss: 0.36398 | val_0_auc: 0.52297 |  0:03:53s\n",
      "epoch 51 | loss: 0.35956 | val_0_auc: 0.5239  |  0:03:58s\n",
      "epoch 52 | loss: 0.36131 | val_0_auc: 0.52542 |  0:04:03s\n",
      "epoch 53 | loss: 0.36269 | val_0_auc: 0.53022 |  0:04:08s\n",
      "epoch 54 | loss: 0.35968 | val_0_auc: 0.52755 |  0:04:13s\n",
      "epoch 55 | loss: 0.3571  | val_0_auc: 0.52573 |  0:04:19s\n",
      "epoch 56 | loss: 0.35871 | val_0_auc: 0.52582 |  0:04:24s\n",
      "epoch 57 | loss: 0.35877 | val_0_auc: 0.52346 |  0:04:29s\n",
      "epoch 58 | loss: 0.35805 | val_0_auc: 0.52757 |  0:04:33s\n",
      "epoch 59 | loss: 0.35817 | val_0_auc: 0.52746 |  0:04:38s\n",
      "epoch 60 | loss: 0.35892 | val_0_auc: 0.5296  |  0:04:42s\n",
      "epoch 61 | loss: 0.35275 | val_0_auc: 0.52264 |  0:04:46s\n",
      "epoch 62 | loss: 0.35533 | val_0_auc: 0.51912 |  0:04:50s\n",
      "epoch 63 | loss: 0.35464 | val_0_auc: 0.52141 |  0:04:54s\n",
      "epoch 64 | loss: 0.35754 | val_0_auc: 0.52027 |  0:04:59s\n",
      "epoch 65 | loss: 0.35289 | val_0_auc: 0.52851 |  0:05:03s\n",
      "epoch 66 | loss: 0.34679 | val_0_auc: 0.53393 |  0:05:07s\n",
      "epoch 67 | loss: 0.35049 | val_0_auc: 0.53329 |  0:05:11s\n",
      "epoch 68 | loss: 0.35113 | val_0_auc: 0.5296  |  0:05:16s\n",
      "epoch 69 | loss: 0.35105 | val_0_auc: 0.53283 |  0:05:20s\n",
      "epoch 70 | loss: 0.34748 | val_0_auc: 0.53466 |  0:05:25s\n",
      "epoch 71 | loss: 0.3516  | val_0_auc: 0.53274 |  0:05:30s\n",
      "epoch 72 | loss: 0.3477  | val_0_auc: 0.53679 |  0:05:35s\n",
      "epoch 73 | loss: 0.35035 | val_0_auc: 0.5339  |  0:05:40s\n",
      "epoch 74 | loss: 0.34765 | val_0_auc: 0.53002 |  0:05:44s\n",
      "epoch 75 | loss: 0.34944 | val_0_auc: 0.53316 |  0:05:49s\n",
      "epoch 76 | loss: 0.348   | val_0_auc: 0.54209 |  0:05:53s\n",
      "epoch 77 | loss: 0.3479  | val_0_auc: 0.54231 |  0:05:58s\n",
      "epoch 78 | loss: 0.3521  | val_0_auc: 0.53793 |  0:06:02s\n",
      "epoch 79 | loss: 0.34767 | val_0_auc: 0.53565 |  0:06:06s\n",
      "epoch 80 | loss: 0.34552 | val_0_auc: 0.53451 |  0:06:10s\n",
      "epoch 81 | loss: 0.34831 | val_0_auc: 0.54216 |  0:06:14s\n",
      "epoch 82 | loss: 0.34507 | val_0_auc: 0.5349  |  0:06:19s\n",
      "epoch 83 | loss: 0.34729 | val_0_auc: 0.53544 |  0:06:24s\n",
      "epoch 84 | loss: 0.3438  | val_0_auc: 0.53784 |  0:06:28s\n",
      "epoch 85 | loss: 0.3431  | val_0_auc: 0.53971 |  0:06:34s\n",
      "epoch 86 | loss: 0.34149 | val_0_auc: 0.54229 |  0:06:39s\n",
      "epoch 87 | loss: 0.34719 | val_0_auc: 0.54046 |  0:06:43s\n",
      "epoch 88 | loss: 0.34494 | val_0_auc: 0.54553 |  0:06:48s\n",
      "epoch 89 | loss: 0.34495 | val_0_auc: 0.54643 |  0:06:52s\n",
      "epoch 90 | loss: 0.34212 | val_0_auc: 0.54301 |  0:06:56s\n",
      "epoch 91 | loss: 0.34441 | val_0_auc: 0.54723 |  0:07:00s\n",
      "epoch 92 | loss: 0.34249 | val_0_auc: 0.54227 |  0:07:05s\n",
      "epoch 93 | loss: 0.3392  | val_0_auc: 0.5484  |  0:07:10s\n",
      "epoch 94 | loss: 0.34009 | val_0_auc: 0.55055 |  0:07:15s\n",
      "epoch 95 | loss: 0.33942 | val_0_auc: 0.54866 |  0:07:19s\n",
      "epoch 96 | loss: 0.33965 | val_0_auc: 0.54749 |  0:07:25s\n",
      "epoch 97 | loss: 0.34173 | val_0_auc: 0.55295 |  0:07:29s\n",
      "epoch 98 | loss: 0.33971 | val_0_auc: 0.55459 |  0:07:34s\n",
      "epoch 99 | loss: 0.33971 | val_0_auc: 0.55343 |  0:07:38s\n",
      "epoch 100| loss: 0.33933 | val_0_auc: 0.5507  |  0:07:42s\n",
      "epoch 101| loss: 0.34109 | val_0_auc: 0.55421 |  0:07:46s\n",
      "epoch 102| loss: 0.33841 | val_0_auc: 0.55233 |  0:07:51s\n",
      "epoch 103| loss: 0.34149 | val_0_auc: 0.55456 |  0:07:55s\n",
      "epoch 104| loss: 0.3353  | val_0_auc: 0.5529  |  0:07:59s\n",
      "epoch 105| loss: 0.33754 | val_0_auc: 0.55501 |  0:08:04s\n",
      "epoch 106| loss: 0.33904 | val_0_auc: 0.55676 |  0:08:08s\n",
      "epoch 107| loss: 0.34025 | val_0_auc: 0.55684 |  0:08:13s\n",
      "epoch 108| loss: 0.33601 | val_0_auc: 0.55754 |  0:08:18s\n",
      "epoch 109| loss: 0.33597 | val_0_auc: 0.55283 |  0:08:22s\n",
      "epoch 110| loss: 0.33587 | val_0_auc: 0.55244 |  0:08:28s\n",
      "epoch 111| loss: 0.34005 | val_0_auc: 0.5551  |  0:08:34s\n",
      "epoch 112| loss: 0.33631 | val_0_auc: 0.56291 |  0:08:39s\n",
      "epoch 113| loss: 0.33696 | val_0_auc: 0.56026 |  0:08:43s\n",
      "epoch 114| loss: 0.33854 | val_0_auc: 0.56315 |  0:08:48s\n",
      "epoch 115| loss: 0.33814 | val_0_auc: 0.56133 |  0:08:52s\n",
      "epoch 116| loss: 0.33551 | val_0_auc: 0.5659  |  0:08:56s\n",
      "epoch 117| loss: 0.33537 | val_0_auc: 0.56212 |  0:09:00s\n",
      "epoch 118| loss: 0.33479 | val_0_auc: 0.56167 |  0:09:04s\n",
      "epoch 119| loss: 0.33555 | val_0_auc: 0.56615 |  0:09:09s\n",
      "epoch 120| loss: 0.33343 | val_0_auc: 0.5632  |  0:09:13s\n",
      "epoch 121| loss: 0.33216 | val_0_auc: 0.56581 |  0:09:18s\n",
      "epoch 122| loss: 0.3369  | val_0_auc: 0.56547 |  0:09:23s\n",
      "epoch 123| loss: 0.33321 | val_0_auc: 0.56548 |  0:09:28s\n",
      "epoch 124| loss: 0.33586 | val_0_auc: 0.56629 |  0:09:32s\n",
      "epoch 125| loss: 0.33311 | val_0_auc: 0.56945 |  0:09:37s\n",
      "epoch 126| loss: 0.33413 | val_0_auc: 0.56854 |  0:09:42s\n",
      "epoch 127| loss: 0.33321 | val_0_auc: 0.57126 |  0:09:47s\n",
      "epoch 128| loss: 0.33285 | val_0_auc: 0.56637 |  0:09:52s\n",
      "epoch 129| loss: 0.32971 | val_0_auc: 0.5705  |  0:09:57s\n",
      "epoch 130| loss: 0.33236 | val_0_auc: 0.573   |  0:10:02s\n",
      "epoch 131| loss: 0.33084 | val_0_auc: 0.57346 |  0:10:07s\n",
      "epoch 132| loss: 0.33548 | val_0_auc: 0.57165 |  0:10:12s\n",
      "epoch 133| loss: 0.33238 | val_0_auc: 0.57158 |  0:10:16s\n",
      "epoch 134| loss: 0.33213 | val_0_auc: 0.57055 |  0:10:21s\n",
      "epoch 135| loss: 0.33289 | val_0_auc: 0.56922 |  0:10:26s\n",
      "epoch 136| loss: 0.33263 | val_0_auc: 0.56902 |  0:10:31s\n",
      "epoch 137| loss: 0.33281 | val_0_auc: 0.5683  |  0:10:35s\n",
      "epoch 138| loss: 0.33076 | val_0_auc: 0.57224 |  0:10:40s\n",
      "epoch 139| loss: 0.33271 | val_0_auc: 0.57454 |  0:10:44s\n",
      "epoch 140| loss: 0.33093 | val_0_auc: 0.57315 |  0:10:48s\n",
      "epoch 141| loss: 0.33014 | val_0_auc: 0.57137 |  0:10:53s\n",
      "epoch 142| loss: 0.33277 | val_0_auc: 0.56775 |  0:10:57s\n",
      "epoch 143| loss: 0.33033 | val_0_auc: 0.57013 |  0:11:02s\n",
      "epoch 144| loss: 0.33136 | val_0_auc: 0.57268 |  0:11:07s\n",
      "epoch 145| loss: 0.32931 | val_0_auc: 0.57301 |  0:11:12s\n",
      "epoch 146| loss: 0.33132 | val_0_auc: 0.57369 |  0:11:17s\n",
      "epoch 147| loss: 0.32969 | val_0_auc: 0.57181 |  0:11:22s\n",
      "epoch 148| loss: 0.32839 | val_0_auc: 0.57068 |  0:11:27s\n",
      "epoch 149| loss: 0.33177 | val_0_auc: 0.57525 |  0:11:32s\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_0_auc = 0.57525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 03:17:13,725] Trial 39 finished with value: 0.5752517853872917 and parameters: {'n_d': 41, 'n_a': 28, 'n_steps': 3, 'gamma': 1.0196167884493428, 'lambda_sparse': 0.0005138468417352428, 'lr': 0.00033479881797975845}. Best is trial 36 with value: 0.6660550062056196.\n",
      "Best AUC: 0.6660550062056196\n",
      "Best parameters: {'n_d': 43, 'n_a': 26, 'n_steps': 3, 'gamma': 1.0167676577817777, 'lambda_sparse': 0.0003870983872813724, 'lr': 0.0022631373167367017}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=40, show_progress_bar=True)\n",
    "\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best parameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304eed7",
   "metadata": {},
   "source": [
    "Final retraining using the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3f4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.90639 | val_0_auc: 0.49286 |  0:00:04s\n",
      "epoch 1  | loss: 0.53552 | val_0_auc: 0.47466 |  0:00:08s\n",
      "epoch 2  | loss: 0.47212 | val_0_auc: 0.47876 |  0:00:12s\n",
      "epoch 3  | loss: 0.45079 | val_0_auc: 0.48696 |  0:00:16s\n",
      "epoch 4  | loss: 0.41561 | val_0_auc: 0.50481 |  0:00:19s\n",
      "epoch 5  | loss: 0.40963 | val_0_auc: 0.52041 |  0:00:23s\n",
      "epoch 6  | loss: 0.3964  | val_0_auc: 0.53619 |  0:00:27s\n",
      "epoch 7  | loss: 0.38757 | val_0_auc: 0.53538 |  0:00:31s\n",
      "epoch 8  | loss: 0.38161 | val_0_auc: 0.53778 |  0:00:36s\n",
      "epoch 9  | loss: 0.37441 | val_0_auc: 0.55833 |  0:00:40s\n",
      "epoch 10 | loss: 0.36532 | val_0_auc: 0.54451 |  0:00:45s\n",
      "epoch 11 | loss: 0.36777 | val_0_auc: 0.54887 |  0:00:49s\n",
      "epoch 12 | loss: 0.36526 | val_0_auc: 0.55608 |  0:00:54s\n",
      "epoch 13 | loss: 0.36341 | val_0_auc: 0.56289 |  0:00:58s\n",
      "epoch 14 | loss: 0.35651 | val_0_auc: 0.56591 |  0:01:03s\n",
      "epoch 15 | loss: 0.36125 | val_0_auc: 0.57567 |  0:01:07s\n",
      "epoch 16 | loss: 0.35238 | val_0_auc: 0.57757 |  0:01:11s\n",
      "epoch 17 | loss: 0.34806 | val_0_auc: 0.58024 |  0:01:14s\n",
      "epoch 18 | loss: 0.35108 | val_0_auc: 0.58969 |  0:01:18s\n",
      "epoch 19 | loss: 0.34963 | val_0_auc: 0.58977 |  0:01:22s\n",
      "epoch 20 | loss: 0.34782 | val_0_auc: 0.58783 |  0:01:26s\n",
      "epoch 21 | loss: 0.34787 | val_0_auc: 0.59372 |  0:01:31s\n",
      "epoch 22 | loss: 0.34984 | val_0_auc: 0.60134 |  0:01:35s\n",
      "epoch 23 | loss: 0.34525 | val_0_auc: 0.60935 |  0:01:40s\n",
      "epoch 24 | loss: 0.34002 | val_0_auc: 0.61185 |  0:01:45s\n",
      "epoch 25 | loss: 0.33702 | val_0_auc: 0.60913 |  0:01:50s\n",
      "epoch 26 | loss: 0.34436 | val_0_auc: 0.6038  |  0:01:54s\n",
      "epoch 27 | loss: 0.3354  | val_0_auc: 0.60867 |  0:01:59s\n",
      "epoch 28 | loss: 0.34141 | val_0_auc: 0.60643 |  0:02:04s\n",
      "epoch 29 | loss: 0.33665 | val_0_auc: 0.6111  |  0:02:08s\n",
      "epoch 30 | loss: 0.33693 | val_0_auc: 0.61237 |  0:02:12s\n",
      "epoch 31 | loss: 0.33146 | val_0_auc: 0.60909 |  0:02:16s\n",
      "epoch 32 | loss: 0.33972 | val_0_auc: 0.61498 |  0:02:20s\n",
      "epoch 33 | loss: 0.33591 | val_0_auc: 0.6153  |  0:02:24s\n",
      "epoch 34 | loss: 0.33268 | val_0_auc: 0.61652 |  0:02:28s\n",
      "epoch 35 | loss: 0.33231 | val_0_auc: 0.62453 |  0:02:32s\n",
      "epoch 36 | loss: 0.32911 | val_0_auc: 0.62138 |  0:02:37s\n",
      "epoch 37 | loss: 0.33359 | val_0_auc: 0.62213 |  0:02:41s\n",
      "epoch 38 | loss: 0.3315  | val_0_auc: 0.62813 |  0:02:45s\n",
      "epoch 39 | loss: 0.329   | val_0_auc: 0.6324  |  0:02:49s\n",
      "epoch 40 | loss: 0.33366 | val_0_auc: 0.6293  |  0:02:54s\n",
      "epoch 41 | loss: 0.32444 | val_0_auc: 0.63253 |  0:02:58s\n",
      "epoch 42 | loss: 0.32803 | val_0_auc: 0.63575 |  0:03:03s\n",
      "epoch 43 | loss: 0.32937 | val_0_auc: 0.63367 |  0:03:08s\n",
      "epoch 44 | loss: 0.32448 | val_0_auc: 0.63254 |  0:03:13s\n",
      "epoch 45 | loss: 0.32659 | val_0_auc: 0.63084 |  0:03:17s\n",
      "epoch 46 | loss: 0.32451 | val_0_auc: 0.63328 |  0:03:22s\n",
      "epoch 47 | loss: 0.32481 | val_0_auc: 0.63564 |  0:03:26s\n",
      "epoch 48 | loss: 0.32441 | val_0_auc: 0.63338 |  0:03:31s\n",
      "epoch 49 | loss: 0.32463 | val_0_auc: 0.63136 |  0:03:36s\n",
      "epoch 50 | loss: 0.32407 | val_0_auc: 0.63463 |  0:03:41s\n",
      "epoch 51 | loss: 0.32571 | val_0_auc: 0.63931 |  0:03:46s\n",
      "epoch 52 | loss: 0.32416 | val_0_auc: 0.63556 |  0:03:51s\n",
      "epoch 53 | loss: 0.32454 | val_0_auc: 0.63739 |  0:03:55s\n",
      "epoch 54 | loss: 0.32187 | val_0_auc: 0.63844 |  0:03:59s\n",
      "epoch 55 | loss: 0.31913 | val_0_auc: 0.64382 |  0:04:04s\n",
      "epoch 56 | loss: 0.32007 | val_0_auc: 0.6426  |  0:04:08s\n",
      "epoch 57 | loss: 0.31753 | val_0_auc: 0.6389  |  0:04:12s\n",
      "epoch 58 | loss: 0.32117 | val_0_auc: 0.63677 |  0:04:16s\n",
      "epoch 59 | loss: 0.31977 | val_0_auc: 0.63824 |  0:04:20s\n",
      "epoch 60 | loss: 0.31956 | val_0_auc: 0.64034 |  0:04:24s\n",
      "epoch 61 | loss: 0.31987 | val_0_auc: 0.64048 |  0:04:28s\n",
      "epoch 62 | loss: 0.32101 | val_0_auc: 0.64247 |  0:04:32s\n",
      "epoch 63 | loss: 0.32174 | val_0_auc: 0.63905 |  0:04:36s\n",
      "epoch 64 | loss: 0.32161 | val_0_auc: 0.63852 |  0:04:40s\n",
      "epoch 65 | loss: 0.31759 | val_0_auc: 0.63636 |  0:04:44s\n",
      "epoch 66 | loss: 0.31488 | val_0_auc: 0.6383  |  0:04:48s\n",
      "epoch 67 | loss: 0.31795 | val_0_auc: 0.63848 |  0:04:53s\n",
      "epoch 68 | loss: 0.31339 | val_0_auc: 0.63952 |  0:04:58s\n",
      "epoch 69 | loss: 0.31421 | val_0_auc: 0.63996 |  0:05:03s\n",
      "epoch 70 | loss: 0.31502 | val_0_auc: 0.64376 |  0:05:07s\n",
      "epoch 71 | loss: 0.31444 | val_0_auc: 0.64409 |  0:05:12s\n",
      "epoch 72 | loss: 0.31304 | val_0_auc: 0.64443 |  0:05:16s\n",
      "epoch 73 | loss: 0.31437 | val_0_auc: 0.64633 |  0:05:20s\n",
      "epoch 74 | loss: 0.31032 | val_0_auc: 0.6485  |  0:05:25s\n",
      "epoch 75 | loss: 0.31404 | val_0_auc: 0.647   |  0:05:29s\n",
      "epoch 76 | loss: 0.31378 | val_0_auc: 0.64462 |  0:05:33s\n",
      "epoch 77 | loss: 0.31204 | val_0_auc: 0.64235 |  0:05:37s\n",
      "epoch 78 | loss: 0.31083 | val_0_auc: 0.63694 |  0:05:41s\n",
      "epoch 79 | loss: 0.31207 | val_0_auc: 0.63961 |  0:05:45s\n",
      "epoch 80 | loss: 0.31343 | val_0_auc: 0.63758 |  0:05:50s\n",
      "epoch 81 | loss: 0.31207 | val_0_auc: 0.63808 |  0:05:54s\n",
      "epoch 82 | loss: 0.31258 | val_0_auc: 0.63698 |  0:05:58s\n",
      "epoch 83 | loss: 0.31035 | val_0_auc: 0.64073 |  0:06:02s\n",
      "epoch 84 | loss: 0.31039 | val_0_auc: 0.63833 |  0:06:06s\n",
      "epoch 85 | loss: 0.31054 | val_0_auc: 0.64059 |  0:06:11s\n",
      "epoch 86 | loss: 0.30915 | val_0_auc: 0.6439  |  0:06:15s\n",
      "epoch 87 | loss: 0.31082 | val_0_auc: 0.64852 |  0:06:19s\n",
      "epoch 88 | loss: 0.30864 | val_0_auc: 0.64871 |  0:06:24s\n",
      "epoch 89 | loss: 0.30443 | val_0_auc: 0.64783 |  0:06:29s\n",
      "epoch 90 | loss: 0.30812 | val_0_auc: 0.64604 |  0:06:34s\n",
      "epoch 91 | loss: 0.30766 | val_0_auc: 0.64447 |  0:06:38s\n",
      "epoch 92 | loss: 0.30675 | val_0_auc: 0.64235 |  0:06:42s\n",
      "epoch 93 | loss: 0.30558 | val_0_auc: 0.64105 |  0:06:46s\n",
      "epoch 94 | loss: 0.30721 | val_0_auc: 0.64389 |  0:06:51s\n",
      "epoch 95 | loss: 0.30721 | val_0_auc: 0.63897 |  0:06:56s\n",
      "epoch 96 | loss: 0.30448 | val_0_auc: 0.63873 |  0:07:00s\n",
      "epoch 97 | loss: 0.30356 | val_0_auc: 0.63798 |  0:07:03s\n",
      "epoch 98 | loss: 0.30371 | val_0_auc: 0.64007 |  0:07:08s\n",
      "epoch 99 | loss: 0.30241 | val_0_auc: 0.63908 |  0:07:12s\n",
      "epoch 100| loss: 0.30122 | val_0_auc: 0.63956 |  0:07:16s\n",
      "epoch 101| loss: 0.30212 | val_0_auc: 0.63907 |  0:07:21s\n",
      "epoch 102| loss: 0.30175 | val_0_auc: 0.63877 |  0:07:25s\n",
      "epoch 103| loss: 0.30082 | val_0_auc: 0.63681 |  0:07:29s\n",
      "epoch 104| loss: 0.29838 | val_0_auc: 0.6358  |  0:07:34s\n",
      "epoch 105| loss: 0.29708 | val_0_auc: 0.64076 |  0:07:38s\n",
      "epoch 106| loss: 0.29929 | val_0_auc: 0.64317 |  0:07:43s\n",
      "epoch 107| loss: 0.29742 | val_0_auc: 0.64491 |  0:07:47s\n",
      "epoch 108| loss: 0.30072 | val_0_auc: 0.64555 |  0:07:52s\n",
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 88 and best_val_0_auc = 0.64871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "final_model = TabNetClassifier(\n",
    "    n_d=best_params[\"n_d\"],\n",
    "    n_a=best_params[\"n_a\"],\n",
    "    n_steps=best_params[\"n_steps\"],\n",
    "    gamma=best_params[\"gamma\"],\n",
    "    lambda_sparse=best_params[\"lambda_sparse\"],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_params[\"lr\"]),\n",
    "    mask_type=\"sparsemax\",\n",
    ")\n",
    "\n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=[\"auc\"],\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=2048,\n",
    "    virtual_batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f7df6",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e354efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "from sklearn.metrics import roc_curve, classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae61a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94      8164\n",
      "           1       0.21      0.01      0.01       983\n",
      "\n",
      "    accuracy                           0.89      9147\n",
      "   macro avg       0.55      0.50      0.48      9147\n",
      "weighted avg       0.82      0.89      0.84      9147\n",
      "\n",
      "ROC-AUC: 0.6696\n",
      "95% CI = [0.6519, 0.6862]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"601.9375pt\" height=\"392.684375pt\" viewBox=\"0 0 601.9375 392.684375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-12-03T03:25:21.154880</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 392.684375 \n",
       "L 601.9375 392.684375 \n",
       "L 601.9375 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 148.3375 355.021875 \n",
       "L 594.7375 355.021875 \n",
       "L 594.7375 22.381875 \n",
       "L 148.3375 22.381875 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 148.3375 24.045075 \n",
       "L 573.480357 24.045075 \n",
       "L 573.480357 37.350675 \n",
       "L 148.3375 37.350675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 148.3375 40.677075 \n",
       "L 330.552151 40.677075 \n",
       "L 330.552151 53.982675 \n",
       "L 148.3375 53.982675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 148.3375 57.309075 \n",
       "L 327.016329 57.309075 \n",
       "L 327.016329 70.614675 \n",
       "L 148.3375 70.614675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 148.3375 73.941075 \n",
       "L 324.588472 73.941075 \n",
       "L 324.588472 87.246675 \n",
       "L 148.3375 87.246675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 148.3375 90.573075 \n",
       "L 312.898092 90.573075 \n",
       "L 312.898092 103.878675 \n",
       "L 148.3375 103.878675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 148.3375 107.205075 \n",
       "L 305.559836 107.205075 \n",
       "L 305.559836 120.510675 \n",
       "L 148.3375 120.510675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 148.3375 123.837075 \n",
       "L 294.795261 123.837075 \n",
       "L 294.795261 137.142675 \n",
       "L 148.3375 137.142675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 148.3375 140.469075 \n",
       "L 291.323207 140.469075 \n",
       "L 291.323207 153.774675 \n",
       "L 148.3375 153.774675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 148.3375 157.101075 \n",
       "L 285.87614 157.101075 \n",
       "L 285.87614 170.406675 \n",
       "L 148.3375 170.406675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 148.3375 173.733075 \n",
       "L 278.779443 173.733075 \n",
       "L 278.779443 187.038675 \n",
       "L 148.3375 187.038675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 148.3375 190.365075 \n",
       "L 271.536845 190.365075 \n",
       "L 271.536845 203.670675 \n",
       "L 148.3375 203.670675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 148.3375 206.997075 \n",
       "L 267.278397 206.997075 \n",
       "L 267.278397 220.302675 \n",
       "L 148.3375 220.302675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 148.3375 223.629075 \n",
       "L 264.944506 223.629075 \n",
       "L 264.944506 236.934675 \n",
       "L 148.3375 236.934675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 148.3375 240.261075 \n",
       "L 263.960203 240.261075 \n",
       "L 263.960203 253.566675 \n",
       "L 148.3375 253.566675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 148.3375 256.893075 \n",
       "L 257.251437 256.893075 \n",
       "L 257.251437 270.198675 \n",
       "L 148.3375 270.198675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 148.3375 273.525075 \n",
       "L 254.238553 273.525075 \n",
       "L 254.238553 286.830675 \n",
       "L 148.3375 286.830675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 148.3375 290.157075 \n",
       "L 241.627811 290.157075 \n",
       "L 241.627811 303.462675 \n",
       "L 148.3375 303.462675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 148.3375 306.789075 \n",
       "L 239.760211 306.789075 \n",
       "L 239.760211 320.094675 \n",
       "L 148.3375 320.094675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 148.3375 323.421075 \n",
       "L 238.899456 323.421075 \n",
       "L 238.899456 336.726675 \n",
       "L 148.3375 336.726675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 148.3375 340.053075 \n",
       "L 236.0617 340.053075 \n",
       "L 236.0617 353.358675 \n",
       "L 148.3375 353.358675 \n",
       "z\n",
       "\" clip-path=\"url(#p9d8a050f2e)\" style=\"fill: #107010\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mdf5e46c85e\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"148.3375\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(136.296875 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-30\" d=\"M 1509 2344 \n",
       "Q 1509 2516 1629 2641 \n",
       "Q 1750 2766 1919 2766 \n",
       "Q 2094 2766 2219 2641 \n",
       "Q 2344 2516 2344 2344 \n",
       "Q 2344 2169 2220 2047 \n",
       "Q 2097 1925 1919 1925 \n",
       "Q 1744 1925 1626 2044 \n",
       "Q 1509 2163 1509 2344 \n",
       "z\n",
       "M 1925 4250 \n",
       "Q 1484 4250 1267 3775 \n",
       "Q 1050 3300 1050 2328 \n",
       "Q 1050 1359 1267 884 \n",
       "Q 1484 409 1925 409 \n",
       "Q 2369 409 2586 884 \n",
       "Q 2803 1359 2803 2328 \n",
       "Q 2803 3300 2586 3775 \n",
       "Q 2369 4250 1925 4250 \n",
       "z\n",
       "M 1925 4750 \n",
       "Q 2672 4750 3055 4137 \n",
       "Q 3438 3525 3438 2328 \n",
       "Q 3438 1134 3055 521 \n",
       "Q 2672 -91 1925 -91 \n",
       "Q 1178 -91 797 521 \n",
       "Q 416 1134 416 2328 \n",
       "Q 416 3525 797 4137 \n",
       "Q 1178 4750 1925 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSansMono-2e\" d=\"M 1528 953 \n",
       "L 2316 953 \n",
       "L 2316 0 \n",
       "L 1528 0 \n",
       "L 1528 953 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"201.738969\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.01 -->\n",
       "      <g transform=\"translate(189.698344 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-31\" d=\"M 844 531 \n",
       "L 1825 531 \n",
       "L 1825 4097 \n",
       "L 769 3859 \n",
       "L 769 4434 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3419 531 \n",
       "L 3419 0 \n",
       "L 844 0 \n",
       "L 844 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-31\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"255.140437\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.02 -->\n",
       "      <g transform=\"translate(243.099812 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-32\" d=\"M 1166 531 \n",
       "L 3309 531 \n",
       "L 3309 0 \n",
       "L 475 0 \n",
       "L 475 531 \n",
       "Q 1059 1147 1496 1619 \n",
       "Q 1934 2091 2100 2284 \n",
       "Q 2413 2666 2522 2902 \n",
       "Q 2631 3138 2631 3384 \n",
       "Q 2631 3775 2401 3997 \n",
       "Q 2172 4219 1772 4219 \n",
       "Q 1488 4219 1175 4116 \n",
       "Q 863 4013 513 3803 \n",
       "L 513 4441 \n",
       "Q 834 4594 1145 4672 \n",
       "Q 1456 4750 1759 4750 \n",
       "Q 2444 4750 2861 4386 \n",
       "Q 3278 4022 3278 3431 \n",
       "Q 3278 3131 3139 2831 \n",
       "Q 3000 2531 2688 2169 \n",
       "Q 2513 1966 2180 1606 \n",
       "Q 1847 1247 1166 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-32\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"308.541906\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.03 -->\n",
       "      <g transform=\"translate(296.501281 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-33\" d=\"M 2425 2497 \n",
       "Q 2884 2375 3128 2064 \n",
       "Q 3372 1753 3372 1288 \n",
       "Q 3372 644 2939 276 \n",
       "Q 2506 -91 1741 -91 \n",
       "Q 1419 -91 1084 -31 \n",
       "Q 750 28 428 141 \n",
       "L 428 769 \n",
       "Q 747 603 1056 522 \n",
       "Q 1366 441 1672 441 \n",
       "Q 2191 441 2469 675 \n",
       "Q 2747 909 2747 1350 \n",
       "Q 2747 1756 2469 1995 \n",
       "Q 2191 2234 1716 2234 \n",
       "L 1234 2234 \n",
       "L 1234 2753 \n",
       "L 1716 2753 \n",
       "Q 2150 2753 2394 2943 \n",
       "Q 2638 3134 2638 3475 \n",
       "Q 2638 3834 2411 4026 \n",
       "Q 2184 4219 1766 4219 \n",
       "Q 1488 4219 1191 4156 \n",
       "Q 894 4094 569 3969 \n",
       "L 569 4550 \n",
       "Q 947 4650 1242 4700 \n",
       "Q 1538 4750 1766 4750 \n",
       "Q 2447 4750 2855 4408 \n",
       "Q 3263 4066 3263 3500 \n",
       "Q 3263 3116 3048 2859 \n",
       "Q 2834 2603 2425 2497 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-33\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"361.943374\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.04 -->\n",
       "      <g transform=\"translate(349.902749 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-34\" d=\"M 2297 4091 \n",
       "L 825 1625 \n",
       "L 2297 1625 \n",
       "L 2297 4091 \n",
       "z\n",
       "M 2194 4666 \n",
       "L 2925 4666 \n",
       "L 2925 1625 \n",
       "L 3547 1625 \n",
       "L 3547 1113 \n",
       "L 2925 1113 \n",
       "L 2925 0 \n",
       "L 2297 0 \n",
       "L 2297 1113 \n",
       "L 319 1113 \n",
       "L 319 1709 \n",
       "L 2194 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-34\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"415.344843\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(403.304218 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-35\" d=\"M 647 4666 \n",
       "L 3009 4666 \n",
       "L 3009 4134 \n",
       "L 1222 4134 \n",
       "L 1222 2988 \n",
       "Q 1356 3038 1492 3061 \n",
       "Q 1628 3084 1766 3084 \n",
       "Q 2491 3084 2916 2656 \n",
       "Q 3341 2228 3341 1497 \n",
       "Q 3341 759 2895 334 \n",
       "Q 2450 -91 1678 -91 \n",
       "Q 1306 -91 998 -41 \n",
       "Q 691 9 447 109 \n",
       "L 447 750 \n",
       "Q 734 594 1025 517 \n",
       "Q 1316 441 1619 441 \n",
       "Q 2141 441 2423 716 \n",
       "Q 2706 991 2706 1497 \n",
       "Q 2706 1997 2414 2275 \n",
       "Q 2122 2553 1600 2553 \n",
       "Q 1347 2553 1106 2495 \n",
       "Q 866 2438 647 2322 \n",
       "L 647 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-35\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"468.746311\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.06 -->\n",
       "      <g transform=\"translate(456.705686 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-36\" d=\"M 3097 4563 \n",
       "L 3097 3981 \n",
       "Q 2900 4097 2678 4158 \n",
       "Q 2456 4219 2216 4219 \n",
       "Q 1616 4219 1306 3767 \n",
       "Q 997 3316 997 2438 \n",
       "Q 1147 2750 1412 2917 \n",
       "Q 1678 3084 2022 3084 \n",
       "Q 2697 3084 3067 2670 \n",
       "Q 3438 2256 3438 1497 \n",
       "Q 3438 741 3056 325 \n",
       "Q 2675 -91 1984 -91 \n",
       "Q 1172 -91 794 492 \n",
       "Q 416 1075 416 2328 \n",
       "Q 416 3509 870 4129 \n",
       "Q 1325 4750 2188 4750 \n",
       "Q 2419 4750 2650 4701 \n",
       "Q 2881 4653 3097 4563 \n",
       "z\n",
       "M 1972 2591 \n",
       "Q 1569 2591 1337 2300 \n",
       "Q 1106 2009 1106 1497 \n",
       "Q 1106 984 1337 693 \n",
       "Q 1569 403 1972 403 \n",
       "Q 2391 403 2603 679 \n",
       "Q 2816 956 2816 1497 \n",
       "Q 2816 2041 2603 2316 \n",
       "Q 2391 2591 1972 2591 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-36\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"522.14778\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.07 -->\n",
       "      <g transform=\"translate(510.107155 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-37\" d=\"M 434 4666 \n",
       "L 3372 4666 \n",
       "L 3372 4397 \n",
       "L 1703 0 \n",
       "L 1044 0 \n",
       "L 2669 4134 \n",
       "L 434 4134 \n",
       "L 434 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-37\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf5e46c85e\" x=\"575.549248\" y=\"355.021875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.08 -->\n",
       "      <g transform=\"translate(563.508623 369.673438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-38\" d=\"M 1925 2216 \n",
       "Q 1503 2216 1273 1980 \n",
       "Q 1044 1744 1044 1313 \n",
       "Q 1044 881 1276 642 \n",
       "Q 1509 403 1925 403 \n",
       "Q 2350 403 2579 639 \n",
       "Q 2809 875 2809 1313 \n",
       "Q 2809 1741 2576 1978 \n",
       "Q 2344 2216 1925 2216 \n",
       "z\n",
       "M 1375 2478 \n",
       "Q 972 2581 745 2862 \n",
       "Q 519 3144 519 3541 \n",
       "Q 519 4097 897 4423 \n",
       "Q 1275 4750 1925 4750 \n",
       "Q 2578 4750 2956 4423 \n",
       "Q 3334 4097 3334 3541 \n",
       "Q 3334 3144 3107 2862 \n",
       "Q 2881 2581 2478 2478 \n",
       "Q 2947 2375 3195 2062 \n",
       "Q 3444 1750 3444 1253 \n",
       "Q 3444 622 3041 265 \n",
       "Q 2638 -91 1925 -91 \n",
       "Q 1213 -91 811 264 \n",
       "Q 409 619 409 1247 \n",
       "Q 409 1747 657 2061 \n",
       "Q 906 2375 1375 2478 \n",
       "z\n",
       "M 1147 3481 \n",
       "Q 1147 3106 1347 2909 \n",
       "Q 1547 2713 1925 2713 \n",
       "Q 2306 2713 2506 2909 \n",
       "Q 2706 3106 2706 3481 \n",
       "Q 2706 3863 2507 4063 \n",
       "Q 2309 4263 1925 4263 \n",
       "Q 1547 4263 1347 4061 \n",
       "Q 1147 3859 1147 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-2e\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-38\" transform=\"translate(180.615234 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- importance -->\n",
       "     <g transform=\"translate(341.435938 383.404688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSansMono-69\" d=\"M 800 3500 \n",
       "L 2272 3500 \n",
       "L 2272 447 \n",
       "L 3413 447 \n",
       "L 3413 0 \n",
       "L 556 0 \n",
       "L 556 447 \n",
       "L 1697 447 \n",
       "L 1697 3053 \n",
       "L 800 3053 \n",
       "L 800 3500 \n",
       "z\n",
       "M 1697 4863 \n",
       "L 2272 4863 \n",
       "L 2272 4134 \n",
       "L 1697 4134 \n",
       "L 1697 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-6d\" d=\"M 2113 3144 \n",
       "Q 2219 3369 2383 3476 \n",
       "Q 2547 3584 2778 3584 \n",
       "Q 3200 3584 3373 3257 \n",
       "Q 3547 2931 3547 2028 \n",
       "L 3547 0 \n",
       "L 3022 0 \n",
       "L 3022 2003 \n",
       "Q 3022 2744 2939 2923 \n",
       "Q 2856 3103 2638 3103 \n",
       "Q 2388 3103 2295 2911 \n",
       "Q 2203 2719 2203 2003 \n",
       "L 2203 0 \n",
       "L 1678 0 \n",
       "L 1678 2003 \n",
       "Q 1678 2753 1589 2928 \n",
       "Q 1500 3103 1269 3103 \n",
       "Q 1041 3103 952 2911 \n",
       "Q 863 2719 863 2003 \n",
       "L 863 0 \n",
       "L 341 0 \n",
       "L 341 3500 \n",
       "L 863 3500 \n",
       "L 863 3200 \n",
       "Q 966 3388 1120 3486 \n",
       "Q 1275 3584 1472 3584 \n",
       "Q 1709 3584 1867 3475 \n",
       "Q 2025 3366 2113 3144 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-70\" d=\"M 1172 441 \n",
       "L 1172 -1331 \n",
       "L 594 -1331 \n",
       "L 594 3500 \n",
       "L 1172 3500 \n",
       "L 1172 3053 \n",
       "Q 1316 3313 1555 3448 \n",
       "Q 1794 3584 2106 3584 \n",
       "Q 2741 3584 3102 3093 \n",
       "Q 3463 2603 3463 1734 \n",
       "Q 3463 881 3100 395 \n",
       "Q 2738 -91 2106 -91 \n",
       "Q 1788 -91 1548 45 \n",
       "Q 1309 181 1172 441 \n",
       "z\n",
       "M 2859 1747 \n",
       "Q 2859 2416 2648 2756 \n",
       "Q 2438 3097 2022 3097 \n",
       "Q 1603 3097 1387 2755 \n",
       "Q 1172 2413 1172 1747 \n",
       "Q 1172 1084 1387 740 \n",
       "Q 1603 397 2022 397 \n",
       "Q 2438 397 2648 737 \n",
       "Q 2859 1078 2859 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-6f\" d=\"M 1925 3097 \n",
       "Q 1488 3097 1263 2756 \n",
       "Q 1038 2416 1038 1747 \n",
       "Q 1038 1081 1263 739 \n",
       "Q 1488 397 1925 397 \n",
       "Q 2366 397 2591 739 \n",
       "Q 2816 1081 2816 1747 \n",
       "Q 2816 2416 2591 2756 \n",
       "Q 2366 3097 1925 3097 \n",
       "z\n",
       "M 1925 3584 \n",
       "Q 2653 3584 3039 3112 \n",
       "Q 3425 2641 3425 1747 \n",
       "Q 3425 850 3040 379 \n",
       "Q 2656 -91 1925 -91 \n",
       "Q 1197 -91 812 379 \n",
       "Q 428 850 428 1747 \n",
       "Q 428 2641 812 3112 \n",
       "Q 1197 3584 1925 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-72\" d=\"M 3609 2778 \n",
       "Q 3425 2922 3234 2987 \n",
       "Q 3044 3053 2816 3053 \n",
       "Q 2278 3053 1993 2715 \n",
       "Q 1709 2378 1709 1741 \n",
       "L 1709 0 \n",
       "L 1131 0 \n",
       "L 1131 3500 \n",
       "L 1709 3500 \n",
       "L 1709 2816 \n",
       "Q 1853 3188 2151 3386 \n",
       "Q 2450 3584 2859 3584 \n",
       "Q 3072 3584 3256 3531 \n",
       "Q 3441 3478 3609 3366 \n",
       "L 3609 2778 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-74\" d=\"M 1919 4494 \n",
       "L 1919 3500 \n",
       "L 3225 3500 \n",
       "L 3225 3053 \n",
       "L 1919 3053 \n",
       "L 1919 1153 \n",
       "Q 1919 766 2066 612 \n",
       "Q 2213 459 2578 459 \n",
       "L 3225 459 \n",
       "L 3225 0 \n",
       "L 2522 0 \n",
       "Q 1875 0 1609 259 \n",
       "Q 1344 519 1344 1153 \n",
       "L 1344 3053 \n",
       "L 409 3053 \n",
       "L 409 3500 \n",
       "L 1344 3500 \n",
       "L 1344 4494 \n",
       "L 1919 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-61\" d=\"M 2194 1759 \n",
       "L 2003 1759 \n",
       "Q 1500 1759 1245 1582 \n",
       "Q 991 1406 991 1056 \n",
       "Q 991 741 1181 566 \n",
       "Q 1372 391 1709 391 \n",
       "Q 2184 391 2456 720 \n",
       "Q 2728 1050 2731 1631 \n",
       "L 2731 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3309 1997 \n",
       "L 3309 0 \n",
       "L 2731 0 \n",
       "L 2731 519 \n",
       "Q 2547 206 2267 57 \n",
       "Q 1988 -91 1588 -91 \n",
       "Q 1053 -91 734 211 \n",
       "Q 416 513 416 1019 \n",
       "Q 416 1603 808 1906 \n",
       "Q 1200 2209 1959 2209 \n",
       "L 2731 2209 \n",
       "L 2731 2300 \n",
       "Q 2728 2719 2518 2908 \n",
       "Q 2309 3097 1850 3097 \n",
       "Q 1556 3097 1256 3012 \n",
       "Q 956 2928 672 2766 \n",
       "L 672 3341 \n",
       "Q 991 3463 1283 3523 \n",
       "Q 1575 3584 1850 3584 \n",
       "Q 2284 3584 2592 3456 \n",
       "Q 2900 3328 3091 3072 \n",
       "Q 3209 2916 3259 2686 \n",
       "Q 3309 2456 3309 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-6e\" d=\"M 3284 2169 \n",
       "L 3284 0 \n",
       "L 2706 0 \n",
       "L 2706 2169 \n",
       "Q 2706 2641 2540 2862 \n",
       "Q 2375 3084 2022 3084 \n",
       "Q 1619 3084 1401 2798 \n",
       "Q 1184 2513 1184 1978 \n",
       "L 1184 0 \n",
       "L 609 0 \n",
       "L 609 3500 \n",
       "L 1184 3500 \n",
       "L 1184 2975 \n",
       "Q 1338 3275 1600 3429 \n",
       "Q 1863 3584 2222 3584 \n",
       "Q 2756 3584 3020 3232 \n",
       "Q 3284 2881 3284 2169 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-63\" d=\"M 3316 178 \n",
       "Q 3084 44 2839 -23 \n",
       "Q 2594 -91 2338 -91 \n",
       "Q 1525 -91 1067 396 \n",
       "Q 609 884 609 1747 \n",
       "Q 609 2609 1067 3096 \n",
       "Q 1525 3584 2338 3584 \n",
       "Q 2591 3584 2831 3518 \n",
       "Q 3072 3453 3316 3316 \n",
       "L 3316 2713 \n",
       "Q 3088 2916 2858 3006 \n",
       "Q 2628 3097 2338 3097 \n",
       "Q 1797 3097 1506 2747 \n",
       "Q 1216 2397 1216 1747 \n",
       "Q 1216 1100 1508 748 \n",
       "Q 1800 397 2338 397 \n",
       "Q 2638 397 2875 489 \n",
       "Q 3113 581 3316 775 \n",
       "L 3316 178 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSansMono-65\" d=\"M 3475 1894 \n",
       "L 3475 1613 \n",
       "L 984 1613 \n",
       "L 984 1594 \n",
       "Q 984 1022 1282 709 \n",
       "Q 1581 397 2125 397 \n",
       "Q 2400 397 2700 484 \n",
       "Q 3000 572 3341 750 \n",
       "L 3341 178 \n",
       "Q 3013 44 2708 -23 \n",
       "Q 2403 -91 2119 -91 \n",
       "Q 1303 -91 843 398 \n",
       "Q 384 888 384 1747 \n",
       "Q 384 2584 834 3084 \n",
       "Q 1284 3584 2034 3584 \n",
       "Q 2703 3584 3089 3131 \n",
       "Q 3475 2678 3475 1894 \n",
       "z\n",
       "M 2900 2063 \n",
       "Q 2888 2569 2661 2833 \n",
       "Q 2434 3097 2009 3097 \n",
       "Q 1594 3097 1325 2822 \n",
       "Q 1056 2547 1006 2059 \n",
       "L 2900 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSansMono-69\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(60.205078 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-70\" transform=\"translate(120.410156 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(180.615234 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(240.820312 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(301.025391 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(361.230469 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(421.435547 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(481.640625 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(541.845703 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"m3963a04c2a\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"30.697875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- rdw_max -->\n",
       "      <g transform=\"translate(99.195312 34.497094) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-64\" d=\"M 2681 3053 \n",
       "L 2681 4863 \n",
       "L 3256 4863 \n",
       "L 3256 0 \n",
       "L 2681 0 \n",
       "L 2681 441 \n",
       "Q 2538 181 2298 45 \n",
       "Q 2059 -91 1747 -91 \n",
       "Q 1113 -91 748 401 \n",
       "Q 384 894 384 1759 \n",
       "Q 384 2613 750 3098 \n",
       "Q 1116 3584 1747 3584 \n",
       "Q 2063 3584 2303 3448 \n",
       "Q 2544 3313 2681 3053 \n",
       "z\n",
       "M 991 1747 \n",
       "Q 991 1078 1203 737 \n",
       "Q 1416 397 1831 397 \n",
       "Q 2247 397 2464 740 \n",
       "Q 2681 1084 2681 1747 \n",
       "Q 2681 2413 2464 2755 \n",
       "Q 2247 3097 1831 3097 \n",
       "Q 1416 3097 1203 2756 \n",
       "Q 991 2416 991 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSansMono-77\" d=\"M 0 3500 \n",
       "L 569 3500 \n",
       "L 1178 672 \n",
       "L 1678 2478 \n",
       "L 2169 2478 \n",
       "L 2675 672 \n",
       "L 3284 3500 \n",
       "L 3853 3500 \n",
       "L 3034 0 \n",
       "L 2484 0 \n",
       "L 1925 1919 \n",
       "L 1369 0 \n",
       "L 819 0 \n",
       "L 0 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSansMono-5f\" d=\"M 3853 -1259 \n",
       "L 3853 -1509 \n",
       "L 0 -1509 \n",
       "L 0 -1259 \n",
       "L 3853 -1259 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSansMono-78\" d=\"M 3494 3500 \n",
       "L 2241 1825 \n",
       "L 3616 0 \n",
       "L 2950 0 \n",
       "L 1925 1403 \n",
       "L 903 0 \n",
       "L 238 0 \n",
       "L 1613 1825 \n",
       "L 359 3500 \n",
       "L 997 3500 \n",
       "L 1925 2234 \n",
       "L 2847 3500 \n",
       "L 3494 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-77\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-78\" transform=\"translate(361.230469 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"47.329875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- mcv_max -->\n",
       "      <g transform=\"translate(99.195312 51.016594) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-76\" d=\"M 313 3500 \n",
       "L 909 3500 \n",
       "L 1925 563 \n",
       "L 2944 3500 \n",
       "L 3541 3500 \n",
       "L 2297 0 \n",
       "L 1556 0 \n",
       "L 313 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-76\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-78\" transform=\"translate(361.230469 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"63.961875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- hematocrit_mean -->\n",
       "      <g transform=\"translate(51.032813 67.761094) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-68\" d=\"M 3284 2169 \n",
       "L 3284 0 \n",
       "L 2706 0 \n",
       "L 2706 2169 \n",
       "Q 2706 2641 2540 2862 \n",
       "Q 2375 3084 2022 3084 \n",
       "Q 1619 3084 1401 2798 \n",
       "Q 1184 2513 1184 1978 \n",
       "L 1184 0 \n",
       "L 609 0 \n",
       "L 609 4863 \n",
       "L 1184 4863 \n",
       "L 1184 2975 \n",
       "Q 1338 3275 1600 3429 \n",
       "Q 1863 3584 2222 3584 \n",
       "Q 2756 3584 3020 3232 \n",
       "Q 3284 2881 3284 2169 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-68\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(782.666016 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(842.871094 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"80.593875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- mcv_mean -->\n",
       "      <g transform=\"translate(93.175 84.280594) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-76\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(421.435547 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"97.225875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- bicarbonate_max -->\n",
       "      <g transform=\"translate(51.032813 101.025094) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-62\" d=\"M 2869 1747 \n",
       "Q 2869 2416 2656 2756 \n",
       "Q 2444 3097 2028 3097 \n",
       "Q 1609 3097 1393 2755 \n",
       "Q 1178 2413 1178 1747 \n",
       "Q 1178 1084 1393 740 \n",
       "Q 1609 397 2028 397 \n",
       "Q 2444 397 2656 737 \n",
       "Q 2869 1078 2869 1747 \n",
       "z\n",
       "M 1178 3053 \n",
       "Q 1316 3309 1558 3446 \n",
       "Q 1800 3584 2119 3584 \n",
       "Q 2750 3584 3112 3098 \n",
       "Q 3475 2613 3475 1759 \n",
       "Q 3475 894 3111 401 \n",
       "Q 2747 -91 2113 -91 \n",
       "Q 1800 -91 1561 45 \n",
       "Q 1322 181 1178 441 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 3053 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-62\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-62\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(782.666016 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-78\" transform=\"translate(842.871094 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"113.857875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- glucose_mean -->\n",
       "      <g transform=\"translate(69.09375 117.683656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSansMono-67\" d=\"M 2681 1778 \n",
       "Q 2681 2425 2470 2761 \n",
       "Q 2259 3097 1856 3097 \n",
       "Q 1434 3097 1212 2761 \n",
       "Q 991 2425 991 1778 \n",
       "Q 991 1131 1214 792 \n",
       "Q 1438 453 1863 453 \n",
       "Q 2259 453 2470 793 \n",
       "Q 2681 1134 2681 1778 \n",
       "z\n",
       "M 3256 225 \n",
       "Q 3256 -563 2884 -969 \n",
       "Q 2513 -1375 1791 -1375 \n",
       "Q 1553 -1375 1293 -1331 \n",
       "Q 1034 -1288 775 -1203 \n",
       "L 775 -634 \n",
       "Q 1081 -778 1331 -847 \n",
       "Q 1581 -916 1791 -916 \n",
       "Q 2256 -916 2468 -662 \n",
       "Q 2681 -409 2681 141 \n",
       "L 2681 166 \n",
       "L 2681 556 \n",
       "Q 2544 263 2306 119 \n",
       "Q 2069 -25 1728 -25 \n",
       "Q 1116 -25 750 465 \n",
       "Q 384 956 384 1778 \n",
       "Q 384 2603 750 3093 \n",
       "Q 1116 3584 1728 3584 \n",
       "Q 2066 3584 2300 3450 \n",
       "Q 2534 3316 2681 3034 \n",
       "L 2681 3488 \n",
       "L 3256 3488 \n",
       "L 3256 225 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSansMono-6c\" d=\"M 1997 1269 \n",
       "Q 1997 881 2139 684 \n",
       "Q 2281 488 2559 488 \n",
       "L 3231 488 \n",
       "L 3231 0 \n",
       "L 2503 0 \n",
       "Q 1988 0 1705 331 \n",
       "Q 1422 663 1422 1269 \n",
       "L 1422 4447 \n",
       "L 500 4447 \n",
       "L 500 4897 \n",
       "L 1997 4897 \n",
       "L 1997 1269 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSansMono-75\" d=\"M 609 1325 \n",
       "L 609 3494 \n",
       "L 1184 3494 \n",
       "L 1184 1325 \n",
       "Q 1184 853 1351 631 \n",
       "Q 1519 409 1869 409 \n",
       "Q 2275 409 2490 695 \n",
       "Q 2706 981 2706 1516 \n",
       "L 2706 3494 \n",
       "L 3284 3494 \n",
       "L 3284 0 \n",
       "L 2706 0 \n",
       "L 2706 525 \n",
       "Q 2553 222 2289 65 \n",
       "Q 2025 -91 1672 -91 \n",
       "Q 1134 -91 871 261 \n",
       "Q 609 613 609 1325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSansMono-73\" d=\"M 3041 3378 \n",
       "L 3041 2816 \n",
       "Q 2794 2959 2544 3031 \n",
       "Q 2294 3103 2034 3103 \n",
       "Q 1644 3103 1451 2976 \n",
       "Q 1259 2850 1259 2591 \n",
       "Q 1259 2356 1403 2240 \n",
       "Q 1547 2125 2119 2016 \n",
       "L 2350 1972 \n",
       "Q 2778 1891 2998 1647 \n",
       "Q 3219 1403 3219 1013 \n",
       "Q 3219 494 2850 201 \n",
       "Q 2481 -91 1825 -91 \n",
       "Q 1566 -91 1281 -36 \n",
       "Q 997 19 666 128 \n",
       "L 666 722 \n",
       "Q 988 556 1281 473 \n",
       "Q 1575 391 1838 391 \n",
       "Q 2219 391 2428 545 \n",
       "Q 2638 700 2638 978 \n",
       "Q 2638 1378 1872 1531 \n",
       "L 1847 1538 \n",
       "L 1631 1581 \n",
       "Q 1134 1678 906 1908 \n",
       "Q 678 2138 678 2534 \n",
       "Q 678 3038 1018 3311 \n",
       "Q 1359 3584 1991 3584 \n",
       "Q 2272 3584 2531 3532 \n",
       "Q 2791 3481 3041 3378 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSansMono-67\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-75\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-73\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(662.255859 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"130.489875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- creatinine_mean -->\n",
       "      <g transform=\"translate(51.032813 134.289094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(782.666016 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(842.871094 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"147.121875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- hemoglobin_mean -->\n",
       "      <g transform=\"translate(51.032813 150.947656) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-68\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-67\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-62\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(782.666016 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(842.871094 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"163.753875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- rdw_sd -->\n",
       "      <g transform=\"translate(105.215625 167.553094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-77\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-73\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(301.025391 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"180.385875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- calcium_total_sd -->\n",
       "      <g transform=\"translate(45.0125 184.211656) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-75\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(782.666016 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-73\" transform=\"translate(842.871094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(903.076172 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"197.017875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- urea_nitrogen_mean -->\n",
       "      <g transform=\"translate(32.971875 200.817094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-75\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-67\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(782.666016 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(842.871094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(903.076172 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(963.28125 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(1023.486328 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"213.649875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- hematocrit_min -->\n",
       "      <g transform=\"translate(57.053125 217.449094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-68\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(782.666016 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"230.281875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- chloride_min -->\n",
       "      <g transform=\"translate(69.09375 234.107656) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-68\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(662.255859 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"246.913875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- hematocrit_max -->\n",
       "      <g transform=\"translate(57.053125 250.713094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-68\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-78\" transform=\"translate(782.666016 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"263.545875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- mch_min -->\n",
       "      <g transform=\"translate(99.195312 267.345094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-68\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(361.230469 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"280.177875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- rdw_min -->\n",
       "      <g transform=\"translate(99.195312 283.977094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-77\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(361.230469 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_17\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"296.809875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- platelet_count_min -->\n",
       "      <g transform=\"translate(32.971875 300.635656) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-70\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-75\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(782.666016 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(842.871094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(903.076172 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-69\" transform=\"translate(963.28125 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(1023.486328 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_18\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"313.441875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- mch_sd -->\n",
       "      <g transform=\"translate(105.215625 317.241094) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-68\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-73\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(301.025391 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_19\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"330.073875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- red_blood_cells_mean -->\n",
       "      <g transform=\"translate(20.93125 333.899656) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-72\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-62\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(541.845703 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(602.050781 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(662.255859 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(722.460938 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(782.666016 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-73\" transform=\"translate(842.871094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(903.076172 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(963.28125 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(1023.486328 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(1083.691406 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(1143.896484 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_20\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3963a04c2a\" x=\"148.3375\" y=\"346.705875\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- glucose_sd -->\n",
       "      <g transform=\"translate(81.134375 350.531656) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSansMono-67\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6c\" transform=\"translate(60.205078 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-75\" transform=\"translate(120.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(180.615234 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(240.820312 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-73\" transform=\"translate(301.025391 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(361.230469 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-5f\" transform=\"translate(421.435547 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-73\" transform=\"translate(481.640625 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSansMono-64\" transform=\"translate(541.845703 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_31\">\n",
       "     <!-- feature -->\n",
       "     <g transform=\"translate(14.851563 209.772969) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSansMono-66\" d=\"M 3322 4863 \n",
       "L 3322 4384 \n",
       "L 2669 4384 \n",
       "Q 2359 4384 2239 4257 \n",
       "Q 2119 4131 2119 3809 \n",
       "L 2119 3500 \n",
       "L 3322 3500 \n",
       "L 3322 3053 \n",
       "L 2119 3053 \n",
       "L 2119 0 \n",
       "L 1544 0 \n",
       "L 1544 3053 \n",
       "L 609 3053 \n",
       "L 609 3500 \n",
       "L 1544 3500 \n",
       "L 1544 3744 \n",
       "Q 1544 4319 1808 4591 \n",
       "Q 2072 4863 2631 4863 \n",
       "L 3322 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSansMono-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(60.205078 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(120.410156 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(180.615234 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-75\" transform=\"translate(240.820312 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(301.025391 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(361.230469 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_30\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_31\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_32\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_33\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_34\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_35\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_36\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_37\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_38\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_39\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_40\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_41\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_42\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_43\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_44\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_45\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_46\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_47\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_48\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_49\">\n",
       "    <path clip-path=\"url(#p9d8a050f2e)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 148.3375 355.021875 \n",
       "L 148.3375 22.381875 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 594.7375 355.021875 \n",
       "L 594.7375 22.381875 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_25\">\n",
       "    <path d=\"M 148.3375 355.021875 \n",
       "L 594.7375 355.021875 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_26\">\n",
       "    <path d=\"M 148.3375 22.381875 \n",
       "L 594.7375 22.381875 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_32\">\n",
       "    <!-- Top 20 Feature Importances -->\n",
       "    <g transform=\"translate(277.620625 16.381875) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSansMono-54\" d=\"M 147 4666 \n",
       "L 3706 4666 \n",
       "L 3706 4134 \n",
       "L 2247 4134 \n",
       "L 2247 0 \n",
       "L 1613 0 \n",
       "L 1613 4134 \n",
       "L 147 4134 \n",
       "L 147 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSansMono-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSansMono-46\" d=\"M 728 4666 \n",
       "L 3475 4666 \n",
       "L 3475 4134 \n",
       "L 1363 4134 \n",
       "L 1363 2759 \n",
       "L 3278 2759 \n",
       "L 3278 2228 \n",
       "L 1363 2228 \n",
       "L 1363 0 \n",
       "L 728 0 \n",
       "L 728 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSansMono-49\" d=\"M 628 4666 \n",
       "L 3219 4666 \n",
       "L 3219 4134 \n",
       "L 2241 4134 \n",
       "L 2241 531 \n",
       "L 3219 531 \n",
       "L 3219 0 \n",
       "L 628 0 \n",
       "L 628 531 \n",
       "L 1606 531 \n",
       "L 1606 4134 \n",
       "L 628 4134 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSansMono-54\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(60.205078 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-70\" transform=\"translate(120.410156 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-20\" transform=\"translate(180.615234 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-32\" transform=\"translate(240.820312 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-30\" transform=\"translate(301.025391 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-20\" transform=\"translate(361.230469 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-46\" transform=\"translate(421.435547 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(481.640625 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(541.845703 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(602.050781 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-75\" transform=\"translate(662.255859 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(722.460938 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(782.666016 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-20\" transform=\"translate(842.871094 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-49\" transform=\"translate(903.076172 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-6d\" transform=\"translate(963.28125 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-70\" transform=\"translate(1023.486328 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-6f\" transform=\"translate(1083.691406 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-72\" transform=\"translate(1143.896484 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-74\" transform=\"translate(1204.101562 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-61\" transform=\"translate(1264.306641 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-6e\" transform=\"translate(1324.511719 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-63\" transform=\"translate(1384.716797 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-65\" transform=\"translate(1444.921875 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSansMono-73\" transform=\"translate(1505.126953 0)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p9d8a050f2e\">\n",
       "   <rect x=\"148.3375\" y=\"22.381875\" width=\"446.4\" height=\"332.64\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "y_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "np.savez(f'../results/tabnet_base_fpr_tpr_thresholds.npz', fpr, tpr, thresholds)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "_, lower, upper = bootstrap_auc_ci(y_test, y_proba)\n",
    "print(f\"95% CI = [{lower:.4f}, {upper:.4f}]\")\n",
    "\n",
    "importances = final_model.feature_importances_\n",
    "feat_imp = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "feat_imp = feat_imp.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y='feature', x='importance', data=feat_imp, color='green')\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960fa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

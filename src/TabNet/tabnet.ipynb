{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2416a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='svg'\n",
    "plt.rcParams.update({\n",
    "    'text.usetex':False,\n",
    "    'font.family':'monospace'\n",
    "})  \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77067961",
   "metadata": {},
   "source": [
    "Simple Bootstrapping method to get an confidence interval on the AUROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72c1591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc_ci(y_true, y_scores, n_bootstraps=2000, ci=0.95):\n",
    "    rng = np.random.default_rng(42)\n",
    "    aucs = []\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = rng.integers(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        aucs.append(roc_auc_score(y_true[idx], y_scores[idx]))\n",
    "\n",
    "    lower = np.percentile(aucs, (1 - ci) / 2 * 100)\n",
    "    upper = np.percentile(aucs, (1 + ci) / 2 * 100)\n",
    "    return np.mean(aucs), lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997f81a",
   "metadata": {},
   "source": [
    "## Loading the dataset, pre-processing, and analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8fefe0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>anion_gap_mean</th>\n",
       "      <th>anion_gap_sd</th>\n",
       "      <th>anion_gap_min</th>\n",
       "      <th>anion_gap_max</th>\n",
       "      <th>bicarbonate_mean</th>\n",
       "      <th>bicarbonate_sd</th>\n",
       "      <th>bicarbonate_min</th>\n",
       "      <th>bicarbonate_max</th>\n",
       "      <th>calcium_total_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>urea_nitrogen_min</th>\n",
       "      <th>urea_nitrogen_max</th>\n",
       "      <th>white_blood_cells_mean</th>\n",
       "      <th>white_blood_cells_sd</th>\n",
       "      <th>white_blood_cells_min</th>\n",
       "      <th>white_blood_cells_max</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>icu_los_hours</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200003</td>\n",
       "      <td>13.375000</td>\n",
       "      <td>3.583195</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>3.105295</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.771429</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.471429</td>\n",
       "      <td>13.176711</td>\n",
       "      <td>13.2</td>\n",
       "      <td>43.9</td>\n",
       "      <td>48</td>\n",
       "      <td>M</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200007</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>1.272792</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200009</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.471429</td>\n",
       "      <td>1.471637</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.3</td>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200014</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>2.203028</td>\n",
       "      <td>10.7</td>\n",
       "      <td>14.7</td>\n",
       "      <td>85</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30484</th>\n",
       "      <td>299992</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>2.856153</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.125000</td>\n",
       "      <td>2.609556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.307143</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.134783</td>\n",
       "      <td>3.781727</td>\n",
       "      <td>8.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>299993</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>1.341641</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>2.073644</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.605530</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>299994</td>\n",
       "      <td>16.157895</td>\n",
       "      <td>2.477973</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.631579</td>\n",
       "      <td>3.451417</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>10.076190</td>\n",
       "      <td>2.642329</td>\n",
       "      <td>5.3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>74</td>\n",
       "      <td>F</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>299998</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1.210372</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>87</td>\n",
       "      <td>M</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>299999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>3.394113</td>\n",
       "      <td>15.9</td>\n",
       "      <td>20.7</td>\n",
       "      <td>49</td>\n",
       "      <td>M</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30489 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       icustay_id  anion_gap_mean  anion_gap_sd  anion_gap_min  anion_gap_max  \\\n",
       "0          200003       13.375000      3.583195            9.0           21.0   \n",
       "1          200007       15.500000      2.121320           14.0           17.0   \n",
       "2          200009        9.500000      2.121320            8.0           11.0   \n",
       "3          200012             NaN           NaN            NaN            NaN   \n",
       "4          200014       10.000000      1.732051            9.0           12.0   \n",
       "...           ...             ...           ...            ...            ...   \n",
       "30484      299992       15.375000      2.856153           11.0           25.0   \n",
       "30485      299993        9.400000      1.341641            8.0           11.0   \n",
       "30486      299994       16.157895      2.477973           13.0           24.0   \n",
       "30487      299998       11.500000      1.732051           10.0           14.0   \n",
       "30488      299999             NaN           NaN            NaN            NaN   \n",
       "\n",
       "       bicarbonate_mean  bicarbonate_sd  bicarbonate_min  bicarbonate_max  \\\n",
       "0             25.250000        3.105295             18.0             28.0   \n",
       "1             23.000000        1.414214             22.0             24.0   \n",
       "2             23.333333        2.081666             21.0             25.0   \n",
       "3                   NaN             NaN              NaN              NaN   \n",
       "4             24.000000        1.000000             23.0             25.0   \n",
       "...                 ...             ...              ...              ...   \n",
       "30484         23.125000        2.609556             15.0             26.0   \n",
       "30485         29.600000        2.073644             26.0             31.0   \n",
       "30486         21.631579        3.451417             17.0             31.0   \n",
       "30487         23.500000        1.290994             22.0             25.0   \n",
       "30488         24.000000             NaN             24.0             24.0   \n",
       "\n",
       "       calcium_total_mean  ...  urea_nitrogen_min  urea_nitrogen_max  \\\n",
       "0                7.771429  ...               10.0               21.0   \n",
       "1                8.900000  ...                8.0               10.0   \n",
       "2                8.000000  ...               15.0               21.0   \n",
       "3                     NaN  ...                NaN                NaN   \n",
       "4                7.733333  ...               21.0               24.0   \n",
       "...                   ...  ...                ...                ...   \n",
       "30484            8.307143  ...                8.0               23.0   \n",
       "30485            8.000000  ...               12.0               15.0   \n",
       "30486            8.100000  ...               28.0               63.0   \n",
       "30487            8.800000  ...               20.0               22.0   \n",
       "30488                 NaN  ...               11.0               13.0   \n",
       "\n",
       "       white_blood_cells_mean  white_blood_cells_sd  white_blood_cells_min  \\\n",
       "0                   26.471429             13.176711                   13.2   \n",
       "1                   10.300000              1.272792                    9.4   \n",
       "2                   12.471429              1.471637                   10.5   \n",
       "3                    4.900000                   NaN                    4.9   \n",
       "4                   13.233333              2.203028                   10.7   \n",
       "...                       ...                   ...                    ...   \n",
       "30484               14.134783              3.781727                    8.1   \n",
       "30485               12.600000              0.605530                   12.0   \n",
       "30486               10.076190              2.642329                    5.3   \n",
       "30487                9.900000              1.210372                    7.9   \n",
       "30488               18.300000              3.394113                   15.9   \n",
       "\n",
       "       white_blood_cells_max  age  gender  icu_los_hours  target  \n",
       "0                       43.9   48       M            141       0  \n",
       "1                       11.2   44       M             30       0  \n",
       "2                       14.3   47       F             51       0  \n",
       "3                        4.9   33       F             10       0  \n",
       "4                       14.7   85       M             41       0  \n",
       "...                      ...  ...     ...            ...     ...  \n",
       "30484                   22.1   41       M            499       0  \n",
       "30485                   13.3   26       M             67       0  \n",
       "30486                   14.5   74       F            152       1  \n",
       "30487                   11.0   87       M             46       1  \n",
       "30488                   20.7   49       M             31       0  \n",
       "\n",
       "[30489 rows x 97 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort_data = pd.read_csv('../cohort_data_new.csv')\n",
    "cohort_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0e877cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_cols = [\n",
    "    'anion_gap_mean', 'anion_gap_min', 'anion_gap_max', 'anion_gap_sd',\n",
    "    'bicarbonate_mean', 'bicarbonate_min', 'bicarbonate_max', 'bicarbonate_sd',\n",
    "    'calcium_total_mean', 'calcium_total_min', 'calcium_total_max', 'calcium_total_sd',\n",
    "    'chloride_mean', 'chloride_min', 'chloride_max', 'chloride_sd',\n",
    "    'creatinine_mean', 'creatinine_min', 'creatinine_max', 'creatinine_sd',\n",
    "    'glucose_mean', 'glucose_min', 'glucose_max', 'glucose_sd',\n",
    "    'hematocrit_mean', 'hematocrit_min', 'hematocrit_max', 'hematocrit_sd',\n",
    "    'hemoglobin_mean', 'hemoglobin_min', 'hemoglobin_max', 'hemoglobin_sd',\n",
    "    'mchc_mean', 'mchc_min', 'mchc_max', 'mchc_sd',\n",
    "    'mcv_mean', 'mcv_min', 'mcv_max', 'mcv_sd',\n",
    "    'magnesium_mean', 'magnesium_min', 'magnesium_max', 'magnesium_sd',\n",
    "    'pt_mean', 'pt_min', 'pt_max', 'pt_sd',\n",
    "    'phosphate_mean', 'phosphate_min', 'phosphate_max', 'phosphate_sd',\n",
    "    'platelet_count_mean', 'platelet_count_min', 'platelet_count_max', 'platelet_count_sd',\n",
    "    'potassium_mean', 'potassium_min', 'potassium_max', 'potassium_sd',\n",
    "    'rdw_mean', 'rdw_min', 'rdw_max', 'rdw_sd',\n",
    "    'red_blood_cells_mean', 'red_blood_cells_min', 'red_blood_cells_max', 'red_blood_cells_sd',\n",
    "    'sodium_mean', 'sodium_min', 'sodium_max', 'sodium_sd',\n",
    "    'urea_nitrogen_mean', 'urea_nitrogen_min', 'urea_nitrogen_max', 'urea_nitrogen_sd',\n",
    "    'white_blood_cells_mean', 'white_blood_cells_min', 'white_blood_cells_max', 'white_blood_cells_sd',\n",
    "    'age', 'icu_los_hours'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c00ad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix shape: (30489, 94)\n"
     ]
    }
   ],
   "source": [
    "# REmove the ICUstay_id and the gender\n",
    "drop_cols = [c for c in cohort_data.columns if 'icustay_id' in c.lower() or 'gender' in c.lower()]\n",
    "df = cohort_data.drop(columns=['icustay_id', 'gender'], errors='ignore')\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[lab_cols] = imputer.fit_transform(df[lab_cols])\n",
    "\n",
    "# Keep only numeric\n",
    "X = X.select_dtypes(include=['number']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cfa74",
   "metadata": {},
   "source": [
    "Simple Imputation to handle the missing/ NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a15595ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.375     ,   3.5831949 ,   9.        , ...,  43.9       ,\n",
       "         48.        , 141.        ],\n",
       "       [ 15.5       ,   2.12132034,  14.        , ...,  11.2       ,\n",
       "         44.        ,  30.        ],\n",
       "       [  9.5       ,   2.12132034,   8.        , ...,  14.3       ,\n",
       "         47.        ,  51.        ],\n",
       "       ...,\n",
       "       [ 16.15789474,   2.47797314,  13.        , ...,  14.5       ,\n",
       "         74.        , 152.        ],\n",
       "       [ 11.5       ,   1.73205081,  10.        , ...,  11.        ,\n",
       "         87.        ,  46.        ],\n",
       "       [         nan,          nan,          nan, ...,  20.7       ,\n",
       "         49.        ,  31.        ]], shape=(30489, 94))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d09c1a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['inr_pt__mean' 'inr_pt__sd' 'inr_pt__min' 'inr_pt__max']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2da6c",
   "metadata": {},
   "source": [
    "#### Splitting and creating the final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e56c2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% readmission in train: 10.731021555763824\n",
      "% readmission in train: 10.746812386156648\n",
      "% readmission in train: 10.735760358587514\n"
     ]
    }
   ],
   "source": [
    "# train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.30, random_state=7)\n",
    "\n",
    "# train-dev\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_test, y_test, test_size=0.30, random_state=42, stratify=y_test)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val_train = scaler.transform(X_val_train)\n",
    "X_val_test = scaler.transform(X_val_test)\n",
    "\n",
    "print(f'% readmission in train: {np.mean(y_val_train)*100}')\n",
    "print(f'% readmission in train: {np.mean(y_val_test)*100}')\n",
    "print(f'% readmission in train: {np.mean(y_test)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d4051",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb2e25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 16, 64),\n",
    "        \"n_a\": trial.suggest_int(\"n_a\", 16, 64),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 7),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.5),\n",
    "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True),\n",
    "        \"optimizer_fn\": torch.optim.Adam,\n",
    "        \"optimizer_params\": dict(lr=trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)),\n",
    "        \"mask_type\": \"sparsemax\",\n",
    "    }\n",
    "\n",
    "    model = TabNetClassifier(**params)\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val_train, y_val_train), (X_val_test, y_val_test)],\n",
    "        eval_metric=[\"auc\"],\n",
    "        max_epochs=150,\n",
    "        patience=15,\n",
    "        batch_size=2048,\n",
    "        virtual_batch_size=256,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # Predict on validation test split\n",
    "    val_pred_proba = model.predict_proba(X_val_test)[:, 1]\n",
    "    auc = roc_auc_score(y_val_test, val_pred_proba)\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bc30c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:07:35,788] A new study created in memory with name: no-name-9d351a4a-700c-4e4f-b093-a080cc3fd87f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6439eeb951ce42cf81790213a0427593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.31105 | val_0_auc: 0.47387 | val_1_auc: 0.49133 |  0:00:06s\n",
      "epoch 1  | loss: 1.03798 | val_0_auc: 0.47567 | val_1_auc: 0.48662 |  0:00:14s\n",
      "epoch 2  | loss: 0.84756 | val_0_auc: 0.46776 | val_1_auc: 0.4838  |  0:00:23s\n",
      "epoch 3  | loss: 0.7183  | val_0_auc: 0.45989 | val_1_auc: 0.47143 |  0:00:32s\n",
      "epoch 4  | loss: 0.65099 | val_0_auc: 0.45906 | val_1_auc: 0.47001 |  0:00:41s\n",
      "epoch 5  | loss: 0.61761 | val_0_auc: 0.47248 | val_1_auc: 0.45602 |  0:00:49s\n",
      "epoch 6  | loss: 0.58399 | val_0_auc: 0.46051 | val_1_auc: 0.49064 |  0:00:57s\n",
      "epoch 7  | loss: 0.59339 | val_0_auc: 0.46907 | val_1_auc: 0.4677  |  0:01:05s\n",
      "epoch 8  | loss: 0.57427 | val_0_auc: 0.46097 | val_1_auc: 0.48063 |  0:01:14s\n",
      "epoch 9  | loss: 0.56195 | val_0_auc: 0.47132 | val_1_auc: 0.48681 |  0:01:24s\n",
      "epoch 10 | loss: 0.5536  | val_0_auc: 0.47436 | val_1_auc: 0.49062 |  0:01:32s\n",
      "epoch 11 | loss: 0.54995 | val_0_auc: 0.49165 | val_1_auc: 0.49974 |  0:01:42s\n",
      "epoch 12 | loss: 0.54118 | val_0_auc: 0.49769 | val_1_auc: 0.48165 |  0:01:51s\n",
      "epoch 13 | loss: 0.52721 | val_0_auc: 0.49562 | val_1_auc: 0.4888  |  0:02:00s\n",
      "epoch 14 | loss: 0.51635 | val_0_auc: 0.50601 | val_1_auc: 0.48551 |  0:02:10s\n",
      "epoch 15 | loss: 0.51582 | val_0_auc: 0.50334 | val_1_auc: 0.49451 |  0:02:16s\n",
      "epoch 16 | loss: 0.50776 | val_0_auc: 0.49499 | val_1_auc: 0.50705 |  0:02:22s\n",
      "epoch 17 | loss: 0.49816 | val_0_auc: 0.5119  | val_1_auc: 0.50203 |  0:02:29s\n",
      "epoch 18 | loss: 0.48879 | val_0_auc: 0.50386 | val_1_auc: 0.51125 |  0:02:38s\n",
      "epoch 19 | loss: 0.48078 | val_0_auc: 0.49501 | val_1_auc: 0.5191  |  0:02:47s\n",
      "epoch 20 | loss: 0.48128 | val_0_auc: 0.50483 | val_1_auc: 0.50457 |  0:02:55s\n",
      "epoch 21 | loss: 0.48099 | val_0_auc: 0.49677 | val_1_auc: 0.51289 |  0:03:03s\n",
      "epoch 22 | loss: 0.48094 | val_0_auc: 0.50921 | val_1_auc: 0.51657 |  0:03:13s\n",
      "epoch 23 | loss: 0.46375 | val_0_auc: 0.51134 | val_1_auc: 0.51902 |  0:03:20s\n",
      "epoch 24 | loss: 0.45813 | val_0_auc: 0.51191 | val_1_auc: 0.48241 |  0:03:29s\n",
      "epoch 25 | loss: 0.4463  | val_0_auc: 0.50829 | val_1_auc: 0.50073 |  0:03:37s\n",
      "epoch 26 | loss: 0.4552  | val_0_auc: 0.53016 | val_1_auc: 0.49548 |  0:03:45s\n",
      "epoch 27 | loss: 0.45693 | val_0_auc: 0.51826 | val_1_auc: 0.50087 |  0:03:53s\n",
      "epoch 28 | loss: 0.44041 | val_0_auc: 0.50753 | val_1_auc: 0.51007 |  0:04:00s\n",
      "epoch 29 | loss: 0.44592 | val_0_auc: 0.50639 | val_1_auc: 0.49297 |  0:04:08s\n",
      "epoch 30 | loss: 0.44339 | val_0_auc: 0.50888 | val_1_auc: 0.50279 |  0:04:16s\n",
      "epoch 31 | loss: 0.43055 | val_0_auc: 0.5044  | val_1_auc: 0.51872 |  0:04:23s\n",
      "epoch 32 | loss: 0.42976 | val_0_auc: 0.51911 | val_1_auc: 0.52603 |  0:04:33s\n",
      "epoch 33 | loss: 0.43318 | val_0_auc: 0.51726 | val_1_auc: 0.52623 |  0:04:42s\n",
      "epoch 34 | loss: 0.42927 | val_0_auc: 0.50175 | val_1_auc: 0.51069 |  0:04:50s\n",
      "epoch 35 | loss: 0.43047 | val_0_auc: 0.50339 | val_1_auc: 0.50798 |  0:04:58s\n",
      "epoch 36 | loss: 0.42216 | val_0_auc: 0.53686 | val_1_auc: 0.52261 |  0:05:06s\n",
      "epoch 37 | loss: 0.42069 | val_0_auc: 0.52612 | val_1_auc: 0.50926 |  0:05:15s\n",
      "epoch 38 | loss: 0.41777 | val_0_auc: 0.52974 | val_1_auc: 0.5251  |  0:05:24s\n",
      "epoch 39 | loss: 0.41393 | val_0_auc: 0.51557 | val_1_auc: 0.52183 |  0:05:34s\n",
      "epoch 40 | loss: 0.40821 | val_0_auc: 0.53344 | val_1_auc: 0.52493 |  0:05:43s\n",
      "epoch 41 | loss: 0.41739 | val_0_auc: 0.51722 | val_1_auc: 0.52321 |  0:05:52s\n",
      "epoch 42 | loss: 0.40453 | val_0_auc: 0.50793 | val_1_auc: 0.50653 |  0:06:01s\n",
      "epoch 43 | loss: 0.41278 | val_0_auc: 0.51802 | val_1_auc: 0.51008 |  0:06:10s\n",
      "epoch 44 | loss: 0.40691 | val_0_auc: 0.52523 | val_1_auc: 0.49721 |  0:06:19s\n",
      "epoch 45 | loss: 0.40798 | val_0_auc: 0.50614 | val_1_auc: 0.49926 |  0:06:28s\n",
      "epoch 46 | loss: 0.39894 | val_0_auc: 0.52346 | val_1_auc: 0.50705 |  0:06:38s\n",
      "epoch 47 | loss: 0.39792 | val_0_auc: 0.51259 | val_1_auc: 0.51418 |  0:06:47s\n",
      "epoch 48 | loss: 0.39853 | val_0_auc: 0.51852 | val_1_auc: 0.55037 |  0:06:56s\n",
      "epoch 49 | loss: 0.40404 | val_0_auc: 0.53492 | val_1_auc: 0.54705 |  0:07:06s\n",
      "epoch 50 | loss: 0.39442 | val_0_auc: 0.52212 | val_1_auc: 0.5413  |  0:07:17s\n",
      "epoch 51 | loss: 0.39581 | val_0_auc: 0.52245 | val_1_auc: 0.55768 |  0:07:25s\n",
      "epoch 52 | loss: 0.39361 | val_0_auc: 0.54117 | val_1_auc: 0.56514 |  0:07:34s\n",
      "epoch 53 | loss: 0.38961 | val_0_auc: 0.53641 | val_1_auc: 0.52745 |  0:07:43s\n",
      "epoch 54 | loss: 0.38943 | val_0_auc: 0.54136 | val_1_auc: 0.55656 |  0:07:51s\n",
      "epoch 55 | loss: 0.39165 | val_0_auc: 0.54717 | val_1_auc: 0.56516 |  0:07:59s\n",
      "epoch 56 | loss: 0.39415 | val_0_auc: 0.52442 | val_1_auc: 0.5252  |  0:08:09s\n",
      "epoch 57 | loss: 0.38486 | val_0_auc: 0.54503 | val_1_auc: 0.54965 |  0:08:17s\n",
      "epoch 58 | loss: 0.39073 | val_0_auc: 0.561   | val_1_auc: 0.53865 |  0:08:25s\n",
      "epoch 59 | loss: 0.38772 | val_0_auc: 0.52824 | val_1_auc: 0.55796 |  0:08:33s\n",
      "epoch 60 | loss: 0.3794  | val_0_auc: 0.54852 | val_1_auc: 0.54221 |  0:08:40s\n",
      "epoch 61 | loss: 0.38521 | val_0_auc: 0.54583 | val_1_auc: 0.55798 |  0:08:47s\n",
      "epoch 62 | loss: 0.38043 | val_0_auc: 0.55004 | val_1_auc: 0.55162 |  0:08:54s\n",
      "epoch 63 | loss: 0.37979 | val_0_auc: 0.54398 | val_1_auc: 0.5635  |  0:09:00s\n",
      "epoch 64 | loss: 0.38305 | val_0_auc: 0.54798 | val_1_auc: 0.57624 |  0:09:08s\n",
      "epoch 65 | loss: 0.38153 | val_0_auc: 0.54502 | val_1_auc: 0.54995 |  0:09:13s\n",
      "epoch 66 | loss: 0.37462 | val_0_auc: 0.53493 | val_1_auc: 0.56078 |  0:09:20s\n",
      "epoch 67 | loss: 0.37971 | val_0_auc: 0.54741 | val_1_auc: 0.57387 |  0:09:27s\n",
      "epoch 68 | loss: 0.37871 | val_0_auc: 0.53495 | val_1_auc: 0.56374 |  0:09:35s\n",
      "epoch 69 | loss: 0.37307 | val_0_auc: 0.53702 | val_1_auc: 0.55622 |  0:09:43s\n",
      "epoch 70 | loss: 0.3721  | val_0_auc: 0.53906 | val_1_auc: 0.54269 |  0:09:51s\n",
      "epoch 71 | loss: 0.37088 | val_0_auc: 0.55015 | val_1_auc: 0.55545 |  0:09:58s\n",
      "epoch 72 | loss: 0.37546 | val_0_auc: 0.54661 | val_1_auc: 0.54409 |  0:10:04s\n",
      "epoch 73 | loss: 0.37263 | val_0_auc: 0.54999 | val_1_auc: 0.54037 |  0:10:11s\n",
      "epoch 74 | loss: 0.36953 | val_0_auc: 0.56518 | val_1_auc: 0.58114 |  0:10:18s\n",
      "epoch 75 | loss: 0.37252 | val_0_auc: 0.57327 | val_1_auc: 0.53454 |  0:10:25s\n",
      "epoch 76 | loss: 0.36647 | val_0_auc: 0.56898 | val_1_auc: 0.55095 |  0:10:31s\n",
      "epoch 77 | loss: 0.3687  | val_0_auc: 0.56688 | val_1_auc: 0.55569 |  0:10:37s\n",
      "epoch 78 | loss: 0.36677 | val_0_auc: 0.55608 | val_1_auc: 0.56772 |  0:10:44s\n",
      "epoch 79 | loss: 0.36988 | val_0_auc: 0.56951 | val_1_auc: 0.54956 |  0:10:51s\n",
      "epoch 80 | loss: 0.36649 | val_0_auc: 0.56157 | val_1_auc: 0.55103 |  0:10:57s\n",
      "epoch 81 | loss: 0.36556 | val_0_auc: 0.56146 | val_1_auc: 0.54535 |  0:11:04s\n",
      "epoch 82 | loss: 0.36602 | val_0_auc: 0.56565 | val_1_auc: 0.55583 |  0:11:10s\n",
      "epoch 83 | loss: 0.3701  | val_0_auc: 0.5666  | val_1_auc: 0.5592  |  0:11:17s\n",
      "epoch 84 | loss: 0.36638 | val_0_auc: 0.55714 | val_1_auc: 0.57984 |  0:11:24s\n",
      "epoch 85 | loss: 0.36512 | val_0_auc: 0.57773 | val_1_auc: 0.56071 |  0:11:31s\n",
      "epoch 86 | loss: 0.36716 | val_0_auc: 0.58142 | val_1_auc: 0.57526 |  0:11:39s\n",
      "epoch 87 | loss: 0.3623  | val_0_auc: 0.57631 | val_1_auc: 0.5741  |  0:11:46s\n",
      "epoch 88 | loss: 0.36674 | val_0_auc: 0.57448 | val_1_auc: 0.54658 |  0:11:53s\n",
      "epoch 89 | loss: 0.36765 | val_0_auc: 0.57021 | val_1_auc: 0.56997 |  0:11:59s\n",
      "\n",
      "Early stopping occurred at epoch 89 with best_epoch = 74 and best_val_1_auc = 0.58114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:19:39,387] Trial 0 finished with value: 0.5811359391214113 and parameters: {'n_d': 59, 'n_a': 32, 'n_steps': 7, 'gamma': 1.8876628851949757, 'lambda_sparse': 0.00013064329625095796, 'lr': 0.0004036376801516901}. Best is trial 0 with value: 0.5811359391214113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.29488 | val_0_auc: 0.49107 | val_1_auc: 0.47244 |  0:00:04s\n",
      "epoch 1  | loss: 1.95519 | val_0_auc: 0.48767 | val_1_auc: 0.49739 |  0:00:08s\n",
      "epoch 2  | loss: 1.65794 | val_0_auc: 0.4966  | val_1_auc: 0.48264 |  0:00:13s\n",
      "epoch 3  | loss: 1.3863  | val_0_auc: 0.49362 | val_1_auc: 0.49216 |  0:00:17s\n",
      "epoch 4  | loss: 1.18203 | val_0_auc: 0.49215 | val_1_auc: 0.48758 |  0:00:21s\n",
      "epoch 5  | loss: 0.99252 | val_0_auc: 0.488   | val_1_auc: 0.4782  |  0:00:26s\n",
      "epoch 6  | loss: 0.87821 | val_0_auc: 0.48986 | val_1_auc: 0.48609 |  0:00:30s\n",
      "epoch 7  | loss: 0.74936 | val_0_auc: 0.48175 | val_1_auc: 0.47443 |  0:00:34s\n",
      "epoch 8  | loss: 0.66308 | val_0_auc: 0.47774 | val_1_auc: 0.4754  |  0:00:39s\n",
      "epoch 9  | loss: 0.61725 | val_0_auc: 0.47273 | val_1_auc: 0.46797 |  0:00:43s\n",
      "epoch 10 | loss: 0.56564 | val_0_auc: 0.47095 | val_1_auc: 0.47405 |  0:00:47s\n",
      "epoch 11 | loss: 0.54352 | val_0_auc: 0.47191 | val_1_auc: 0.46004 |  0:00:51s\n",
      "epoch 12 | loss: 0.51239 | val_0_auc: 0.47565 | val_1_auc: 0.45717 |  0:00:55s\n",
      "epoch 13 | loss: 0.51493 | val_0_auc: 0.47027 | val_1_auc: 0.47161 |  0:01:00s\n",
      "epoch 14 | loss: 0.48753 | val_0_auc: 0.4729  | val_1_auc: 0.47724 |  0:01:04s\n",
      "epoch 15 | loss: 0.47976 | val_0_auc: 0.46901 | val_1_auc: 0.46179 |  0:01:08s\n",
      "epoch 16 | loss: 0.46687 | val_0_auc: 0.47275 | val_1_auc: 0.45113 |  0:01:13s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 1 and best_val_1_auc = 0.49739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:20:55,121] Trial 1 finished with value: 0.49739398132134216 and parameters: {'n_d': 57, 'n_a': 42, 'n_steps': 4, 'gamma': 1.4555579210759475, 'lambda_sparse': 1.4740501583345327e-06, 'lr': 0.00026261674955689596}. Best is trial 0 with value: 0.5811359391214113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.04028 | val_0_auc: 0.51475 | val_1_auc: 0.4985  |  0:00:07s\n",
      "epoch 1  | loss: 0.57795 | val_0_auc: 0.49308 | val_1_auc: 0.47595 |  0:00:14s\n",
      "epoch 2  | loss: 0.48743 | val_0_auc: 0.47443 | val_1_auc: 0.46678 |  0:00:20s\n",
      "epoch 3  | loss: 0.47031 | val_0_auc: 0.48483 | val_1_auc: 0.49503 |  0:00:26s\n",
      "epoch 4  | loss: 0.45695 | val_0_auc: 0.47265 | val_1_auc: 0.48091 |  0:00:31s\n",
      "epoch 5  | loss: 0.4294  | val_0_auc: 0.49381 | val_1_auc: 0.47662 |  0:00:37s\n",
      "epoch 6  | loss: 0.42022 | val_0_auc: 0.50402 | val_1_auc: 0.48228 |  0:00:43s\n",
      "epoch 7  | loss: 0.41127 | val_0_auc: 0.50738 | val_1_auc: 0.5013  |  0:00:48s\n",
      "epoch 8  | loss: 0.40191 | val_0_auc: 0.48884 | val_1_auc: 0.502   |  0:00:54s\n",
      "epoch 9  | loss: 0.39224 | val_0_auc: 0.51982 | val_1_auc: 0.49391 |  0:00:59s\n",
      "epoch 10 | loss: 0.38822 | val_0_auc: 0.52837 | val_1_auc: 0.50763 |  0:01:05s\n",
      "epoch 11 | loss: 0.38142 | val_0_auc: 0.5116  | val_1_auc: 0.50281 |  0:01:11s\n",
      "epoch 12 | loss: 0.37556 | val_0_auc: 0.52156 | val_1_auc: 0.52054 |  0:01:16s\n",
      "epoch 13 | loss: 0.37578 | val_0_auc: 0.53449 | val_1_auc: 0.51919 |  0:01:22s\n",
      "epoch 14 | loss: 0.37512 | val_0_auc: 0.53367 | val_1_auc: 0.4889  |  0:01:27s\n",
      "epoch 15 | loss: 0.36808 | val_0_auc: 0.52771 | val_1_auc: 0.52469 |  0:01:33s\n",
      "epoch 16 | loss: 0.36463 | val_0_auc: 0.54712 | val_1_auc: 0.51051 |  0:01:38s\n",
      "epoch 17 | loss: 0.36337 | val_0_auc: 0.55085 | val_1_auc: 0.52977 |  0:01:43s\n",
      "epoch 18 | loss: 0.35673 | val_0_auc: 0.55057 | val_1_auc: 0.53698 |  0:01:49s\n",
      "epoch 19 | loss: 0.35476 | val_0_auc: 0.55943 | val_1_auc: 0.55085 |  0:01:54s\n",
      "epoch 20 | loss: 0.35665 | val_0_auc: 0.56592 | val_1_auc: 0.52894 |  0:02:00s\n",
      "epoch 21 | loss: 0.35277 | val_0_auc: 0.55997 | val_1_auc: 0.54053 |  0:02:05s\n",
      "epoch 22 | loss: 0.35472 | val_0_auc: 0.56442 | val_1_auc: 0.53082 |  0:02:10s\n",
      "epoch 23 | loss: 0.35112 | val_0_auc: 0.57206 | val_1_auc: 0.54263 |  0:02:16s\n",
      "epoch 24 | loss: 0.34789 | val_0_auc: 0.57734 | val_1_auc: 0.56515 |  0:02:21s\n",
      "epoch 25 | loss: 0.34821 | val_0_auc: 0.58855 | val_1_auc: 0.56769 |  0:02:27s\n",
      "epoch 26 | loss: 0.34574 | val_0_auc: 0.58673 | val_1_auc: 0.55704 |  0:02:33s\n",
      "epoch 27 | loss: 0.3476  | val_0_auc: 0.58242 | val_1_auc: 0.55174 |  0:02:39s\n",
      "epoch 28 | loss: 0.34398 | val_0_auc: 0.59969 | val_1_auc: 0.5667  |  0:02:44s\n",
      "epoch 29 | loss: 0.34342 | val_0_auc: 0.59176 | val_1_auc: 0.58634 |  0:02:49s\n",
      "epoch 30 | loss: 0.33924 | val_0_auc: 0.59587 | val_1_auc: 0.59078 |  0:02:55s\n",
      "epoch 31 | loss: 0.34035 | val_0_auc: 0.59807 | val_1_auc: 0.59535 |  0:03:01s\n",
      "epoch 32 | loss: 0.33888 | val_0_auc: 0.61027 | val_1_auc: 0.58339 |  0:03:06s\n",
      "epoch 33 | loss: 0.33918 | val_0_auc: 0.61146 | val_1_auc: 0.58832 |  0:03:12s\n",
      "epoch 34 | loss: 0.34001 | val_0_auc: 0.61593 | val_1_auc: 0.5718  |  0:03:17s\n",
      "epoch 35 | loss: 0.33937 | val_0_auc: 0.61908 | val_1_auc: 0.59688 |  0:03:23s\n",
      "epoch 36 | loss: 0.33748 | val_0_auc: 0.61597 | val_1_auc: 0.57671 |  0:03:29s\n",
      "epoch 37 | loss: 0.33609 | val_0_auc: 0.62343 | val_1_auc: 0.61639 |  0:03:34s\n",
      "epoch 38 | loss: 0.33496 | val_0_auc: 0.63365 | val_1_auc: 0.59962 |  0:03:40s\n",
      "epoch 39 | loss: 0.33355 | val_0_auc: 0.62492 | val_1_auc: 0.61498 |  0:03:47s\n",
      "epoch 40 | loss: 0.33657 | val_0_auc: 0.63427 | val_1_auc: 0.62682 |  0:03:54s\n",
      "epoch 41 | loss: 0.33518 | val_0_auc: 0.62107 | val_1_auc: 0.6005  |  0:04:01s\n",
      "epoch 42 | loss: 0.3355  | val_0_auc: 0.62139 | val_1_auc: 0.6082  |  0:04:08s\n",
      "epoch 43 | loss: 0.33186 | val_0_auc: 0.62817 | val_1_auc: 0.60394 |  0:04:15s\n",
      "epoch 44 | loss: 0.33203 | val_0_auc: 0.62084 | val_1_auc: 0.61691 |  0:04:22s\n",
      "epoch 45 | loss: 0.3362  | val_0_auc: 0.63031 | val_1_auc: 0.63127 |  0:04:30s\n",
      "epoch 46 | loss: 0.33553 | val_0_auc: 0.62889 | val_1_auc: 0.62562 |  0:04:37s\n",
      "epoch 47 | loss: 0.33499 | val_0_auc: 0.62272 | val_1_auc: 0.62171 |  0:04:43s\n",
      "epoch 48 | loss: 0.332   | val_0_auc: 0.63509 | val_1_auc: 0.62027 |  0:04:49s\n",
      "epoch 49 | loss: 0.33216 | val_0_auc: 0.62525 | val_1_auc: 0.59699 |  0:04:55s\n",
      "epoch 50 | loss: 0.33246 | val_0_auc: 0.62998 | val_1_auc: 0.60217 |  0:05:00s\n",
      "epoch 51 | loss: 0.332   | val_0_auc: 0.62712 | val_1_auc: 0.61675 |  0:05:06s\n",
      "epoch 52 | loss: 0.33194 | val_0_auc: 0.63005 | val_1_auc: 0.63113 |  0:05:11s\n",
      "epoch 53 | loss: 0.33197 | val_0_auc: 0.63861 | val_1_auc: 0.62958 |  0:05:16s\n",
      "epoch 54 | loss: 0.3312  | val_0_auc: 0.63009 | val_1_auc: 0.62708 |  0:05:22s\n",
      "epoch 55 | loss: 0.33108 | val_0_auc: 0.63123 | val_1_auc: 0.62845 |  0:05:27s\n",
      "epoch 56 | loss: 0.32974 | val_0_auc: 0.64023 | val_1_auc: 0.62836 |  0:05:33s\n",
      "epoch 57 | loss: 0.3318  | val_0_auc: 0.63057 | val_1_auc: 0.63516 |  0:05:38s\n",
      "epoch 58 | loss: 0.32997 | val_0_auc: 0.6398  | val_1_auc: 0.62398 |  0:05:44s\n",
      "epoch 59 | loss: 0.3286  | val_0_auc: 0.64024 | val_1_auc: 0.61455 |  0:05:49s\n",
      "epoch 60 | loss: 0.32703 | val_0_auc: 0.64797 | val_1_auc: 0.61279 |  0:05:54s\n",
      "epoch 61 | loss: 0.32962 | val_0_auc: 0.64588 | val_1_auc: 0.60968 |  0:06:00s\n",
      "epoch 62 | loss: 0.32843 | val_0_auc: 0.64256 | val_1_auc: 0.60746 |  0:06:06s\n",
      "epoch 63 | loss: 0.32966 | val_0_auc: 0.64585 | val_1_auc: 0.61261 |  0:06:12s\n",
      "epoch 64 | loss: 0.33016 | val_0_auc: 0.64396 | val_1_auc: 0.62905 |  0:06:17s\n",
      "epoch 65 | loss: 0.32959 | val_0_auc: 0.64846 | val_1_auc: 0.63586 |  0:06:23s\n",
      "epoch 66 | loss: 0.32896 | val_0_auc: 0.64733 | val_1_auc: 0.64341 |  0:06:28s\n",
      "epoch 67 | loss: 0.32974 | val_0_auc: 0.64383 | val_1_auc: 0.6408  |  0:06:34s\n",
      "epoch 68 | loss: 0.32841 | val_0_auc: 0.63314 | val_1_auc: 0.62717 |  0:06:40s\n",
      "epoch 69 | loss: 0.32991 | val_0_auc: 0.63734 | val_1_auc: 0.62684 |  0:06:47s\n",
      "epoch 70 | loss: 0.33023 | val_0_auc: 0.64033 | val_1_auc: 0.64247 |  0:06:53s\n",
      "epoch 71 | loss: 0.33008 | val_0_auc: 0.64161 | val_1_auc: 0.62883 |  0:06:58s\n",
      "epoch 72 | loss: 0.33009 | val_0_auc: 0.63805 | val_1_auc: 0.62816 |  0:07:03s\n",
      "epoch 73 | loss: 0.33065 | val_0_auc: 0.65107 | val_1_auc: 0.62297 |  0:07:09s\n",
      "epoch 74 | loss: 0.32964 | val_0_auc: 0.6476  | val_1_auc: 0.63132 |  0:07:15s\n",
      "epoch 75 | loss: 0.32907 | val_0_auc: 0.6455  | val_1_auc: 0.62307 |  0:07:20s\n",
      "epoch 76 | loss: 0.328   | val_0_auc: 0.64541 | val_1_auc: 0.63326 |  0:07:25s\n",
      "epoch 77 | loss: 0.32852 | val_0_auc: 0.64936 | val_1_auc: 0.63991 |  0:07:31s\n",
      "epoch 78 | loss: 0.32758 | val_0_auc: 0.6392  | val_1_auc: 0.62142 |  0:07:36s\n",
      "epoch 79 | loss: 0.32914 | val_0_auc: 0.64881 | val_1_auc: 0.63779 |  0:07:41s\n",
      "epoch 80 | loss: 0.32803 | val_0_auc: 0.64283 | val_1_auc: 0.63111 |  0:07:46s\n",
      "epoch 81 | loss: 0.32694 | val_0_auc: 0.64833 | val_1_auc: 0.63171 |  0:07:51s\n",
      "\n",
      "Early stopping occurred at epoch 81 with best_epoch = 66 and best_val_1_auc = 0.64341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:28:48,676] Trial 2 finished with value: 0.6434050501556555 and parameters: {'n_d': 22, 'n_a': 37, 'n_steps': 7, 'gamma': 1.4940671727838684, 'lambda_sparse': 1.9880234323488604e-06, 'lr': 0.0019924844436065912}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.20566 | val_0_auc: 0.5028  | val_1_auc: 0.52507 |  0:00:05s\n",
      "epoch 1  | loss: 2.13387 | val_0_auc: 0.49933 | val_1_auc: 0.51807 |  0:00:10s\n",
      "epoch 2  | loss: 2.05971 | val_0_auc: 0.49691 | val_1_auc: 0.54833 |  0:00:15s\n",
      "epoch 3  | loss: 1.98665 | val_0_auc: 0.50374 | val_1_auc: 0.53845 |  0:00:20s\n",
      "epoch 4  | loss: 1.92461 | val_0_auc: 0.50374 | val_1_auc: 0.53067 |  0:00:25s\n",
      "epoch 5  | loss: 1.86385 | val_0_auc: 0.49516 | val_1_auc: 0.51315 |  0:00:30s\n",
      "epoch 6  | loss: 1.78322 | val_0_auc: 0.50401 | val_1_auc: 0.5218  |  0:00:36s\n",
      "epoch 7  | loss: 1.72098 | val_0_auc: 0.50334 | val_1_auc: 0.52455 |  0:00:41s\n",
      "epoch 8  | loss: 1.65591 | val_0_auc: 0.50286 | val_1_auc: 0.52663 |  0:00:46s\n",
      "epoch 9  | loss: 1.58241 | val_0_auc: 0.5026  | val_1_auc: 0.51685 |  0:00:51s\n",
      "epoch 10 | loss: 1.50643 | val_0_auc: 0.49615 | val_1_auc: 0.51081 |  0:00:56s\n",
      "epoch 11 | loss: 1.43803 | val_0_auc: 0.49888 | val_1_auc: 0.52966 |  0:01:02s\n",
      "epoch 12 | loss: 1.37529 | val_0_auc: 0.48287 | val_1_auc: 0.51333 |  0:01:07s\n",
      "epoch 13 | loss: 1.31599 | val_0_auc: 0.4862  | val_1_auc: 0.51674 |  0:01:12s\n",
      "epoch 14 | loss: 1.26551 | val_0_auc: 0.50244 | val_1_auc: 0.53764 |  0:01:17s\n",
      "epoch 15 | loss: 1.20483 | val_0_auc: 0.48705 | val_1_auc: 0.50498 |  0:01:22s\n",
      "epoch 16 | loss: 1.14408 | val_0_auc: 0.49783 | val_1_auc: 0.52491 |  0:01:27s\n",
      "epoch 17 | loss: 1.1033  | val_0_auc: 0.49772 | val_1_auc: 0.51176 |  0:01:32s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 2 and best_val_1_auc = 0.54833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:30:23,321] Trial 3 finished with value: 0.5483258388101003 and parameters: {'n_d': 22, 'n_a': 28, 'n_steps': 7, 'gamma': 1.720803875652532, 'lambda_sparse': 6.997558431778817e-05, 'lr': 0.0001411493562639232}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.84493 | val_0_auc: 0.48701 | val_1_auc: 0.46812 |  0:00:04s\n",
      "epoch 1  | loss: 0.64449 | val_0_auc: 0.49037 | val_1_auc: 0.51167 |  0:00:08s\n",
      "epoch 2  | loss: 0.54973 | val_0_auc: 0.48702 | val_1_auc: 0.49677 |  0:00:12s\n",
      "epoch 3  | loss: 0.51647 | val_0_auc: 0.48549 | val_1_auc: 0.50973 |  0:00:16s\n",
      "epoch 4  | loss: 0.49711 | val_0_auc: 0.4994  | val_1_auc: 0.51032 |  0:00:21s\n",
      "epoch 5  | loss: 0.47619 | val_0_auc: 0.49149 | val_1_auc: 0.52432 |  0:00:25s\n",
      "epoch 6  | loss: 0.47238 | val_0_auc: 0.49469 | val_1_auc: 0.52414 |  0:00:29s\n",
      "epoch 7  | loss: 0.46283 | val_0_auc: 0.49619 | val_1_auc: 0.524   |  0:00:34s\n",
      "epoch 8  | loss: 0.45227 | val_0_auc: 0.51353 | val_1_auc: 0.51744 |  0:00:38s\n",
      "epoch 9  | loss: 0.44633 | val_0_auc: 0.50711 | val_1_auc: 0.52522 |  0:00:42s\n",
      "epoch 10 | loss: 0.4313  | val_0_auc: 0.51389 | val_1_auc: 0.53026 |  0:00:47s\n",
      "epoch 11 | loss: 0.42575 | val_0_auc: 0.52406 | val_1_auc: 0.51955 |  0:00:51s\n",
      "epoch 12 | loss: 0.42178 | val_0_auc: 0.52994 | val_1_auc: 0.5386  |  0:00:55s\n",
      "epoch 13 | loss: 0.40758 | val_0_auc: 0.53192 | val_1_auc: 0.55403 |  0:00:59s\n",
      "epoch 14 | loss: 0.40324 | val_0_auc: 0.53476 | val_1_auc: 0.56409 |  0:01:03s\n",
      "epoch 15 | loss: 0.3988  | val_0_auc: 0.53366 | val_1_auc: 0.56834 |  0:01:08s\n",
      "epoch 16 | loss: 0.39788 | val_0_auc: 0.52733 | val_1_auc: 0.54741 |  0:01:12s\n",
      "epoch 17 | loss: 0.39569 | val_0_auc: 0.5216  | val_1_auc: 0.55029 |  0:01:16s\n",
      "epoch 18 | loss: 0.3879  | val_0_auc: 0.53096 | val_1_auc: 0.56763 |  0:01:20s\n",
      "epoch 19 | loss: 0.38435 | val_0_auc: 0.54759 | val_1_auc: 0.57361 |  0:01:24s\n",
      "epoch 20 | loss: 0.38298 | val_0_auc: 0.53549 | val_1_auc: 0.57084 |  0:01:28s\n",
      "epoch 21 | loss: 0.37868 | val_0_auc: 0.53939 | val_1_auc: 0.57111 |  0:01:33s\n",
      "epoch 22 | loss: 0.37949 | val_0_auc: 0.53826 | val_1_auc: 0.56579 |  0:01:37s\n",
      "epoch 23 | loss: 0.38007 | val_0_auc: 0.54279 | val_1_auc: 0.56369 |  0:01:41s\n",
      "epoch 24 | loss: 0.37217 | val_0_auc: 0.53591 | val_1_auc: 0.58449 |  0:01:45s\n",
      "epoch 25 | loss: 0.37438 | val_0_auc: 0.54368 | val_1_auc: 0.58074 |  0:01:50s\n",
      "epoch 26 | loss: 0.36808 | val_0_auc: 0.54685 | val_1_auc: 0.55838 |  0:01:54s\n",
      "epoch 27 | loss: 0.36697 | val_0_auc: 0.55089 | val_1_auc: 0.55513 |  0:01:58s\n",
      "epoch 28 | loss: 0.36902 | val_0_auc: 0.54808 | val_1_auc: 0.56341 |  0:02:03s\n",
      "epoch 29 | loss: 0.37069 | val_0_auc: 0.53974 | val_1_auc: 0.554   |  0:02:07s\n",
      "epoch 30 | loss: 0.36704 | val_0_auc: 0.56262 | val_1_auc: 0.57343 |  0:02:11s\n",
      "epoch 31 | loss: 0.36598 | val_0_auc: 0.55959 | val_1_auc: 0.57863 |  0:02:15s\n",
      "epoch 32 | loss: 0.36477 | val_0_auc: 0.55592 | val_1_auc: 0.57342 |  0:02:19s\n",
      "epoch 33 | loss: 0.36295 | val_0_auc: 0.55273 | val_1_auc: 0.57233 |  0:02:23s\n",
      "epoch 34 | loss: 0.35745 | val_0_auc: 0.55936 | val_1_auc: 0.57617 |  0:02:28s\n",
      "epoch 35 | loss: 0.3631  | val_0_auc: 0.55805 | val_1_auc: 0.57036 |  0:02:32s\n",
      "epoch 36 | loss: 0.36065 | val_0_auc: 0.55835 | val_1_auc: 0.56676 |  0:02:36s\n",
      "epoch 37 | loss: 0.36015 | val_0_auc: 0.55923 | val_1_auc: 0.57109 |  0:02:41s\n",
      "epoch 38 | loss: 0.35863 | val_0_auc: 0.54816 | val_1_auc: 0.57276 |  0:02:46s\n",
      "epoch 39 | loss: 0.35997 | val_0_auc: 0.55649 | val_1_auc: 0.59969 |  0:02:50s\n",
      "epoch 40 | loss: 0.35915 | val_0_auc: 0.55777 | val_1_auc: 0.58448 |  0:02:54s\n",
      "epoch 41 | loss: 0.35529 | val_0_auc: 0.5569  | val_1_auc: 0.58396 |  0:02:59s\n",
      "epoch 42 | loss: 0.35562 | val_0_auc: 0.56955 | val_1_auc: 0.59306 |  0:03:03s\n",
      "epoch 43 | loss: 0.3569  | val_0_auc: 0.56791 | val_1_auc: 0.59511 |  0:03:07s\n",
      "epoch 44 | loss: 0.35464 | val_0_auc: 0.55732 | val_1_auc: 0.59637 |  0:03:11s\n",
      "epoch 45 | loss: 0.35435 | val_0_auc: 0.5656  | val_1_auc: 0.59605 |  0:03:15s\n",
      "epoch 46 | loss: 0.35503 | val_0_auc: 0.54954 | val_1_auc: 0.57903 |  0:03:19s\n",
      "epoch 47 | loss: 0.35442 | val_0_auc: 0.55586 | val_1_auc: 0.57269 |  0:03:24s\n",
      "epoch 48 | loss: 0.35441 | val_0_auc: 0.56757 | val_1_auc: 0.58527 |  0:03:28s\n",
      "epoch 49 | loss: 0.35141 | val_0_auc: 0.56347 | val_1_auc: 0.58132 |  0:03:32s\n",
      "epoch 50 | loss: 0.35144 | val_0_auc: 0.56196 | val_1_auc: 0.59412 |  0:03:37s\n",
      "epoch 51 | loss: 0.35242 | val_0_auc: 0.57727 | val_1_auc: 0.59532 |  0:03:41s\n",
      "epoch 52 | loss: 0.34857 | val_0_auc: 0.58621 | val_1_auc: 0.58988 |  0:03:45s\n",
      "epoch 53 | loss: 0.35226 | val_0_auc: 0.57925 | val_1_auc: 0.58944 |  0:03:49s\n",
      "epoch 54 | loss: 0.34894 | val_0_auc: 0.57057 | val_1_auc: 0.59336 |  0:03:53s\n",
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 39 and best_val_1_auc = 0.59969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:34:18,752] Trial 4 finished with value: 0.5996949152542373 and parameters: {'n_d': 53, 'n_a': 59, 'n_steps': 4, 'gamma': 2.3986118171757993, 'lambda_sparse': 2.814064515287881e-05, 'lr': 0.0005854687228698557}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.69905 | val_0_auc: 0.50105 | val_1_auc: 0.46687 |  0:00:06s\n",
      "epoch 1  | loss: 1.08132 | val_0_auc: 0.50249 | val_1_auc: 0.50191 |  0:00:15s\n",
      "epoch 2  | loss: 0.74851 | val_0_auc: 0.49659 | val_1_auc: 0.50069 |  0:00:22s\n",
      "epoch 3  | loss: 0.61638 | val_0_auc: 0.49009 | val_1_auc: 0.50752 |  0:00:28s\n",
      "epoch 4  | loss: 0.58063 | val_0_auc: 0.51085 | val_1_auc: 0.4811  |  0:00:35s\n",
      "epoch 5  | loss: 0.54523 | val_0_auc: 0.50483 | val_1_auc: 0.51559 |  0:00:42s\n",
      "epoch 6  | loss: 0.52474 | val_0_auc: 0.49354 | val_1_auc: 0.51784 |  0:00:48s\n",
      "epoch 7  | loss: 0.5068  | val_0_auc: 0.51851 | val_1_auc: 0.51737 |  0:00:55s\n",
      "epoch 8  | loss: 0.50121 | val_0_auc: 0.50876 | val_1_auc: 0.52128 |  0:01:01s\n",
      "epoch 9  | loss: 0.49844 | val_0_auc: 0.51601 | val_1_auc: 0.52561 |  0:01:08s\n",
      "epoch 10 | loss: 0.49805 | val_0_auc: 0.51829 | val_1_auc: 0.5104  |  0:01:14s\n",
      "epoch 11 | loss: 0.47833 | val_0_auc: 0.52171 | val_1_auc: 0.50533 |  0:01:20s\n",
      "epoch 12 | loss: 0.4582  | val_0_auc: 0.53287 | val_1_auc: 0.51998 |  0:01:26s\n",
      "epoch 13 | loss: 0.46655 | val_0_auc: 0.55036 | val_1_auc: 0.52273 |  0:01:33s\n",
      "epoch 14 | loss: 0.44926 | val_0_auc: 0.51869 | val_1_auc: 0.51197 |  0:01:39s\n",
      "epoch 15 | loss: 0.44763 | val_0_auc: 0.50738 | val_1_auc: 0.5343  |  0:01:46s\n",
      "epoch 16 | loss: 0.44925 | val_0_auc: 0.51561 | val_1_auc: 0.52876 |  0:01:52s\n",
      "epoch 17 | loss: 0.43867 | val_0_auc: 0.53222 | val_1_auc: 0.54868 |  0:01:58s\n",
      "epoch 18 | loss: 0.43065 | val_0_auc: 0.5359  | val_1_auc: 0.54553 |  0:02:04s\n",
      "epoch 19 | loss: 0.42976 | val_0_auc: 0.54819 | val_1_auc: 0.54674 |  0:02:11s\n",
      "epoch 20 | loss: 0.41673 | val_0_auc: 0.53503 | val_1_auc: 0.54621 |  0:02:17s\n",
      "epoch 21 | loss: 0.42252 | val_0_auc: 0.53966 | val_1_auc: 0.5414  |  0:02:23s\n",
      "epoch 22 | loss: 0.40825 | val_0_auc: 0.54602 | val_1_auc: 0.5474  |  0:02:30s\n",
      "epoch 23 | loss: 0.41245 | val_0_auc: 0.55855 | val_1_auc: 0.55239 |  0:02:38s\n",
      "epoch 24 | loss: 0.40561 | val_0_auc: 0.55316 | val_1_auc: 0.53114 |  0:02:45s\n",
      "epoch 25 | loss: 0.39704 | val_0_auc: 0.55526 | val_1_auc: 0.54308 |  0:02:53s\n",
      "epoch 26 | loss: 0.39934 | val_0_auc: 0.56691 | val_1_auc: 0.53695 |  0:03:00s\n",
      "epoch 27 | loss: 0.39303 | val_0_auc: 0.56405 | val_1_auc: 0.5521  |  0:03:07s\n",
      "epoch 28 | loss: 0.39289 | val_0_auc: 0.56072 | val_1_auc: 0.57891 |  0:03:13s\n",
      "epoch 29 | loss: 0.39453 | val_0_auc: 0.54619 | val_1_auc: 0.57117 |  0:03:20s\n",
      "epoch 30 | loss: 0.38875 | val_0_auc: 0.56515 | val_1_auc: 0.55611 |  0:03:27s\n",
      "epoch 31 | loss: 0.38141 | val_0_auc: 0.55672 | val_1_auc: 0.57692 |  0:03:34s\n",
      "epoch 32 | loss: 0.38494 | val_0_auc: 0.56902 | val_1_auc: 0.58288 |  0:03:42s\n",
      "epoch 33 | loss: 0.37987 | val_0_auc: 0.54952 | val_1_auc: 0.56676 |  0:03:50s\n",
      "epoch 34 | loss: 0.37937 | val_0_auc: 0.56131 | val_1_auc: 0.54742 |  0:03:58s\n",
      "epoch 35 | loss: 0.3739  | val_0_auc: 0.55745 | val_1_auc: 0.58064 |  0:04:05s\n",
      "epoch 36 | loss: 0.37154 | val_0_auc: 0.56085 | val_1_auc: 0.55327 |  0:04:11s\n",
      "epoch 37 | loss: 0.37494 | val_0_auc: 0.56963 | val_1_auc: 0.55102 |  0:04:18s\n",
      "epoch 38 | loss: 0.37526 | val_0_auc: 0.58315 | val_1_auc: 0.57288 |  0:04:24s\n",
      "epoch 39 | loss: 0.37508 | val_0_auc: 0.58057 | val_1_auc: 0.56042 |  0:04:31s\n",
      "epoch 40 | loss: 0.36661 | val_0_auc: 0.57202 | val_1_auc: 0.58875 |  0:04:39s\n",
      "epoch 41 | loss: 0.3674  | val_0_auc: 0.56819 | val_1_auc: 0.59012 |  0:04:47s\n",
      "epoch 42 | loss: 0.36693 | val_0_auc: 0.57057 | val_1_auc: 0.5839  |  0:04:56s\n",
      "epoch 43 | loss: 0.36347 | val_0_auc: 0.5824  | val_1_auc: 0.56845 |  0:05:04s\n",
      "epoch 44 | loss: 0.36637 | val_0_auc: 0.58233 | val_1_auc: 0.55469 |  0:05:12s\n",
      "epoch 45 | loss: 0.3624  | val_0_auc: 0.57306 | val_1_auc: 0.59845 |  0:05:19s\n",
      "epoch 46 | loss: 0.36108 | val_0_auc: 0.57532 | val_1_auc: 0.59591 |  0:05:28s\n",
      "epoch 47 | loss: 0.35857 | val_0_auc: 0.58028 | val_1_auc: 0.58569 |  0:05:36s\n",
      "epoch 48 | loss: 0.35566 | val_0_auc: 0.58235 | val_1_auc: 0.57202 |  0:05:44s\n",
      "epoch 49 | loss: 0.35795 | val_0_auc: 0.59944 | val_1_auc: 0.57108 |  0:05:51s\n",
      "epoch 50 | loss: 0.35746 | val_0_auc: 0.58319 | val_1_auc: 0.58629 |  0:05:58s\n",
      "epoch 51 | loss: 0.35414 | val_0_auc: 0.58908 | val_1_auc: 0.5837  |  0:06:04s\n",
      "epoch 52 | loss: 0.35286 | val_0_auc: 0.58993 | val_1_auc: 0.58806 |  0:06:10s\n",
      "epoch 53 | loss: 0.35212 | val_0_auc: 0.58934 | val_1_auc: 0.59871 |  0:06:17s\n",
      "epoch 54 | loss: 0.34993 | val_0_auc: 0.6104  | val_1_auc: 0.60097 |  0:06:25s\n",
      "epoch 55 | loss: 0.35056 | val_0_auc: 0.58745 | val_1_auc: 0.59493 |  0:06:33s\n",
      "epoch 56 | loss: 0.34887 | val_0_auc: 0.58975 | val_1_auc: 0.58907 |  0:06:39s\n",
      "epoch 57 | loss: 0.35162 | val_0_auc: 0.57916 | val_1_auc: 0.60166 |  0:06:47s\n",
      "epoch 58 | loss: 0.34806 | val_0_auc: 0.59138 | val_1_auc: 0.60691 |  0:06:53s\n",
      "epoch 59 | loss: 0.34675 | val_0_auc: 0.59003 | val_1_auc: 0.61095 |  0:07:00s\n",
      "epoch 60 | loss: 0.34524 | val_0_auc: 0.59989 | val_1_auc: 0.60424 |  0:07:06s\n",
      "epoch 61 | loss: 0.3451  | val_0_auc: 0.61349 | val_1_auc: 0.60859 |  0:07:12s\n",
      "epoch 62 | loss: 0.34537 | val_0_auc: 0.59951 | val_1_auc: 0.60044 |  0:07:18s\n",
      "epoch 63 | loss: 0.34588 | val_0_auc: 0.62049 | val_1_auc: 0.61285 |  0:07:25s\n",
      "epoch 64 | loss: 0.34379 | val_0_auc: 0.62032 | val_1_auc: 0.62287 |  0:07:31s\n",
      "epoch 65 | loss: 0.34439 | val_0_auc: 0.62595 | val_1_auc: 0.6228  |  0:07:38s\n",
      "epoch 66 | loss: 0.34324 | val_0_auc: 0.62329 | val_1_auc: 0.61741 |  0:07:44s\n",
      "epoch 67 | loss: 0.34256 | val_0_auc: 0.60971 | val_1_auc: 0.6108  |  0:07:51s\n",
      "epoch 68 | loss: 0.34456 | val_0_auc: 0.59943 | val_1_auc: 0.60982 |  0:07:57s\n",
      "epoch 69 | loss: 0.34095 | val_0_auc: 0.60935 | val_1_auc: 0.62845 |  0:08:04s\n",
      "epoch 70 | loss: 0.34312 | val_0_auc: 0.61917 | val_1_auc: 0.62443 |  0:08:10s\n",
      "epoch 71 | loss: 0.34342 | val_0_auc: 0.61215 | val_1_auc: 0.61513 |  0:08:17s\n",
      "epoch 72 | loss: 0.34005 | val_0_auc: 0.60466 | val_1_auc: 0.61832 |  0:08:23s\n",
      "epoch 73 | loss: 0.33885 | val_0_auc: 0.59792 | val_1_auc: 0.61304 |  0:08:31s\n",
      "epoch 74 | loss: 0.3364  | val_0_auc: 0.61454 | val_1_auc: 0.62384 |  0:08:38s\n",
      "epoch 75 | loss: 0.33762 | val_0_auc: 0.6045  | val_1_auc: 0.63262 |  0:08:45s\n",
      "epoch 76 | loss: 0.3371  | val_0_auc: 0.6178  | val_1_auc: 0.63081 |  0:08:53s\n",
      "epoch 77 | loss: 0.33485 | val_0_auc: 0.6108  | val_1_auc: 0.60979 |  0:09:00s\n",
      "epoch 78 | loss: 0.33741 | val_0_auc: 0.62055 | val_1_auc: 0.62139 |  0:09:08s\n",
      "epoch 79 | loss: 0.33826 | val_0_auc: 0.61227 | val_1_auc: 0.61374 |  0:09:15s\n",
      "epoch 80 | loss: 0.33618 | val_0_auc: 0.62242 | val_1_auc: 0.60713 |  0:09:22s\n",
      "epoch 81 | loss: 0.33317 | val_0_auc: 0.62034 | val_1_auc: 0.62497 |  0:09:29s\n",
      "epoch 82 | loss: 0.33617 | val_0_auc: 0.63389 | val_1_auc: 0.62106 |  0:09:37s\n",
      "epoch 83 | loss: 0.33489 | val_0_auc: 0.62496 | val_1_auc: 0.60352 |  0:09:44s\n",
      "epoch 84 | loss: 0.33616 | val_0_auc: 0.63427 | val_1_auc: 0.61383 |  0:09:51s\n",
      "epoch 85 | loss: 0.33484 | val_0_auc: 0.61932 | val_1_auc: 0.61878 |  0:09:58s\n",
      "epoch 86 | loss: 0.33646 | val_0_auc: 0.6274  | val_1_auc: 0.63197 |  0:10:06s\n",
      "epoch 87 | loss: 0.33502 | val_0_auc: 0.62785 | val_1_auc: 0.6239  |  0:10:14s\n",
      "epoch 88 | loss: 0.33728 | val_0_auc: 0.63205 | val_1_auc: 0.62328 |  0:10:21s\n",
      "epoch 89 | loss: 0.33461 | val_0_auc: 0.63382 | val_1_auc: 0.64263 |  0:10:28s\n",
      "epoch 90 | loss: 0.33714 | val_0_auc: 0.63144 | val_1_auc: 0.63076 |  0:10:36s\n",
      "epoch 91 | loss: 0.33306 | val_0_auc: 0.62585 | val_1_auc: 0.61698 |  0:10:42s\n",
      "epoch 92 | loss: 0.3336  | val_0_auc: 0.62266 | val_1_auc: 0.62216 |  0:10:48s\n",
      "epoch 93 | loss: 0.33377 | val_0_auc: 0.62665 | val_1_auc: 0.62919 |  0:10:54s\n",
      "epoch 94 | loss: 0.33095 | val_0_auc: 0.63222 | val_1_auc: 0.63152 |  0:11:00s\n",
      "epoch 95 | loss: 0.33392 | val_0_auc: 0.63607 | val_1_auc: 0.62292 |  0:11:07s\n",
      "epoch 96 | loss: 0.33135 | val_0_auc: 0.62473 | val_1_auc: 0.61032 |  0:11:13s\n",
      "epoch 97 | loss: 0.33254 | val_0_auc: 0.63204 | val_1_auc: 0.6145  |  0:11:19s\n",
      "epoch 98 | loss: 0.33331 | val_0_auc: 0.6239  | val_1_auc: 0.62424 |  0:11:26s\n",
      "epoch 99 | loss: 0.33324 | val_0_auc: 0.64095 | val_1_auc: 0.61371 |  0:11:33s\n",
      "epoch 100| loss: 0.33245 | val_0_auc: 0.64444 | val_1_auc: 0.61675 |  0:11:39s\n",
      "epoch 101| loss: 0.33306 | val_0_auc: 0.63691 | val_1_auc: 0.61837 |  0:11:46s\n",
      "epoch 102| loss: 0.33089 | val_0_auc: 0.64349 | val_1_auc: 0.63495 |  0:11:52s\n",
      "epoch 103| loss: 0.33211 | val_0_auc: 0.64809 | val_1_auc: 0.62352 |  0:11:59s\n",
      "epoch 104| loss: 0.33054 | val_0_auc: 0.63754 | val_1_auc: 0.62303 |  0:12:06s\n",
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 89 and best_val_1_auc = 0.64263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:46:27,112] Trial 5 finished with value: 0.6426260809408508 and parameters: {'n_d': 22, 'n_a': 61, 'n_steps': 7, 'gamma': 1.4232157781007246, 'lambda_sparse': 1.65924251041474e-06, 'lr': 0.0010775570923226984}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.17557 | val_0_auc: 0.46987 | val_1_auc: 0.48162 |  0:00:04s\n",
      "epoch 1  | loss: 0.97928 | val_0_auc: 0.47657 | val_1_auc: 0.5126  |  0:00:08s\n",
      "epoch 2  | loss: 0.83794 | val_0_auc: 0.46926 | val_1_auc: 0.50221 |  0:00:12s\n",
      "epoch 3  | loss: 0.72587 | val_0_auc: 0.46248 | val_1_auc: 0.49766 |  0:00:16s\n",
      "epoch 4  | loss: 0.64229 | val_0_auc: 0.46126 | val_1_auc: 0.50426 |  0:00:20s\n",
      "epoch 5  | loss: 0.58524 | val_0_auc: 0.45975 | val_1_auc: 0.4911  |  0:00:24s\n",
      "epoch 6  | loss: 0.55912 | val_0_auc: 0.45211 | val_1_auc: 0.49099 |  0:00:28s\n",
      "epoch 7  | loss: 0.53195 | val_0_auc: 0.45145 | val_1_auc: 0.47377 |  0:00:32s\n",
      "epoch 8  | loss: 0.50082 | val_0_auc: 0.45292 | val_1_auc: 0.47927 |  0:00:36s\n",
      "epoch 9  | loss: 0.49083 | val_0_auc: 0.45921 | val_1_auc: 0.47746 |  0:00:40s\n",
      "epoch 10 | loss: 0.4774  | val_0_auc: 0.4525  | val_1_auc: 0.4748  |  0:00:44s\n",
      "epoch 11 | loss: 0.47982 | val_0_auc: 0.46347 | val_1_auc: 0.47943 |  0:00:49s\n",
      "epoch 12 | loss: 0.46557 | val_0_auc: 0.4641  | val_1_auc: 0.48479 |  0:00:53s\n",
      "epoch 13 | loss: 0.46642 | val_0_auc: 0.46861 | val_1_auc: 0.48082 |  0:00:57s\n",
      "epoch 14 | loss: 0.4544  | val_0_auc: 0.47253 | val_1_auc: 0.47731 |  0:01:01s\n",
      "epoch 15 | loss: 0.45772 | val_0_auc: 0.47745 | val_1_auc: 0.48088 |  0:01:05s\n",
      "epoch 16 | loss: 0.45162 | val_0_auc: 0.47099 | val_1_auc: 0.48639 |  0:01:09s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 1 and best_val_1_auc = 0.5126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:47:38,160] Trial 6 finished with value: 0.5125963334486336 and parameters: {'n_d': 43, 'n_a': 42, 'n_steps': 4, 'gamma': 1.1367057463125578, 'lambda_sparse': 3.086496160997109e-06, 'lr': 0.0003332846333511127}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.73305 | val_0_auc: 0.53472 | val_1_auc: 0.58359 |  0:00:03s\n",
      "epoch 1  | loss: 1.34997 | val_0_auc: 0.55118 | val_1_auc: 0.55894 |  0:00:06s\n",
      "epoch 2  | loss: 1.12634 | val_0_auc: 0.54223 | val_1_auc: 0.53962 |  0:00:09s\n",
      "epoch 3  | loss: 0.90172 | val_0_auc: 0.54398 | val_1_auc: 0.55616 |  0:00:13s\n",
      "epoch 4  | loss: 0.75966 | val_0_auc: 0.53977 | val_1_auc: 0.54154 |  0:00:16s\n",
      "epoch 5  | loss: 0.62868 | val_0_auc: 0.53797 | val_1_auc: 0.54336 |  0:00:20s\n",
      "epoch 6  | loss: 0.57195 | val_0_auc: 0.52578 | val_1_auc: 0.53517 |  0:00:23s\n",
      "epoch 7  | loss: 0.53395 | val_0_auc: 0.53786 | val_1_auc: 0.53474 |  0:00:26s\n",
      "epoch 8  | loss: 0.50724 | val_0_auc: 0.52251 | val_1_auc: 0.53328 |  0:00:30s\n",
      "epoch 9  | loss: 0.49518 | val_0_auc: 0.52444 | val_1_auc: 0.5184  |  0:00:35s\n",
      "epoch 10 | loss: 0.4776  | val_0_auc: 0.51925 | val_1_auc: 0.51932 |  0:00:39s\n",
      "epoch 11 | loss: 0.47402 | val_0_auc: 0.52099 | val_1_auc: 0.50292 |  0:00:44s\n",
      "epoch 12 | loss: 0.4701  | val_0_auc: 0.52996 | val_1_auc: 0.51208 |  0:00:47s\n",
      "epoch 13 | loss: 0.46427 | val_0_auc: 0.52633 | val_1_auc: 0.51688 |  0:00:51s\n",
      "epoch 14 | loss: 0.45474 | val_0_auc: 0.52518 | val_1_auc: 0.51687 |  0:00:54s\n",
      "epoch 15 | loss: 0.45122 | val_0_auc: 0.52464 | val_1_auc: 0.5263  |  0:00:57s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_1_auc = 0.58359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:48:37,215] Trial 7 finished with value: 0.5835897613282601 and parameters: {'n_d': 34, 'n_a': 24, 'n_steps': 4, 'gamma': 1.6364969604817057, 'lambda_sparse': 3.326498305926343e-05, 'lr': 0.0005928873097285178}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.31393 | val_0_auc: 0.53363 | val_1_auc: 0.54429 |  0:00:02s\n",
      "epoch 1  | loss: 0.85697 | val_0_auc: 0.50911 | val_1_auc: 0.49692 |  0:00:06s\n",
      "epoch 2  | loss: 0.59248 | val_0_auc: 0.49452 | val_1_auc: 0.47641 |  0:00:09s\n",
      "epoch 3  | loss: 0.47238 | val_0_auc: 0.48821 | val_1_auc: 0.45412 |  0:00:12s\n",
      "epoch 4  | loss: 0.42719 | val_0_auc: 0.48793 | val_1_auc: 0.45772 |  0:00:15s\n",
      "epoch 5  | loss: 0.41027 | val_0_auc: 0.50094 | val_1_auc: 0.44676 |  0:00:18s\n",
      "epoch 6  | loss: 0.40023 | val_0_auc: 0.50681 | val_1_auc: 0.46866 |  0:00:22s\n",
      "epoch 7  | loss: 0.39485 | val_0_auc: 0.52    | val_1_auc: 0.49836 |  0:00:25s\n",
      "epoch 8  | loss: 0.38771 | val_0_auc: 0.5238  | val_1_auc: 0.50913 |  0:00:28s\n",
      "epoch 9  | loss: 0.37973 | val_0_auc: 0.53427 | val_1_auc: 0.52079 |  0:00:31s\n",
      "epoch 10 | loss: 0.37376 | val_0_auc: 0.54959 | val_1_auc: 0.51865 |  0:00:34s\n",
      "epoch 11 | loss: 0.36777 | val_0_auc: 0.55072 | val_1_auc: 0.51644 |  0:00:37s\n",
      "epoch 12 | loss: 0.36717 | val_0_auc: 0.56391 | val_1_auc: 0.52641 |  0:00:40s\n",
      "epoch 13 | loss: 0.36874 | val_0_auc: 0.57529 | val_1_auc: 0.52457 |  0:00:44s\n",
      "epoch 14 | loss: 0.36413 | val_0_auc: 0.56738 | val_1_auc: 0.53552 |  0:00:47s\n",
      "epoch 15 | loss: 0.3586  | val_0_auc: 0.57304 | val_1_auc: 0.54103 |  0:00:50s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_1_auc = 0.54429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:49:28,933] Trial 8 finished with value: 0.5442947077135939 and parameters: {'n_d': 33, 'n_a': 33, 'n_steps': 3, 'gamma': 1.799862097558768, 'lambda_sparse': 1.5053923733460144e-06, 'lr': 0.0010552329637937155}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.28274 | val_0_auc: 0.55905 | val_1_auc: 0.54679 |  0:00:05s\n",
      "epoch 1  | loss: 1.06056 | val_0_auc: 0.56118 | val_1_auc: 0.53606 |  0:00:10s\n",
      "epoch 2  | loss: 0.90078 | val_0_auc: 0.55042 | val_1_auc: 0.52339 |  0:00:16s\n",
      "epoch 3  | loss: 0.76564 | val_0_auc: 0.54627 | val_1_auc: 0.49988 |  0:00:22s\n",
      "epoch 4  | loss: 0.66033 | val_0_auc: 0.54618 | val_1_auc: 0.51927 |  0:00:28s\n",
      "epoch 5  | loss: 0.58334 | val_0_auc: 0.54467 | val_1_auc: 0.51446 |  0:00:33s\n",
      "epoch 6  | loss: 0.5479  | val_0_auc: 0.53539 | val_1_auc: 0.51837 |  0:00:38s\n",
      "epoch 7  | loss: 0.51302 | val_0_auc: 0.5411  | val_1_auc: 0.52725 |  0:00:44s\n",
      "epoch 8  | loss: 0.49575 | val_0_auc: 0.52604 | val_1_auc: 0.52078 |  0:00:49s\n",
      "epoch 9  | loss: 0.48643 | val_0_auc: 0.52872 | val_1_auc: 0.52919 |  0:00:54s\n",
      "epoch 10 | loss: 0.47749 | val_0_auc: 0.52884 | val_1_auc: 0.52469 |  0:00:59s\n",
      "epoch 11 | loss: 0.47466 | val_0_auc: 0.52137 | val_1_auc: 0.5306  |  0:01:05s\n",
      "epoch 12 | loss: 0.47541 | val_0_auc: 0.53291 | val_1_auc: 0.53375 |  0:01:10s\n",
      "epoch 13 | loss: 0.46565 | val_0_auc: 0.53167 | val_1_auc: 0.53853 |  0:01:18s\n",
      "epoch 14 | loss: 0.46386 | val_0_auc: 0.54086 | val_1_auc: 0.53762 |  0:01:26s\n",
      "epoch 15 | loss: 0.45212 | val_0_auc: 0.538   | val_1_auc: 0.53545 |  0:01:33s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_1_auc = 0.54679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:51:04,552] Trial 9 finished with value: 0.5467872708405396 and parameters: {'n_d': 61, 'n_a': 50, 'n_steps': 5, 'gamma': 1.0366436657295581, 'lambda_sparse': 0.0006841310978017553, 'lr': 0.0002561252649535893}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.05835 | val_0_auc: 0.51809 | val_1_auc: 0.49677 |  0:00:06s\n",
      "epoch 1  | loss: 0.62528 | val_0_auc: 0.49417 | val_1_auc: 0.4614  |  0:00:12s\n",
      "epoch 2  | loss: 0.45742 | val_0_auc: 0.46625 | val_1_auc: 0.45431 |  0:00:20s\n",
      "epoch 3  | loss: 0.43196 | val_0_auc: 0.49612 | val_1_auc: 0.49177 |  0:00:27s\n",
      "epoch 4  | loss: 0.41919 | val_0_auc: 0.48726 | val_1_auc: 0.44982 |  0:00:33s\n",
      "epoch 5  | loss: 0.40287 | val_0_auc: 0.49652 | val_1_auc: 0.47713 |  0:00:39s\n",
      "epoch 6  | loss: 0.39266 | val_0_auc: 0.50436 | val_1_auc: 0.46675 |  0:00:45s\n",
      "epoch 7  | loss: 0.38717 | val_0_auc: 0.52814 | val_1_auc: 0.49954 |  0:00:51s\n",
      "epoch 8  | loss: 0.37446 | val_0_auc: 0.51208 | val_1_auc: 0.48571 |  0:00:57s\n",
      "epoch 9  | loss: 0.37091 | val_0_auc: 0.52715 | val_1_auc: 0.52586 |  0:01:03s\n",
      "epoch 10 | loss: 0.36685 | val_0_auc: 0.54028 | val_1_auc: 0.52378 |  0:01:09s\n",
      "epoch 11 | loss: 0.36345 | val_0_auc: 0.54167 | val_1_auc: 0.53047 |  0:01:15s\n",
      "epoch 12 | loss: 0.35758 | val_0_auc: 0.53546 | val_1_auc: 0.54943 |  0:01:20s\n",
      "epoch 13 | loss: 0.35608 | val_0_auc: 0.55204 | val_1_auc: 0.53828 |  0:01:25s\n",
      "epoch 14 | loss: 0.35461 | val_0_auc: 0.54256 | val_1_auc: 0.52599 |  0:01:31s\n",
      "epoch 15 | loss: 0.35203 | val_0_auc: 0.55023 | val_1_auc: 0.53123 |  0:01:36s\n",
      "epoch 16 | loss: 0.35329 | val_0_auc: 0.55367 | val_1_auc: 0.53623 |  0:01:41s\n",
      "epoch 17 | loss: 0.351   | val_0_auc: 0.55227 | val_1_auc: 0.52617 |  0:01:48s\n",
      "epoch 18 | loss: 0.34868 | val_0_auc: 0.55882 | val_1_auc: 0.5256  |  0:01:55s\n",
      "epoch 19 | loss: 0.34664 | val_0_auc: 0.57327 | val_1_auc: 0.52756 |  0:02:02s\n",
      "epoch 20 | loss: 0.34802 | val_0_auc: 0.55883 | val_1_auc: 0.53974 |  0:02:08s\n",
      "epoch 21 | loss: 0.34669 | val_0_auc: 0.57114 | val_1_auc: 0.55735 |  0:02:15s\n",
      "epoch 22 | loss: 0.3458  | val_0_auc: 0.58437 | val_1_auc: 0.57364 |  0:02:20s\n",
      "epoch 23 | loss: 0.3438  | val_0_auc: 0.57825 | val_1_auc: 0.58361 |  0:02:27s\n",
      "epoch 24 | loss: 0.34384 | val_0_auc: 0.5762  | val_1_auc: 0.59072 |  0:02:33s\n",
      "epoch 25 | loss: 0.34334 | val_0_auc: 0.5674  | val_1_auc: 0.588   |  0:02:40s\n",
      "epoch 26 | loss: 0.34267 | val_0_auc: 0.57331 | val_1_auc: 0.60596 |  0:02:46s\n",
      "epoch 27 | loss: 0.34024 | val_0_auc: 0.57662 | val_1_auc: 0.60604 |  0:02:51s\n",
      "epoch 28 | loss: 0.33921 | val_0_auc: 0.57934 | val_1_auc: 0.61969 |  0:02:57s\n",
      "epoch 29 | loss: 0.33958 | val_0_auc: 0.58462 | val_1_auc: 0.61163 |  0:03:03s\n",
      "epoch 30 | loss: 0.3382  | val_0_auc: 0.60472 | val_1_auc: 0.61026 |  0:03:09s\n",
      "epoch 31 | loss: 0.33872 | val_0_auc: 0.60216 | val_1_auc: 0.59407 |  0:03:16s\n",
      "epoch 32 | loss: 0.33986 | val_0_auc: 0.60079 | val_1_auc: 0.60026 |  0:03:23s\n",
      "epoch 33 | loss: 0.33921 | val_0_auc: 0.58736 | val_1_auc: 0.6055  |  0:03:29s\n",
      "epoch 34 | loss: 0.33815 | val_0_auc: 0.59049 | val_1_auc: 0.58295 |  0:03:34s\n",
      "epoch 35 | loss: 0.33496 | val_0_auc: 0.60529 | val_1_auc: 0.60817 |  0:03:39s\n",
      "epoch 36 | loss: 0.33595 | val_0_auc: 0.59826 | val_1_auc: 0.59948 |  0:03:45s\n",
      "epoch 37 | loss: 0.33751 | val_0_auc: 0.60311 | val_1_auc: 0.59562 |  0:03:50s\n",
      "epoch 38 | loss: 0.33557 | val_0_auc: 0.60279 | val_1_auc: 0.5882  |  0:03:56s\n",
      "epoch 39 | loss: 0.33515 | val_0_auc: 0.60776 | val_1_auc: 0.57775 |  0:04:01s\n",
      "epoch 40 | loss: 0.3348  | val_0_auc: 0.60398 | val_1_auc: 0.58214 |  0:04:06s\n",
      "epoch 41 | loss: 0.33537 | val_0_auc: 0.60687 | val_1_auc: 0.60063 |  0:04:12s\n",
      "epoch 42 | loss: 0.33484 | val_0_auc: 0.61022 | val_1_auc: 0.5958  |  0:04:18s\n",
      "epoch 43 | loss: 0.33562 | val_0_auc: 0.6224  | val_1_auc: 0.61214 |  0:04:24s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 28 and best_val_1_auc = 0.61969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 12:55:30,879] Trial 10 finished with value: 0.6196886890349359 and parameters: {'n_d': 16, 'n_a': 18, 'n_steps': 6, 'gamma': 2.141010279751762, 'lambda_sparse': 7.948592149582204e-06, 'lr': 0.002341308981984162}. Best is trial 2 with value: 0.6434050501556555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.58556 | val_0_auc: 0.44006 | val_1_auc: 0.51237 |  0:00:06s\n",
      "epoch 1  | loss: 0.49144 | val_0_auc: 0.44342 | val_1_auc: 0.48188 |  0:00:14s\n",
      "epoch 2  | loss: 0.4576  | val_0_auc: 0.47583 | val_1_auc: 0.49861 |  0:00:21s\n",
      "epoch 3  | loss: 0.43794 | val_0_auc: 0.48178 | val_1_auc: 0.51205 |  0:00:31s\n",
      "epoch 4  | loss: 0.42572 | val_0_auc: 0.50113 | val_1_auc: 0.50298 |  0:00:37s\n",
      "epoch 5  | loss: 0.40305 | val_0_auc: 0.51077 | val_1_auc: 0.51985 |  0:00:43s\n",
      "epoch 6  | loss: 0.39311 | val_0_auc: 0.52134 | val_1_auc: 0.54421 |  0:00:49s\n",
      "epoch 7  | loss: 0.38591 | val_0_auc: 0.52387 | val_1_auc: 0.53626 |  0:00:55s\n",
      "epoch 8  | loss: 0.37654 | val_0_auc: 0.52954 | val_1_auc: 0.54315 |  0:01:02s\n",
      "epoch 9  | loss: 0.37128 | val_0_auc: 0.54221 | val_1_auc: 0.57329 |  0:01:10s\n",
      "epoch 10 | loss: 0.37152 | val_0_auc: 0.5455  | val_1_auc: 0.56339 |  0:01:18s\n",
      "epoch 11 | loss: 0.35831 | val_0_auc: 0.57279 | val_1_auc: 0.57128 |  0:01:26s\n",
      "epoch 12 | loss: 0.35633 | val_0_auc: 0.58376 | val_1_auc: 0.57226 |  0:01:34s\n",
      "epoch 13 | loss: 0.35491 | val_0_auc: 0.59905 | val_1_auc: 0.57292 |  0:01:42s\n",
      "epoch 14 | loss: 0.35099 | val_0_auc: 0.60793 | val_1_auc: 0.55615 |  0:01:49s\n",
      "epoch 15 | loss: 0.3505  | val_0_auc: 0.59501 | val_1_auc: 0.56763 |  0:01:57s\n",
      "epoch 16 | loss: 0.34723 | val_0_auc: 0.59568 | val_1_auc: 0.59751 |  0:02:04s\n",
      "epoch 17 | loss: 0.34472 | val_0_auc: 0.59475 | val_1_auc: 0.57348 |  0:02:11s\n",
      "epoch 18 | loss: 0.33895 | val_0_auc: 0.59879 | val_1_auc: 0.56993 |  0:02:18s\n",
      "epoch 19 | loss: 0.33901 | val_0_auc: 0.62366 | val_1_auc: 0.59488 |  0:02:25s\n",
      "epoch 20 | loss: 0.33929 | val_0_auc: 0.59697 | val_1_auc: 0.59389 |  0:02:33s\n",
      "epoch 21 | loss: 0.33754 | val_0_auc: 0.61644 | val_1_auc: 0.5925  |  0:02:41s\n",
      "epoch 22 | loss: 0.3364  | val_0_auc: 0.62771 | val_1_auc: 0.60695 |  0:02:49s\n",
      "epoch 23 | loss: 0.33564 | val_0_auc: 0.62457 | val_1_auc: 0.62174 |  0:02:57s\n",
      "epoch 24 | loss: 0.3347  | val_0_auc: 0.62552 | val_1_auc: 0.60778 |  0:03:04s\n",
      "epoch 25 | loss: 0.33503 | val_0_auc: 0.6242  | val_1_auc: 0.62628 |  0:03:10s\n",
      "epoch 26 | loss: 0.33345 | val_0_auc: 0.63288 | val_1_auc: 0.61495 |  0:03:18s\n",
      "epoch 27 | loss: 0.33364 | val_0_auc: 0.64853 | val_1_auc: 0.62731 |  0:03:25s\n",
      "epoch 28 | loss: 0.33349 | val_0_auc: 0.64754 | val_1_auc: 0.61832 |  0:03:32s\n",
      "epoch 29 | loss: 0.33352 | val_0_auc: 0.64057 | val_1_auc: 0.63994 |  0:03:40s\n",
      "epoch 30 | loss: 0.33254 | val_0_auc: 0.63636 | val_1_auc: 0.62397 |  0:03:48s\n",
      "epoch 31 | loss: 0.33255 | val_0_auc: 0.64629 | val_1_auc: 0.62351 |  0:03:56s\n",
      "epoch 32 | loss: 0.33174 | val_0_auc: 0.6535  | val_1_auc: 0.61724 |  0:04:02s\n",
      "epoch 33 | loss: 0.32905 | val_0_auc: 0.64186 | val_1_auc: 0.61724 |  0:04:08s\n",
      "epoch 34 | loss: 0.32997 | val_0_auc: 0.63888 | val_1_auc: 0.62937 |  0:04:15s\n",
      "epoch 35 | loss: 0.32747 | val_0_auc: 0.63128 | val_1_auc: 0.62385 |  0:04:22s\n",
      "epoch 36 | loss: 0.32851 | val_0_auc: 0.64903 | val_1_auc: 0.64041 |  0:04:30s\n",
      "epoch 37 | loss: 0.32664 | val_0_auc: 0.66538 | val_1_auc: 0.64341 |  0:04:37s\n",
      "epoch 38 | loss: 0.32793 | val_0_auc: 0.65261 | val_1_auc: 0.62814 |  0:04:45s\n",
      "epoch 39 | loss: 0.32721 | val_0_auc: 0.65645 | val_1_auc: 0.63744 |  0:04:53s\n",
      "epoch 40 | loss: 0.32807 | val_0_auc: 0.66049 | val_1_auc: 0.63651 |  0:05:00s\n",
      "epoch 41 | loss: 0.32832 | val_0_auc: 0.65886 | val_1_auc: 0.63256 |  0:05:08s\n",
      "epoch 42 | loss: 0.32804 | val_0_auc: 0.66255 | val_1_auc: 0.63038 |  0:05:15s\n",
      "epoch 43 | loss: 0.32888 | val_0_auc: 0.66854 | val_1_auc: 0.63085 |  0:05:22s\n",
      "epoch 44 | loss: 0.32683 | val_0_auc: 0.66227 | val_1_auc: 0.6307  |  0:05:28s\n",
      "epoch 45 | loss: 0.32778 | val_0_auc: 0.66302 | val_1_auc: 0.63623 |  0:05:35s\n",
      "epoch 46 | loss: 0.32879 | val_0_auc: 0.65239 | val_1_auc: 0.63022 |  0:05:42s\n",
      "epoch 47 | loss: 0.32594 | val_0_auc: 0.65577 | val_1_auc: 0.62367 |  0:05:49s\n",
      "epoch 48 | loss: 0.32574 | val_0_auc: 0.65629 | val_1_auc: 0.63355 |  0:05:57s\n",
      "epoch 49 | loss: 0.32615 | val_0_auc: 0.6633  | val_1_auc: 0.62594 |  0:06:05s\n",
      "epoch 50 | loss: 0.32725 | val_0_auc: 0.66745 | val_1_auc: 0.62231 |  0:06:13s\n",
      "epoch 51 | loss: 0.32703 | val_0_auc: 0.65963 | val_1_auc: 0.61723 |  0:06:21s\n",
      "epoch 52 | loss: 0.32599 | val_0_auc: 0.65628 | val_1_auc: 0.60531 |  0:06:28s\n",
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 37 and best_val_1_auc = 0.64341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:02:02,881] Trial 11 finished with value: 0.6434140435835352 and parameters: {'n_d': 25, 'n_a': 64, 'n_steps': 6, 'gamma': 1.3629987172793752, 'lambda_sparse': 7.149030920559479e-06, 'lr': 0.002144784006775338}. Best is trial 11 with value: 0.6434140435835352.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.79447 | val_0_auc: 0.49455 | val_1_auc: 0.48049 |  0:00:08s\n",
      "epoch 1  | loss: 0.52129 | val_0_auc: 0.47414 | val_1_auc: 0.4872  |  0:00:16s\n",
      "epoch 2  | loss: 0.49978 | val_0_auc: 0.48195 | val_1_auc: 0.49418 |  0:00:23s\n",
      "epoch 3  | loss: 0.44872 | val_0_auc: 0.51582 | val_1_auc: 0.48819 |  0:00:29s\n",
      "epoch 4  | loss: 0.43253 | val_0_auc: 0.51274 | val_1_auc: 0.50661 |  0:00:35s\n",
      "epoch 5  | loss: 0.40936 | val_0_auc: 0.53038 | val_1_auc: 0.52494 |  0:00:42s\n",
      "epoch 6  | loss: 0.40107 | val_0_auc: 0.53718 | val_1_auc: 0.5373  |  0:00:48s\n",
      "epoch 7  | loss: 0.39404 | val_0_auc: 0.53944 | val_1_auc: 0.54826 |  0:00:55s\n",
      "epoch 8  | loss: 0.38355 | val_0_auc: 0.55887 | val_1_auc: 0.54857 |  0:01:02s\n",
      "epoch 9  | loss: 0.37565 | val_0_auc: 0.57375 | val_1_auc: 0.5631  |  0:01:11s\n",
      "epoch 10 | loss: 0.3665  | val_0_auc: 0.55887 | val_1_auc: 0.55778 |  0:01:18s\n",
      "epoch 11 | loss: 0.36684 | val_0_auc: 0.55314 | val_1_auc: 0.57805 |  0:01:26s\n",
      "epoch 12 | loss: 0.36113 | val_0_auc: 0.57548 | val_1_auc: 0.55894 |  0:01:34s\n",
      "epoch 13 | loss: 0.35754 | val_0_auc: 0.59307 | val_1_auc: 0.56314 |  0:01:42s\n",
      "epoch 14 | loss: 0.35482 | val_0_auc: 0.58845 | val_1_auc: 0.60585 |  0:01:49s\n",
      "epoch 15 | loss: 0.3542  | val_0_auc: 0.58414 | val_1_auc: 0.57882 |  0:01:57s\n",
      "epoch 16 | loss: 0.35132 | val_0_auc: 0.59135 | val_1_auc: 0.57654 |  0:02:04s\n",
      "epoch 17 | loss: 0.34944 | val_0_auc: 0.58259 | val_1_auc: 0.58122 |  0:02:11s\n",
      "epoch 18 | loss: 0.34448 | val_0_auc: 0.59201 | val_1_auc: 0.61167 |  0:02:18s\n",
      "epoch 19 | loss: 0.34798 | val_0_auc: 0.60427 | val_1_auc: 0.58939 |  0:02:24s\n",
      "epoch 20 | loss: 0.3451  | val_0_auc: 0.5952  | val_1_auc: 0.56519 |  0:02:31s\n",
      "epoch 21 | loss: 0.34152 | val_0_auc: 0.59506 | val_1_auc: 0.59456 |  0:02:37s\n",
      "epoch 22 | loss: 0.34397 | val_0_auc: 0.60143 | val_1_auc: 0.57529 |  0:02:43s\n",
      "epoch 23 | loss: 0.33991 | val_0_auc: 0.62031 | val_1_auc: 0.5889  |  0:02:50s\n",
      "epoch 24 | loss: 0.33837 | val_0_auc: 0.61373 | val_1_auc: 0.60758 |  0:02:57s\n",
      "epoch 25 | loss: 0.33925 | val_0_auc: 0.6218  | val_1_auc: 0.60709 |  0:03:04s\n",
      "epoch 26 | loss: 0.33497 | val_0_auc: 0.6269  | val_1_auc: 0.61315 |  0:03:11s\n",
      "epoch 27 | loss: 0.33787 | val_0_auc: 0.61766 | val_1_auc: 0.62472 |  0:03:17s\n",
      "epoch 28 | loss: 0.33686 | val_0_auc: 0.61183 | val_1_auc: 0.61583 |  0:03:24s\n",
      "epoch 29 | loss: 0.33617 | val_0_auc: 0.61104 | val_1_auc: 0.61083 |  0:03:30s\n",
      "epoch 30 | loss: 0.33329 | val_0_auc: 0.61444 | val_1_auc: 0.62927 |  0:03:36s\n",
      "epoch 31 | loss: 0.33547 | val_0_auc: 0.62875 | val_1_auc: 0.61842 |  0:03:43s\n",
      "epoch 32 | loss: 0.33319 | val_0_auc: 0.61862 | val_1_auc: 0.61765 |  0:03:50s\n",
      "epoch 33 | loss: 0.33428 | val_0_auc: 0.62904 | val_1_auc: 0.64663 |  0:03:57s\n",
      "epoch 34 | loss: 0.33233 | val_0_auc: 0.63261 | val_1_auc: 0.61783 |  0:04:04s\n",
      "epoch 35 | loss: 0.33265 | val_0_auc: 0.62709 | val_1_auc: 0.59262 |  0:04:11s\n",
      "epoch 36 | loss: 0.33362 | val_0_auc: 0.61802 | val_1_auc: 0.6015  |  0:04:17s\n",
      "epoch 37 | loss: 0.33302 | val_0_auc: 0.64563 | val_1_auc: 0.60381 |  0:04:23s\n",
      "epoch 38 | loss: 0.33371 | val_0_auc: 0.63745 | val_1_auc: 0.6104  |  0:04:30s\n",
      "epoch 39 | loss: 0.33226 | val_0_auc: 0.64512 | val_1_auc: 0.61601 |  0:04:37s\n",
      "epoch 40 | loss: 0.33123 | val_0_auc: 0.63549 | val_1_auc: 0.63185 |  0:04:44s\n",
      "epoch 41 | loss: 0.33255 | val_0_auc: 0.62627 | val_1_auc: 0.60621 |  0:04:51s\n",
      "epoch 42 | loss: 0.3325  | val_0_auc: 0.6464  | val_1_auc: 0.62137 |  0:04:58s\n",
      "epoch 43 | loss: 0.33132 | val_0_auc: 0.64255 | val_1_auc: 0.62286 |  0:05:06s\n",
      "epoch 44 | loss: 0.33204 | val_0_auc: 0.62944 | val_1_auc: 0.61765 |  0:05:14s\n",
      "epoch 45 | loss: 0.33118 | val_0_auc: 0.63714 | val_1_auc: 0.61696 |  0:05:22s\n",
      "epoch 46 | loss: 0.32944 | val_0_auc: 0.63435 | val_1_auc: 0.62982 |  0:05:29s\n",
      "epoch 47 | loss: 0.33059 | val_0_auc: 0.64067 | val_1_auc: 0.63381 |  0:05:36s\n",
      "epoch 48 | loss: 0.33169 | val_0_auc: 0.63866 | val_1_auc: 0.6205  |  0:05:43s\n",
      "\n",
      "Early stopping occurred at epoch 48 with best_epoch = 33 and best_val_1_auc = 0.64663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:07:48,376] Trial 12 finished with value: 0.6466329989622968 and parameters: {'n_d': 31, 'n_a': 51, 'n_steps': 6, 'gamma': 1.3244349826835047, 'lambda_sparse': 7.035728348049984e-06, 'lr': 0.0023959055297062917}. Best is trial 12 with value: 0.6466329989622968.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.549   | val_0_auc: 0.49617 | val_1_auc: 0.45857 |  0:00:06s\n",
      "epoch 1  | loss: 0.49187 | val_0_auc: 0.48206 | val_1_auc: 0.48243 |  0:00:13s\n",
      "epoch 2  | loss: 0.44011 | val_0_auc: 0.52413 | val_1_auc: 0.49269 |  0:00:19s\n",
      "epoch 3  | loss: 0.41684 | val_0_auc: 0.51946 | val_1_auc: 0.51923 |  0:00:26s\n",
      "epoch 4  | loss: 0.39445 | val_0_auc: 0.53687 | val_1_auc: 0.53194 |  0:00:33s\n",
      "epoch 5  | loss: 0.3788  | val_0_auc: 0.52933 | val_1_auc: 0.53463 |  0:00:39s\n",
      "epoch 6  | loss: 0.36924 | val_0_auc: 0.55633 | val_1_auc: 0.55813 |  0:00:46s\n",
      "epoch 7  | loss: 0.36646 | val_0_auc: 0.57599 | val_1_auc: 0.56882 |  0:00:54s\n",
      "epoch 8  | loss: 0.35931 | val_0_auc: 0.55515 | val_1_auc: 0.56273 |  0:01:00s\n",
      "epoch 9  | loss: 0.35434 | val_0_auc: 0.60159 | val_1_auc: 0.5709  |  0:01:07s\n",
      "epoch 10 | loss: 0.35064 | val_0_auc: 0.61414 | val_1_auc: 0.60513 |  0:01:14s\n",
      "epoch 11 | loss: 0.34507 | val_0_auc: 0.58691 | val_1_auc: 0.57651 |  0:01:21s\n",
      "epoch 12 | loss: 0.3444  | val_0_auc: 0.57958 | val_1_auc: 0.59312 |  0:01:28s\n",
      "epoch 13 | loss: 0.34039 | val_0_auc: 0.61221 | val_1_auc: 0.61389 |  0:01:35s\n",
      "epoch 14 | loss: 0.3395  | val_0_auc: 0.61043 | val_1_auc: 0.61279 |  0:01:42s\n",
      "epoch 15 | loss: 0.34139 | val_0_auc: 0.60763 | val_1_auc: 0.60137 |  0:01:49s\n",
      "epoch 16 | loss: 0.33992 | val_0_auc: 0.61579 | val_1_auc: 0.62898 |  0:01:55s\n",
      "epoch 17 | loss: 0.33896 | val_0_auc: 0.61885 | val_1_auc: 0.62723 |  0:02:03s\n",
      "epoch 18 | loss: 0.3363  | val_0_auc: 0.62998 | val_1_auc: 0.62008 |  0:02:10s\n",
      "epoch 19 | loss: 0.33435 | val_0_auc: 0.6275  | val_1_auc: 0.63038 |  0:02:17s\n",
      "epoch 20 | loss: 0.3326  | val_0_auc: 0.63468 | val_1_auc: 0.62128 |  0:02:25s\n",
      "epoch 21 | loss: 0.33256 | val_0_auc: 0.63966 | val_1_auc: 0.62415 |  0:02:33s\n",
      "epoch 22 | loss: 0.33127 | val_0_auc: 0.63724 | val_1_auc: 0.62496 |  0:02:40s\n",
      "epoch 23 | loss: 0.3307  | val_0_auc: 0.63249 | val_1_auc: 0.62131 |  0:02:47s\n",
      "epoch 24 | loss: 0.32936 | val_0_auc: 0.64496 | val_1_auc: 0.6435  |  0:02:54s\n",
      "epoch 25 | loss: 0.32907 | val_0_auc: 0.64193 | val_1_auc: 0.62265 |  0:03:02s\n",
      "epoch 26 | loss: 0.33022 | val_0_auc: 0.65369 | val_1_auc: 0.62786 |  0:03:08s\n",
      "epoch 27 | loss: 0.33029 | val_0_auc: 0.6539  | val_1_auc: 0.64325 |  0:03:15s\n",
      "epoch 28 | loss: 0.3282  | val_0_auc: 0.64707 | val_1_auc: 0.63456 |  0:03:22s\n",
      "epoch 29 | loss: 0.33077 | val_0_auc: 0.64546 | val_1_auc: 0.63573 |  0:03:30s\n",
      "epoch 30 | loss: 0.32964 | val_0_auc: 0.64775 | val_1_auc: 0.63624 |  0:03:38s\n",
      "epoch 31 | loss: 0.32787 | val_0_auc: 0.64765 | val_1_auc: 0.63163 |  0:03:46s\n",
      "epoch 32 | loss: 0.3285  | val_0_auc: 0.65235 | val_1_auc: 0.62179 |  0:03:53s\n",
      "epoch 33 | loss: 0.32779 | val_0_auc: 0.6425  | val_1_auc: 0.62606 |  0:04:00s\n",
      "epoch 34 | loss: 0.32908 | val_0_auc: 0.6519  | val_1_auc: 0.62096 |  0:04:07s\n",
      "epoch 35 | loss: 0.32795 | val_0_auc: 0.65065 | val_1_auc: 0.61578 |  0:04:14s\n",
      "epoch 36 | loss: 0.32775 | val_0_auc: 0.65563 | val_1_auc: 0.62159 |  0:04:21s\n",
      "epoch 37 | loss: 0.32849 | val_0_auc: 0.6539  | val_1_auc: 0.63605 |  0:04:27s\n",
      "epoch 38 | loss: 0.32676 | val_0_auc: 0.65589 | val_1_auc: 0.6517  |  0:04:34s\n",
      "epoch 39 | loss: 0.32686 | val_0_auc: 0.65744 | val_1_auc: 0.65622 |  0:04:41s\n",
      "epoch 40 | loss: 0.32503 | val_0_auc: 0.65617 | val_1_auc: 0.64932 |  0:04:49s\n",
      "epoch 41 | loss: 0.32819 | val_0_auc: 0.64704 | val_1_auc: 0.65461 |  0:04:57s\n",
      "epoch 42 | loss: 0.32855 | val_0_auc: 0.64563 | val_1_auc: 0.65547 |  0:05:05s\n",
      "epoch 43 | loss: 0.32839 | val_0_auc: 0.64563 | val_1_auc: 0.65928 |  0:05:12s\n",
      "epoch 44 | loss: 0.32652 | val_0_auc: 0.6459  | val_1_auc: 0.65848 |  0:05:19s\n",
      "epoch 45 | loss: 0.32681 | val_0_auc: 0.64596 | val_1_auc: 0.65048 |  0:05:27s\n",
      "epoch 46 | loss: 0.32861 | val_0_auc: 0.64999 | val_1_auc: 0.66029 |  0:05:35s\n",
      "epoch 47 | loss: 0.32664 | val_0_auc: 0.66473 | val_1_auc: 0.65497 |  0:05:42s\n",
      "epoch 48 | loss: 0.3253  | val_0_auc: 0.662   | val_1_auc: 0.64626 |  0:05:49s\n",
      "epoch 49 | loss: 0.32687 | val_0_auc: 0.67047 | val_1_auc: 0.6435  |  0:05:56s\n",
      "epoch 50 | loss: 0.32404 | val_0_auc: 0.66236 | val_1_auc: 0.66143 |  0:06:04s\n",
      "epoch 51 | loss: 0.32678 | val_0_auc: 0.655   | val_1_auc: 0.66047 |  0:06:12s\n",
      "epoch 52 | loss: 0.32333 | val_0_auc: 0.6575  | val_1_auc: 0.67495 |  0:06:20s\n",
      "epoch 53 | loss: 0.32351 | val_0_auc: 0.66169 | val_1_auc: 0.6684  |  0:06:35s\n",
      "epoch 54 | loss: 0.32339 | val_0_auc: 0.6591  | val_1_auc: 0.67347 |  0:06:48s\n",
      "epoch 55 | loss: 0.32293 | val_0_auc: 0.6618  | val_1_auc: 0.6634  |  0:07:01s\n",
      "epoch 56 | loss: 0.32476 | val_0_auc: 0.65778 | val_1_auc: 0.66144 |  0:07:18s\n",
      "epoch 57 | loss: 0.32409 | val_0_auc: 0.66382 | val_1_auc: 0.6555  |  0:07:34s\n",
      "epoch 58 | loss: 0.32281 | val_0_auc: 0.65592 | val_1_auc: 0.65941 |  0:07:50s\n",
      "epoch 59 | loss: 0.32249 | val_0_auc: 0.65949 | val_1_auc: 0.65774 |  0:08:04s\n",
      "epoch 60 | loss: 0.32181 | val_0_auc: 0.66644 | val_1_auc: 0.66308 |  0:08:20s\n",
      "epoch 61 | loss: 0.32532 | val_0_auc: 0.66817 | val_1_auc: 0.65144 |  0:08:37s\n",
      "epoch 62 | loss: 0.32411 | val_0_auc: 0.66579 | val_1_auc: 0.6659  |  0:08:54s\n",
      "epoch 63 | loss: 0.32241 | val_0_auc: 0.66486 | val_1_auc: 0.65455 |  0:09:11s\n",
      "epoch 64 | loss: 0.32335 | val_0_auc: 0.66592 | val_1_auc: 0.65347 |  0:09:24s\n",
      "epoch 65 | loss: 0.32206 | val_0_auc: 0.66544 | val_1_auc: 0.66578 |  0:09:32s\n",
      "epoch 66 | loss: 0.32389 | val_0_auc: 0.67961 | val_1_auc: 0.67451 |  0:09:39s\n",
      "epoch 67 | loss: 0.32079 | val_0_auc: 0.67093 | val_1_auc: 0.67725 |  0:09:48s\n",
      "epoch 68 | loss: 0.32136 | val_0_auc: 0.67046 | val_1_auc: 0.66603 |  0:09:57s\n",
      "epoch 69 | loss: 0.32103 | val_0_auc: 0.67779 | val_1_auc: 0.66642 |  0:10:05s\n",
      "epoch 70 | loss: 0.32323 | val_0_auc: 0.67359 | val_1_auc: 0.66011 |  0:10:13s\n",
      "epoch 71 | loss: 0.32087 | val_0_auc: 0.67379 | val_1_auc: 0.66926 |  0:10:20s\n",
      "epoch 72 | loss: 0.32    | val_0_auc: 0.67559 | val_1_auc: 0.65595 |  0:10:27s\n",
      "epoch 73 | loss: 0.32096 | val_0_auc: 0.67269 | val_1_auc: 0.66365 |  0:10:36s\n",
      "epoch 74 | loss: 0.31993 | val_0_auc: 0.66891 | val_1_auc: 0.66297 |  0:10:43s\n",
      "epoch 75 | loss: 0.32026 | val_0_auc: 0.67036 | val_1_auc: 0.6577  |  0:10:50s\n",
      "epoch 76 | loss: 0.32176 | val_0_auc: 0.66371 | val_1_auc: 0.645   |  0:10:56s\n",
      "epoch 77 | loss: 0.32143 | val_0_auc: 0.66595 | val_1_auc: 0.6533  |  0:11:03s\n",
      "epoch 78 | loss: 0.32342 | val_0_auc: 0.6715  | val_1_auc: 0.6621  |  0:11:09s\n",
      "epoch 79 | loss: 0.32289 | val_0_auc: 0.667   | val_1_auc: 0.65765 |  0:11:16s\n",
      "epoch 80 | loss: 0.32011 | val_0_auc: 0.66717 | val_1_auc: 0.65788 |  0:11:23s\n",
      "epoch 81 | loss: 0.3218  | val_0_auc: 0.66213 | val_1_auc: 0.65815 |  0:11:30s\n",
      "epoch 82 | loss: 0.32077 | val_0_auc: 0.66841 | val_1_auc: 0.66319 |  0:11:37s\n",
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 67 and best_val_1_auc = 0.67725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:19:28,528] Trial 13 finished with value: 0.6772466274645451 and parameters: {'n_d': 32, 'n_a': 53, 'n_steps': 6, 'gamma': 1.2314220880374256, 'lambda_sparse': 7.739532197730214e-06, 'lr': 0.0028268738959049576}. Best is trial 13 with value: 0.6772466274645451.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.64432 | val_0_auc: 0.46421 | val_1_auc: 0.49668 |  0:00:07s\n",
      "epoch 1  | loss: 0.57261 | val_0_auc: 0.48849 | val_1_auc: 0.48775 |  0:00:14s\n",
      "epoch 2  | loss: 0.53742 | val_0_auc: 0.46776 | val_1_auc: 0.49857 |  0:00:23s\n",
      "epoch 3  | loss: 0.51201 | val_0_auc: 0.47598 | val_1_auc: 0.48237 |  0:00:30s\n",
      "epoch 4  | loss: 0.50135 | val_0_auc: 0.48385 | val_1_auc: 0.49039 |  0:00:37s\n",
      "epoch 5  | loss: 0.46604 | val_0_auc: 0.50288 | val_1_auc: 0.49542 |  0:00:43s\n",
      "epoch 6  | loss: 0.4543  | val_0_auc: 0.50852 | val_1_auc: 0.50123 |  0:00:49s\n",
      "epoch 7  | loss: 0.4418  | val_0_auc: 0.52527 | val_1_auc: 0.51119 |  0:00:56s\n",
      "epoch 8  | loss: 0.43163 | val_0_auc: 0.51773 | val_1_auc: 0.52177 |  0:01:02s\n",
      "epoch 9  | loss: 0.41836 | val_0_auc: 0.51593 | val_1_auc: 0.53766 |  0:01:09s\n",
      "epoch 10 | loss: 0.40822 | val_0_auc: 0.52355 | val_1_auc: 0.54662 |  0:01:18s\n",
      "epoch 11 | loss: 0.40309 | val_0_auc: 0.53272 | val_1_auc: 0.53945 |  0:01:25s\n",
      "epoch 12 | loss: 0.39445 | val_0_auc: 0.54719 | val_1_auc: 0.55745 |  0:01:33s\n",
      "epoch 13 | loss: 0.3946  | val_0_auc: 0.55537 | val_1_auc: 0.55028 |  0:01:41s\n",
      "epoch 14 | loss: 0.37908 | val_0_auc: 0.55318 | val_1_auc: 0.57825 |  0:01:48s\n",
      "epoch 15 | loss: 0.38079 | val_0_auc: 0.54037 | val_1_auc: 0.56257 |  0:01:56s\n",
      "epoch 16 | loss: 0.38128 | val_0_auc: 0.55956 | val_1_auc: 0.5676  |  0:02:03s\n",
      "epoch 17 | loss: 0.3717  | val_0_auc: 0.55477 | val_1_auc: 0.55264 |  0:02:10s\n",
      "epoch 18 | loss: 0.37345 | val_0_auc: 0.56118 | val_1_auc: 0.54694 |  0:02:18s\n",
      "epoch 19 | loss: 0.3651  | val_0_auc: 0.57634 | val_1_auc: 0.56519 |  0:02:27s\n",
      "epoch 20 | loss: 0.36301 | val_0_auc: 0.57479 | val_1_auc: 0.57253 |  0:02:37s\n",
      "epoch 21 | loss: 0.35978 | val_0_auc: 0.58728 | val_1_auc: 0.58442 |  0:02:48s\n",
      "epoch 22 | loss: 0.35682 | val_0_auc: 0.57637 | val_1_auc: 0.60792 |  0:02:58s\n",
      "epoch 23 | loss: 0.35735 | val_0_auc: 0.57735 | val_1_auc: 0.59271 |  0:03:09s\n",
      "epoch 24 | loss: 0.35631 | val_0_auc: 0.57612 | val_1_auc: 0.58995 |  0:03:16s\n",
      "epoch 25 | loss: 0.35323 | val_0_auc: 0.56517 | val_1_auc: 0.59272 |  0:03:23s\n",
      "epoch 26 | loss: 0.3551  | val_0_auc: 0.58496 | val_1_auc: 0.60561 |  0:03:37s\n",
      "epoch 27 | loss: 0.34904 | val_0_auc: 0.58392 | val_1_auc: 0.6109  |  0:03:53s\n",
      "epoch 28 | loss: 0.3539  | val_0_auc: 0.60638 | val_1_auc: 0.58187 |  0:04:01s\n",
      "epoch 29 | loss: 0.34962 | val_0_auc: 0.60842 | val_1_auc: 0.58849 |  0:04:09s\n",
      "epoch 30 | loss: 0.34623 | val_0_auc: 0.59676 | val_1_auc: 0.61861 |  0:04:16s\n",
      "epoch 31 | loss: 0.34877 | val_0_auc: 0.60607 | val_1_auc: 0.59552 |  0:04:24s\n",
      "epoch 32 | loss: 0.34541 | val_0_auc: 0.60431 | val_1_auc: 0.60938 |  0:04:32s\n",
      "epoch 33 | loss: 0.34524 | val_0_auc: 0.61125 | val_1_auc: 0.60332 |  0:04:39s\n",
      "epoch 34 | loss: 0.34346 | val_0_auc: 0.62279 | val_1_auc: 0.63893 |  0:04:46s\n",
      "epoch 35 | loss: 0.34165 | val_0_auc: 0.61982 | val_1_auc: 0.61808 |  0:04:54s\n",
      "epoch 36 | loss: 0.34169 | val_0_auc: 0.62259 | val_1_auc: 0.62101 |  0:05:02s\n",
      "epoch 37 | loss: 0.33819 | val_0_auc: 0.61069 | val_1_auc: 0.59719 |  0:05:09s\n",
      "epoch 38 | loss: 0.34159 | val_0_auc: 0.61936 | val_1_auc: 0.60024 |  0:05:16s\n",
      "epoch 39 | loss: 0.34006 | val_0_auc: 0.60098 | val_1_auc: 0.57976 |  0:05:24s\n",
      "epoch 40 | loss: 0.33846 | val_0_auc: 0.62373 | val_1_auc: 0.58589 |  0:05:30s\n",
      "epoch 41 | loss: 0.33806 | val_0_auc: 0.61882 | val_1_auc: 0.59683 |  0:05:37s\n",
      "epoch 42 | loss: 0.33716 | val_0_auc: 0.62632 | val_1_auc: 0.59661 |  0:05:44s\n",
      "epoch 43 | loss: 0.33713 | val_0_auc: 0.63179 | val_1_auc: 0.60049 |  0:05:51s\n",
      "epoch 44 | loss: 0.33571 | val_0_auc: 0.63922 | val_1_auc: 0.5984  |  0:05:58s\n",
      "epoch 45 | loss: 0.33373 | val_0_auc: 0.63717 | val_1_auc: 0.61036 |  0:06:05s\n",
      "epoch 46 | loss: 0.33506 | val_0_auc: 0.64567 | val_1_auc: 0.61025 |  0:06:13s\n",
      "epoch 47 | loss: 0.3348  | val_0_auc: 0.63893 | val_1_auc: 0.60713 |  0:06:21s\n",
      "epoch 48 | loss: 0.3353  | val_0_auc: 0.63215 | val_1_auc: 0.62539 |  0:06:27s\n",
      "epoch 49 | loss: 0.33446 | val_0_auc: 0.63339 | val_1_auc: 0.63254 |  0:06:34s\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 34 and best_val_1_auc = 0.63893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:26:05,005] Trial 14 finished with value: 0.6389318574887581 and parameters: {'n_d': 42, 'n_a': 52, 'n_steps': 6, 'gamma': 1.2578380806293228, 'lambda_sparse': 6.938028085113736e-06, 'lr': 0.0012807131703618457}. Best is trial 13 with value: 0.6772466274645451.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.50931 | val_0_auc: 0.47383 | val_1_auc: 0.50214 |  0:00:05s\n",
      "epoch 1  | loss: 0.45249 | val_0_auc: 0.51017 | val_1_auc: 0.53016 |  0:00:11s\n",
      "epoch 2  | loss: 0.42445 | val_0_auc: 0.51876 | val_1_auc: 0.53033 |  0:00:16s\n",
      "epoch 3  | loss: 0.39328 | val_0_auc: 0.55219 | val_1_auc: 0.54055 |  0:00:22s\n",
      "epoch 4  | loss: 0.37911 | val_0_auc: 0.54906 | val_1_auc: 0.53829 |  0:00:28s\n",
      "epoch 5  | loss: 0.36784 | val_0_auc: 0.55491 | val_1_auc: 0.55086 |  0:00:33s\n",
      "epoch 6  | loss: 0.36403 | val_0_auc: 0.56932 | val_1_auc: 0.57555 |  0:00:38s\n",
      "epoch 7  | loss: 0.35647 | val_0_auc: 0.56957 | val_1_auc: 0.5777  |  0:00:44s\n",
      "epoch 8  | loss: 0.35133 | val_0_auc: 0.57933 | val_1_auc: 0.58744 |  0:00:50s\n",
      "epoch 9  | loss: 0.3483  | val_0_auc: 0.59381 | val_1_auc: 0.58816 |  0:00:56s\n",
      "epoch 10 | loss: 0.34521 | val_0_auc: 0.60658 | val_1_auc: 0.61681 |  0:01:06s\n",
      "epoch 11 | loss: 0.33989 | val_0_auc: 0.60415 | val_1_auc: 0.62607 |  0:01:16s\n",
      "epoch 12 | loss: 0.34125 | val_0_auc: 0.60898 | val_1_auc: 0.62161 |  0:01:23s\n",
      "epoch 13 | loss: 0.33942 | val_0_auc: 0.61869 | val_1_auc: 0.61059 |  0:01:30s\n",
      "epoch 14 | loss: 0.3368  | val_0_auc: 0.61737 | val_1_auc: 0.62652 |  0:01:37s\n",
      "epoch 15 | loss: 0.33674 | val_0_auc: 0.62434 | val_1_auc: 0.6275  |  0:01:43s\n",
      "epoch 16 | loss: 0.33622 | val_0_auc: 0.61718 | val_1_auc: 0.63152 |  0:01:49s\n",
      "epoch 17 | loss: 0.33205 | val_0_auc: 0.6262  | val_1_auc: 0.63005 |  0:01:56s\n",
      "epoch 18 | loss: 0.33338 | val_0_auc: 0.64192 | val_1_auc: 0.61346 |  0:02:02s\n",
      "epoch 19 | loss: 0.33221 | val_0_auc: 0.64121 | val_1_auc: 0.62198 |  0:02:08s\n",
      "epoch 20 | loss: 0.33115 | val_0_auc: 0.64135 | val_1_auc: 0.62645 |  0:02:15s\n",
      "epoch 21 | loss: 0.3298  | val_0_auc: 0.64239 | val_1_auc: 0.63724 |  0:02:23s\n",
      "epoch 22 | loss: 0.32978 | val_0_auc: 0.64156 | val_1_auc: 0.63195 |  0:02:31s\n",
      "epoch 23 | loss: 0.32967 | val_0_auc: 0.64517 | val_1_auc: 0.63168 |  0:02:38s\n",
      "epoch 24 | loss: 0.32982 | val_0_auc: 0.64243 | val_1_auc: 0.62847 |  0:02:43s\n",
      "epoch 25 | loss: 0.32827 | val_0_auc: 0.63135 | val_1_auc: 0.64615 |  0:02:49s\n",
      "epoch 26 | loss: 0.32735 | val_0_auc: 0.62777 | val_1_auc: 0.64979 |  0:02:55s\n",
      "epoch 27 | loss: 0.33055 | val_0_auc: 0.6161  | val_1_auc: 0.65529 |  0:03:00s\n",
      "epoch 28 | loss: 0.32868 | val_0_auc: 0.64223 | val_1_auc: 0.66036 |  0:03:07s\n",
      "epoch 29 | loss: 0.32723 | val_0_auc: 0.64532 | val_1_auc: 0.65042 |  0:03:14s\n",
      "epoch 30 | loss: 0.3265  | val_0_auc: 0.64264 | val_1_auc: 0.63749 |  0:03:20s\n",
      "epoch 31 | loss: 0.32636 | val_0_auc: 0.64832 | val_1_auc: 0.63596 |  0:03:26s\n",
      "epoch 32 | loss: 0.32778 | val_0_auc: 0.65502 | val_1_auc: 0.62929 |  0:03:31s\n",
      "epoch 33 | loss: 0.32654 | val_0_auc: 0.64313 | val_1_auc: 0.62087 |  0:03:37s\n",
      "epoch 34 | loss: 0.32571 | val_0_auc: 0.65678 | val_1_auc: 0.62568 |  0:03:43s\n",
      "epoch 35 | loss: 0.32545 | val_0_auc: 0.66821 | val_1_auc: 0.6328  |  0:03:50s\n",
      "epoch 36 | loss: 0.32459 | val_0_auc: 0.65549 | val_1_auc: 0.62183 |  0:03:56s\n",
      "epoch 37 | loss: 0.3247  | val_0_auc: 0.65537 | val_1_auc: 0.63558 |  0:04:03s\n",
      "epoch 38 | loss: 0.32479 | val_0_auc: 0.65529 | val_1_auc: 0.64126 |  0:04:10s\n",
      "epoch 39 | loss: 0.32429 | val_0_auc: 0.65546 | val_1_auc: 0.64229 |  0:04:16s\n",
      "epoch 40 | loss: 0.32292 | val_0_auc: 0.66232 | val_1_auc: 0.64665 |  0:04:22s\n",
      "epoch 41 | loss: 0.32226 | val_0_auc: 0.65916 | val_1_auc: 0.64793 |  0:04:28s\n",
      "epoch 42 | loss: 0.32383 | val_0_auc: 0.65576 | val_1_auc: 0.64535 |  0:04:34s\n",
      "epoch 43 | loss: 0.32132 | val_0_auc: 0.65671 | val_1_auc: 0.64841 |  0:04:40s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 28 and best_val_1_auc = 0.66036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:30:48,069] Trial 15 finished with value: 0.6603555863023175 and parameters: {'n_d': 34, 'n_a': 51, 'n_steps': 5, 'gamma': 1.181897844792012, 'lambda_sparse': 1.7054171733601802e-05, 'lr': 0.0028715058123980172}. Best is trial 13 with value: 0.6772466274645451.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.16018 | val_0_auc: 0.4934  | val_1_auc: 0.51239 |  0:00:06s\n",
      "epoch 1  | loss: 2.15112 | val_0_auc: 0.47068 | val_1_auc: 0.49767 |  0:00:12s\n",
      "epoch 2  | loss: 0.95932 | val_0_auc: 0.44965 | val_1_auc: 0.47156 |  0:00:19s\n",
      "epoch 3  | loss: 0.60977 | val_0_auc: 0.45577 | val_1_auc: 0.49085 |  0:00:25s\n",
      "epoch 4  | loss: 0.54856 | val_0_auc: 0.45576 | val_1_auc: 0.48727 |  0:00:31s\n",
      "epoch 5  | loss: 0.52259 | val_0_auc: 0.47201 | val_1_auc: 0.49055 |  0:00:38s\n",
      "epoch 6  | loss: 0.50094 | val_0_auc: 0.48506 | val_1_auc: 0.49673 |  0:00:44s\n",
      "epoch 7  | loss: 0.48001 | val_0_auc: 0.497   | val_1_auc: 0.51255 |  0:00:50s\n",
      "epoch 8  | loss: 0.45734 | val_0_auc: 0.51049 | val_1_auc: 0.51549 |  0:00:56s\n",
      "epoch 9  | loss: 0.4371  | val_0_auc: 0.50681 | val_1_auc: 0.51505 |  0:01:02s\n",
      "epoch 10 | loss: 0.42759 | val_0_auc: 0.51403 | val_1_auc: 0.51417 |  0:01:09s\n",
      "epoch 11 | loss: 0.41312 | val_0_auc: 0.53211 | val_1_auc: 0.52196 |  0:01:16s\n",
      "epoch 12 | loss: 0.41299 | val_0_auc: 0.53921 | val_1_auc: 0.54564 |  0:01:23s\n",
      "epoch 13 | loss: 0.40822 | val_0_auc: 0.53559 | val_1_auc: 0.547   |  0:01:29s\n",
      "epoch 14 | loss: 0.40203 | val_0_auc: 0.56134 | val_1_auc: 0.56179 |  0:01:35s\n",
      "epoch 15 | loss: 0.40465 | val_0_auc: 0.53659 | val_1_auc: 0.57195 |  0:01:41s\n",
      "epoch 16 | loss: 0.39425 | val_0_auc: 0.53658 | val_1_auc: 0.58036 |  0:01:47s\n",
      "epoch 17 | loss: 0.3982  | val_0_auc: 0.53953 | val_1_auc: 0.55984 |  0:01:53s\n",
      "epoch 18 | loss: 0.38698 | val_0_auc: 0.55039 | val_1_auc: 0.58471 |  0:01:59s\n",
      "epoch 19 | loss: 0.38172 | val_0_auc: 0.54503 | val_1_auc: 0.59361 |  0:02:05s\n",
      "epoch 20 | loss: 0.38121 | val_0_auc: 0.55084 | val_1_auc: 0.61595 |  0:02:11s\n",
      "epoch 21 | loss: 0.37887 | val_0_auc: 0.55629 | val_1_auc: 0.58591 |  0:02:17s\n",
      "epoch 22 | loss: 0.37547 | val_0_auc: 0.56019 | val_1_auc: 0.60202 |  0:02:26s\n",
      "epoch 23 | loss: 0.37448 | val_0_auc: 0.58288 | val_1_auc: 0.6161  |  0:02:34s\n",
      "epoch 24 | loss: 0.37277 | val_0_auc: 0.58352 | val_1_auc: 0.60102 |  0:02:40s\n",
      "epoch 25 | loss: 0.37317 | val_0_auc: 0.59776 | val_1_auc: 0.58048 |  0:02:47s\n",
      "epoch 26 | loss: 0.36835 | val_0_auc: 0.58665 | val_1_auc: 0.58546 |  0:02:53s\n",
      "epoch 27 | loss: 0.36775 | val_0_auc: 0.59756 | val_1_auc: 0.59313 |  0:02:59s\n",
      "epoch 28 | loss: 0.36567 | val_0_auc: 0.59912 | val_1_auc: 0.58049 |  0:03:05s\n",
      "epoch 29 | loss: 0.36066 | val_0_auc: 0.59526 | val_1_auc: 0.59851 |  0:03:11s\n",
      "epoch 30 | loss: 0.36243 | val_0_auc: 0.62256 | val_1_auc: 0.60145 |  0:03:18s\n",
      "epoch 31 | loss: 0.35902 | val_0_auc: 0.60822 | val_1_auc: 0.57992 |  0:03:24s\n",
      "epoch 32 | loss: 0.35567 | val_0_auc: 0.60367 | val_1_auc: 0.59728 |  0:03:31s\n",
      "epoch 33 | loss: 0.35937 | val_0_auc: 0.60265 | val_1_auc: 0.602   |  0:03:39s\n",
      "epoch 34 | loss: 0.35544 | val_0_auc: 0.61031 | val_1_auc: 0.60038 |  0:03:45s\n",
      "epoch 35 | loss: 0.3552  | val_0_auc: 0.61894 | val_1_auc: 0.59962 |  0:03:51s\n",
      "epoch 36 | loss: 0.35184 | val_0_auc: 0.6209  | val_1_auc: 0.60529 |  0:03:57s\n",
      "epoch 37 | loss: 0.35234 | val_0_auc: 0.61871 | val_1_auc: 0.60682 |  0:04:08s\n",
      "epoch 38 | loss: 0.35036 | val_0_auc: 0.62585 | val_1_auc: 0.60769 |  0:04:19s\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 23 and best_val_1_auc = 0.6161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:35:11,983] Trial 16 finished with value: 0.6161044621238326 and parameters: {'n_d': 48, 'n_a': 55, 'n_steps': 5, 'gamma': 1.1510753085757743, 'lambda_sparse': 2.0280067674050265e-05, 'lr': 0.0013892506085023538}. Best is trial 13 with value: 0.6772466274645451.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.57193 | val_0_auc: 0.46709 | val_1_auc: 0.47824 |  0:00:11s\n",
      "epoch 1  | loss: 0.49926 | val_0_auc: 0.50868 | val_1_auc: 0.4952  |  0:00:20s\n",
      "epoch 2  | loss: 0.4531  | val_0_auc: 0.51434 | val_1_auc: 0.5047  |  0:00:31s\n",
      "epoch 3  | loss: 0.42008 | val_0_auc: 0.53051 | val_1_auc: 0.5323  |  0:00:42s\n",
      "epoch 4  | loss: 0.4068  | val_0_auc: 0.54763 | val_1_auc: 0.54782 |  0:00:54s\n",
      "epoch 5  | loss: 0.38957 | val_0_auc: 0.55497 | val_1_auc: 0.52874 |  0:01:07s\n",
      "epoch 6  | loss: 0.38037 | val_0_auc: 0.56014 | val_1_auc: 0.57873 |  0:01:18s\n",
      "epoch 7  | loss: 0.36943 | val_0_auc: 0.58813 | val_1_auc: 0.56138 |  0:01:28s\n",
      "epoch 8  | loss: 0.36228 | val_0_auc: 0.59307 | val_1_auc: 0.57105 |  0:01:37s\n",
      "epoch 9  | loss: 0.35469 | val_0_auc: 0.60336 | val_1_auc: 0.57481 |  0:01:46s\n",
      "epoch 10 | loss: 0.35625 | val_0_auc: 0.60973 | val_1_auc: 0.5683  |  0:01:57s\n",
      "epoch 11 | loss: 0.35009 | val_0_auc: 0.61533 | val_1_auc: 0.58591 |  0:02:10s\n",
      "epoch 12 | loss: 0.35194 | val_0_auc: 0.61189 | val_1_auc: 0.58373 |  0:02:23s\n",
      "epoch 13 | loss: 0.34533 | val_0_auc: 0.6218  | val_1_auc: 0.62101 |  0:02:33s\n",
      "epoch 14 | loss: 0.34159 | val_0_auc: 0.63923 | val_1_auc: 0.63587 |  0:02:42s\n",
      "epoch 15 | loss: 0.34219 | val_0_auc: 0.64799 | val_1_auc: 0.63596 |  0:02:51s\n",
      "epoch 16 | loss: 0.34056 | val_0_auc: 0.63459 | val_1_auc: 0.60844 |  0:03:01s\n",
      "epoch 17 | loss: 0.33756 | val_0_auc: 0.63231 | val_1_auc: 0.60364 |  0:03:11s\n",
      "epoch 18 | loss: 0.3345  | val_0_auc: 0.62394 | val_1_auc: 0.61558 |  0:03:23s\n",
      "epoch 19 | loss: 0.33512 | val_0_auc: 0.62296 | val_1_auc: 0.60536 |  0:03:34s\n",
      "epoch 20 | loss: 0.33406 | val_0_auc: 0.63279 | val_1_auc: 0.61424 |  0:03:45s\n",
      "epoch 21 | loss: 0.33109 | val_0_auc: 0.64112 | val_1_auc: 0.62926 |  0:03:58s\n",
      "epoch 22 | loss: 0.33369 | val_0_auc: 0.63862 | val_1_auc: 0.6174  |  0:04:11s\n",
      "epoch 23 | loss: 0.33046 | val_0_auc: 0.63852 | val_1_auc: 0.6247  |  0:04:23s\n",
      "epoch 24 | loss: 0.32868 | val_0_auc: 0.63877 | val_1_auc: 0.62012 |  0:04:34s\n",
      "epoch 25 | loss: 0.33064 | val_0_auc: 0.6385  | val_1_auc: 0.63228 |  0:04:46s\n",
      "epoch 26 | loss: 0.33227 | val_0_auc: 0.64961 | val_1_auc: 0.64574 |  0:04:59s\n",
      "epoch 27 | loss: 0.33034 | val_0_auc: 0.63859 | val_1_auc: 0.6581  |  0:05:08s\n",
      "epoch 28 | loss: 0.32967 | val_0_auc: 0.63944 | val_1_auc: 0.65336 |  0:05:19s\n",
      "epoch 29 | loss: 0.32714 | val_0_auc: 0.62975 | val_1_auc: 0.64966 |  0:05:29s\n",
      "epoch 30 | loss: 0.32593 | val_0_auc: 0.65388 | val_1_auc: 0.66016 |  0:05:40s\n",
      "epoch 31 | loss: 0.32651 | val_0_auc: 0.64582 | val_1_auc: 0.6451  |  0:05:51s\n",
      "epoch 32 | loss: 0.32462 | val_0_auc: 0.6519  | val_1_auc: 0.64745 |  0:06:01s\n",
      "epoch 33 | loss: 0.3241  | val_0_auc: 0.65893 | val_1_auc: 0.65176 |  0:06:13s\n",
      "epoch 34 | loss: 0.32254 | val_0_auc: 0.65538 | val_1_auc: 0.65763 |  0:06:24s\n",
      "epoch 35 | loss: 0.32112 | val_0_auc: 0.66985 | val_1_auc: 0.66889 |  0:06:36s\n",
      "epoch 36 | loss: 0.32316 | val_0_auc: 0.66234 | val_1_auc: 0.67135 |  0:06:45s\n",
      "epoch 37 | loss: 0.32298 | val_0_auc: 0.65889 | val_1_auc: 0.68134 |  0:06:59s\n",
      "epoch 38 | loss: 0.32293 | val_0_auc: 0.66786 | val_1_auc: 0.68763 |  0:07:09s\n",
      "epoch 39 | loss: 0.32188 | val_0_auc: 0.66228 | val_1_auc: 0.67425 |  0:07:20s\n",
      "epoch 40 | loss: 0.31987 | val_0_auc: 0.66572 | val_1_auc: 0.67081 |  0:07:31s\n",
      "epoch 41 | loss: 0.32081 | val_0_auc: 0.66372 | val_1_auc: 0.66672 |  0:07:42s\n",
      "epoch 42 | loss: 0.32153 | val_0_auc: 0.66089 | val_1_auc: 0.65948 |  0:07:54s\n",
      "epoch 43 | loss: 0.32122 | val_0_auc: 0.66316 | val_1_auc: 0.67923 |  0:08:00s\n",
      "epoch 44 | loss: 0.32022 | val_0_auc: 0.66267 | val_1_auc: 0.68468 |  0:08:07s\n",
      "epoch 45 | loss: 0.31872 | val_0_auc: 0.66532 | val_1_auc: 0.68159 |  0:08:14s\n",
      "epoch 46 | loss: 0.3201  | val_0_auc: 0.65135 | val_1_auc: 0.68344 |  0:08:20s\n",
      "epoch 47 | loss: 0.31962 | val_0_auc: 0.65851 | val_1_auc: 0.68974 |  0:08:27s\n",
      "epoch 48 | loss: 0.31937 | val_0_auc: 0.6633  | val_1_auc: 0.68269 |  0:08:33s\n",
      "epoch 49 | loss: 0.3196  | val_0_auc: 0.6611  | val_1_auc: 0.68452 |  0:08:39s\n",
      "epoch 50 | loss: 0.31758 | val_0_auc: 0.66451 | val_1_auc: 0.68147 |  0:08:44s\n",
      "epoch 51 | loss: 0.31822 | val_0_auc: 0.66887 | val_1_auc: 0.67183 |  0:08:50s\n",
      "epoch 52 | loss: 0.31961 | val_0_auc: 0.66611 | val_1_auc: 0.66927 |  0:08:57s\n",
      "epoch 53 | loss: 0.31913 | val_0_auc: 0.66977 | val_1_auc: 0.66886 |  0:09:03s\n",
      "epoch 54 | loss: 0.31753 | val_0_auc: 0.66704 | val_1_auc: 0.67828 |  0:09:10s\n",
      "epoch 55 | loss: 0.31737 | val_0_auc: 0.67514 | val_1_auc: 0.68522 |  0:09:17s\n",
      "epoch 56 | loss: 0.31701 | val_0_auc: 0.67743 | val_1_auc: 0.67783 |  0:09:23s\n",
      "epoch 57 | loss: 0.31786 | val_0_auc: 0.68039 | val_1_auc: 0.68312 |  0:09:29s\n",
      "epoch 58 | loss: 0.31622 | val_0_auc: 0.67843 | val_1_auc: 0.67684 |  0:09:35s\n",
      "epoch 59 | loss: 0.31585 | val_0_auc: 0.67168 | val_1_auc: 0.68262 |  0:09:40s\n",
      "epoch 60 | loss: 0.31683 | val_0_auc: 0.65751 | val_1_auc: 0.67991 |  0:09:46s\n",
      "epoch 61 | loss: 0.31554 | val_0_auc: 0.6604  | val_1_auc: 0.67809 |  0:09:52s\n",
      "epoch 62 | loss: 0.31576 | val_0_auc: 0.66636 | val_1_auc: 0.68377 |  0:09:58s\n",
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 47 and best_val_1_auc = 0.68974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:45:12,845] Trial 17 finished with value: 0.6897391905914908 and parameters: {'n_d': 37, 'n_a': 44, 'n_steps': 5, 'gamma': 1.020080536443378, 'lambda_sparse': 0.00017353660543982074, 'lr': 0.0028875987502131164}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.93513 | val_0_auc: 0.51771 | val_1_auc: 0.5172  |  0:00:04s\n",
      "epoch 1  | loss: 1.00152 | val_0_auc: 0.51679 | val_1_auc: 0.48528 |  0:00:08s\n",
      "epoch 2  | loss: 0.54269 | val_0_auc: 0.49788 | val_1_auc: 0.48896 |  0:00:12s\n",
      "epoch 3  | loss: 0.43194 | val_0_auc: 0.50149 | val_1_auc: 0.4787  |  0:00:17s\n",
      "epoch 4  | loss: 0.40583 | val_0_auc: 0.49021 | val_1_auc: 0.48343 |  0:00:21s\n",
      "epoch 5  | loss: 0.39166 | val_0_auc: 0.50858 | val_1_auc: 0.49502 |  0:00:25s\n",
      "epoch 6  | loss: 0.38394 | val_0_auc: 0.50625 | val_1_auc: 0.50658 |  0:00:30s\n",
      "epoch 7  | loss: 0.37436 | val_0_auc: 0.52802 | val_1_auc: 0.5207  |  0:00:35s\n",
      "epoch 8  | loss: 0.36428 | val_0_auc: 0.54802 | val_1_auc: 0.53937 |  0:00:39s\n",
      "epoch 9  | loss: 0.36181 | val_0_auc: 0.54642 | val_1_auc: 0.54795 |  0:00:44s\n",
      "epoch 10 | loss: 0.35864 | val_0_auc: 0.55384 | val_1_auc: 0.56228 |  0:00:48s\n",
      "epoch 11 | loss: 0.35513 | val_0_auc: 0.54854 | val_1_auc: 0.5533  |  0:00:52s\n",
      "epoch 12 | loss: 0.35189 | val_0_auc: 0.54882 | val_1_auc: 0.5451  |  0:00:56s\n",
      "epoch 13 | loss: 0.35241 | val_0_auc: 0.56304 | val_1_auc: 0.56297 |  0:01:00s\n",
      "epoch 14 | loss: 0.34865 | val_0_auc: 0.56728 | val_1_auc: 0.56824 |  0:01:04s\n",
      "epoch 15 | loss: 0.34688 | val_0_auc: 0.57191 | val_1_auc: 0.56431 |  0:01:08s\n",
      "epoch 16 | loss: 0.34723 | val_0_auc: 0.57885 | val_1_auc: 0.56786 |  0:01:12s\n",
      "epoch 17 | loss: 0.34686 | val_0_auc: 0.57847 | val_1_auc: 0.56888 |  0:01:16s\n",
      "epoch 18 | loss: 0.34378 | val_0_auc: 0.58717 | val_1_auc: 0.57241 |  0:01:20s\n",
      "epoch 19 | loss: 0.34113 | val_0_auc: 0.5961  | val_1_auc: 0.57065 |  0:01:24s\n",
      "epoch 20 | loss: 0.34287 | val_0_auc: 0.60094 | val_1_auc: 0.56357 |  0:01:27s\n",
      "epoch 21 | loss: 0.34072 | val_0_auc: 0.59937 | val_1_auc: 0.57641 |  0:01:31s\n",
      "epoch 22 | loss: 0.33969 | val_0_auc: 0.61002 | val_1_auc: 0.58577 |  0:01:36s\n",
      "epoch 23 | loss: 0.33775 | val_0_auc: 0.60924 | val_1_auc: 0.58563 |  0:01:40s\n",
      "epoch 24 | loss: 0.33686 | val_0_auc: 0.60968 | val_1_auc: 0.57971 |  0:01:44s\n",
      "epoch 25 | loss: 0.33449 | val_0_auc: 0.60879 | val_1_auc: 0.57158 |  0:01:48s\n",
      "epoch 26 | loss: 0.33614 | val_0_auc: 0.61926 | val_1_auc: 0.59764 |  0:01:53s\n",
      "epoch 27 | loss: 0.33276 | val_0_auc: 0.61819 | val_1_auc: 0.59336 |  0:01:57s\n",
      "epoch 28 | loss: 0.33279 | val_0_auc: 0.62403 | val_1_auc: 0.5948  |  0:02:02s\n",
      "epoch 29 | loss: 0.33155 | val_0_auc: 0.6283  | val_1_auc: 0.59934 |  0:02:06s\n",
      "epoch 30 | loss: 0.3302  | val_0_auc: 0.62817 | val_1_auc: 0.59702 |  0:02:10s\n",
      "epoch 31 | loss: 0.33019 | val_0_auc: 0.63163 | val_1_auc: 0.60517 |  0:02:13s\n",
      "epoch 32 | loss: 0.33239 | val_0_auc: 0.63645 | val_1_auc: 0.61223 |  0:02:17s\n",
      "epoch 33 | loss: 0.32912 | val_0_auc: 0.63213 | val_1_auc: 0.61254 |  0:02:21s\n",
      "epoch 34 | loss: 0.32712 | val_0_auc: 0.62913 | val_1_auc: 0.60611 |  0:02:25s\n",
      "epoch 35 | loss: 0.32857 | val_0_auc: 0.6306  | val_1_auc: 0.6071  |  0:02:29s\n",
      "epoch 36 | loss: 0.32677 | val_0_auc: 0.63666 | val_1_auc: 0.60934 |  0:02:32s\n",
      "epoch 37 | loss: 0.32656 | val_0_auc: 0.63265 | val_1_auc: 0.60665 |  0:02:35s\n",
      "epoch 38 | loss: 0.32866 | val_0_auc: 0.64616 | val_1_auc: 0.61747 |  0:02:38s\n",
      "epoch 39 | loss: 0.32448 | val_0_auc: 0.6513  | val_1_auc: 0.62497 |  0:02:42s\n",
      "epoch 40 | loss: 0.3256  | val_0_auc: 0.64741 | val_1_auc: 0.61997 |  0:02:45s\n",
      "epoch 41 | loss: 0.32672 | val_0_auc: 0.65594 | val_1_auc: 0.61784 |  0:02:48s\n",
      "epoch 42 | loss: 0.32621 | val_0_auc: 0.65245 | val_1_auc: 0.62422 |  0:02:51s\n",
      "epoch 43 | loss: 0.32555 | val_0_auc: 0.6559  | val_1_auc: 0.62358 |  0:02:54s\n",
      "epoch 44 | loss: 0.32461 | val_0_auc: 0.65194 | val_1_auc: 0.63065 |  0:02:57s\n",
      "epoch 45 | loss: 0.3245  | val_0_auc: 0.65079 | val_1_auc: 0.62148 |  0:03:00s\n",
      "epoch 46 | loss: 0.32345 | val_0_auc: 0.64745 | val_1_auc: 0.61168 |  0:03:02s\n",
      "epoch 47 | loss: 0.32263 | val_0_auc: 0.64996 | val_1_auc: 0.61631 |  0:03:05s\n",
      "epoch 48 | loss: 0.32381 | val_0_auc: 0.65204 | val_1_auc: 0.62404 |  0:03:07s\n",
      "epoch 49 | loss: 0.32222 | val_0_auc: 0.64197 | val_1_auc: 0.63042 |  0:03:10s\n",
      "epoch 50 | loss: 0.32395 | val_0_auc: 0.64237 | val_1_auc: 0.61865 |  0:03:13s\n",
      "epoch 51 | loss: 0.32199 | val_0_auc: 0.64582 | val_1_auc: 0.62313 |  0:03:15s\n",
      "epoch 52 | loss: 0.32065 | val_0_auc: 0.64816 | val_1_auc: 0.62899 |  0:03:18s\n",
      "epoch 53 | loss: 0.31982 | val_0_auc: 0.64783 | val_1_auc: 0.6361  |  0:03:20s\n",
      "epoch 54 | loss: 0.32126 | val_0_auc: 0.64938 | val_1_auc: 0.63634 |  0:03:23s\n",
      "epoch 55 | loss: 0.32055 | val_0_auc: 0.65138 | val_1_auc: 0.63535 |  0:03:26s\n",
      "epoch 56 | loss: 0.31973 | val_0_auc: 0.6463  | val_1_auc: 0.63231 |  0:03:28s\n",
      "epoch 57 | loss: 0.31855 | val_0_auc: 0.65502 | val_1_auc: 0.62693 |  0:03:31s\n",
      "epoch 58 | loss: 0.32089 | val_0_auc: 0.65836 | val_1_auc: 0.62967 |  0:03:34s\n",
      "epoch 59 | loss: 0.31814 | val_0_auc: 0.66018 | val_1_auc: 0.62969 |  0:03:36s\n",
      "epoch 60 | loss: 0.31782 | val_0_auc: 0.66288 | val_1_auc: 0.62925 |  0:03:39s\n",
      "epoch 61 | loss: 0.31792 | val_0_auc: 0.65511 | val_1_auc: 0.6358  |  0:03:42s\n",
      "epoch 62 | loss: 0.31706 | val_0_auc: 0.65833 | val_1_auc: 0.63367 |  0:03:44s\n",
      "epoch 63 | loss: 0.31765 | val_0_auc: 0.65704 | val_1_auc: 0.63212 |  0:03:47s\n",
      "epoch 64 | loss: 0.31504 | val_0_auc: 0.65743 | val_1_auc: 0.63403 |  0:03:50s\n",
      "epoch 65 | loss: 0.31562 | val_0_auc: 0.64902 | val_1_auc: 0.64917 |  0:03:52s\n",
      "epoch 66 | loss: 0.31462 | val_0_auc: 0.65196 | val_1_auc: 0.64616 |  0:03:55s\n",
      "epoch 67 | loss: 0.31523 | val_0_auc: 0.65048 | val_1_auc: 0.64872 |  0:03:58s\n",
      "epoch 68 | loss: 0.31532 | val_0_auc: 0.64919 | val_1_auc: 0.64843 |  0:04:00s\n",
      "epoch 69 | loss: 0.3152  | val_0_auc: 0.65401 | val_1_auc: 0.64897 |  0:04:03s\n",
      "epoch 70 | loss: 0.31696 | val_0_auc: 0.65566 | val_1_auc: 0.64467 |  0:04:06s\n",
      "epoch 71 | loss: 0.31473 | val_0_auc: 0.65754 | val_1_auc: 0.65044 |  0:04:09s\n",
      "epoch 72 | loss: 0.31342 | val_0_auc: 0.65446 | val_1_auc: 0.6537  |  0:04:12s\n",
      "epoch 73 | loss: 0.31342 | val_0_auc: 0.65652 | val_1_auc: 0.65419 |  0:04:14s\n",
      "epoch 74 | loss: 0.31514 | val_0_auc: 0.66181 | val_1_auc: 0.65555 |  0:04:17s\n",
      "epoch 75 | loss: 0.31323 | val_0_auc: 0.66438 | val_1_auc: 0.65479 |  0:04:20s\n",
      "epoch 76 | loss: 0.31466 | val_0_auc: 0.65916 | val_1_auc: 0.65412 |  0:04:22s\n",
      "epoch 77 | loss: 0.3111  | val_0_auc: 0.66039 | val_1_auc: 0.65772 |  0:04:25s\n",
      "epoch 78 | loss: 0.31279 | val_0_auc: 0.65996 | val_1_auc: 0.65776 |  0:04:28s\n",
      "epoch 79 | loss: 0.31318 | val_0_auc: 0.65591 | val_1_auc: 0.66449 |  0:04:30s\n",
      "epoch 80 | loss: 0.31149 | val_0_auc: 0.65906 | val_1_auc: 0.65995 |  0:04:33s\n",
      "epoch 81 | loss: 0.31129 | val_0_auc: 0.66006 | val_1_auc: 0.65852 |  0:04:36s\n",
      "epoch 82 | loss: 0.3108  | val_0_auc: 0.65889 | val_1_auc: 0.65558 |  0:04:38s\n",
      "epoch 83 | loss: 0.30984 | val_0_auc: 0.65597 | val_1_auc: 0.6554  |  0:04:41s\n",
      "epoch 84 | loss: 0.30854 | val_0_auc: 0.65722 | val_1_auc: 0.65974 |  0:04:44s\n",
      "epoch 85 | loss: 0.30895 | val_0_auc: 0.66301 | val_1_auc: 0.65693 |  0:04:46s\n",
      "epoch 86 | loss: 0.30929 | val_0_auc: 0.66007 | val_1_auc: 0.65508 |  0:04:49s\n",
      "epoch 87 | loss: 0.31035 | val_0_auc: 0.66102 | val_1_auc: 0.65671 |  0:04:51s\n",
      "epoch 88 | loss: 0.30999 | val_0_auc: 0.66228 | val_1_auc: 0.65384 |  0:04:54s\n",
      "epoch 89 | loss: 0.30957 | val_0_auc: 0.66254 | val_1_auc: 0.65787 |  0:04:57s\n",
      "epoch 90 | loss: 0.3083  | val_0_auc: 0.66074 | val_1_auc: 0.65462 |  0:04:59s\n",
      "epoch 91 | loss: 0.30805 | val_0_auc: 0.65551 | val_1_auc: 0.65215 |  0:05:02s\n",
      "epoch 92 | loss: 0.3079  | val_0_auc: 0.65452 | val_1_auc: 0.64805 |  0:05:05s\n",
      "epoch 93 | loss: 0.30761 | val_0_auc: 0.66002 | val_1_auc: 0.65139 |  0:05:07s\n",
      "epoch 94 | loss: 0.30616 | val_0_auc: 0.66259 | val_1_auc: 0.65701 |  0:05:10s\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 79 and best_val_1_auc = 0.66449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:50:24,176] Trial 18 finished with value: 0.6644897959183673 and parameters: {'n_d': 39, 'n_a': 46, 'n_steps': 3, 'gamma': 1.561848134124785, 'lambda_sparse': 0.00028569714223232646, 'lr': 0.001534573179226712}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70644 | val_0_auc: 0.49382 | val_1_auc: 0.48904 |  0:00:03s\n",
      "epoch 1  | loss: 0.59724 | val_0_auc: 0.47358 | val_1_auc: 0.47497 |  0:00:07s\n",
      "epoch 2  | loss: 0.51578 | val_0_auc: 0.46242 | val_1_auc: 0.46497 |  0:00:11s\n",
      "epoch 3  | loss: 0.50149 | val_0_auc: 0.47061 | val_1_auc: 0.48043 |  0:00:15s\n",
      "epoch 4  | loss: 0.48211 | val_0_auc: 0.45781 | val_1_auc: 0.48226 |  0:00:19s\n",
      "epoch 5  | loss: 0.47942 | val_0_auc: 0.45669 | val_1_auc: 0.48763 |  0:00:23s\n",
      "epoch 6  | loss: 0.46275 | val_0_auc: 0.46816 | val_1_auc: 0.49198 |  0:00:27s\n",
      "epoch 7  | loss: 0.4543  | val_0_auc: 0.47833 | val_1_auc: 0.51588 |  0:00:31s\n",
      "epoch 8  | loss: 0.43927 | val_0_auc: 0.48924 | val_1_auc: 0.50376 |  0:00:34s\n",
      "epoch 9  | loss: 0.43955 | val_0_auc: 0.48496 | val_1_auc: 0.51703 |  0:00:39s\n",
      "epoch 10 | loss: 0.43119 | val_0_auc: 0.48646 | val_1_auc: 0.53009 |  0:00:43s\n",
      "epoch 11 | loss: 0.42811 | val_0_auc: 0.50632 | val_1_auc: 0.53621 |  0:00:47s\n",
      "epoch 12 | loss: 0.41617 | val_0_auc: 0.50697 | val_1_auc: 0.52308 |  0:00:51s\n",
      "epoch 13 | loss: 0.41475 | val_0_auc: 0.52866 | val_1_auc: 0.53576 |  0:00:54s\n",
      "epoch 14 | loss: 0.4015  | val_0_auc: 0.525   | val_1_auc: 0.52716 |  0:00:58s\n",
      "epoch 15 | loss: 0.3993  | val_0_auc: 0.52342 | val_1_auc: 0.53282 |  0:01:02s\n",
      "epoch 16 | loss: 0.38992 | val_0_auc: 0.52295 | val_1_auc: 0.52991 |  0:01:06s\n",
      "epoch 17 | loss: 0.38894 | val_0_auc: 0.53761 | val_1_auc: 0.54634 |  0:01:10s\n",
      "epoch 18 | loss: 0.38895 | val_0_auc: 0.53901 | val_1_auc: 0.54225 |  0:01:14s\n",
      "epoch 19 | loss: 0.37979 | val_0_auc: 0.53705 | val_1_auc: 0.5504  |  0:01:18s\n",
      "epoch 20 | loss: 0.37965 | val_0_auc: 0.54997 | val_1_auc: 0.55567 |  0:01:22s\n",
      "epoch 21 | loss: 0.38577 | val_0_auc: 0.54815 | val_1_auc: 0.57154 |  0:01:25s\n",
      "epoch 22 | loss: 0.37726 | val_0_auc: 0.53214 | val_1_auc: 0.55923 |  0:01:29s\n",
      "epoch 23 | loss: 0.37305 | val_0_auc: 0.54721 | val_1_auc: 0.56724 |  0:01:34s\n",
      "epoch 24 | loss: 0.37245 | val_0_auc: 0.56918 | val_1_auc: 0.58088 |  0:01:40s\n",
      "epoch 25 | loss: 0.3707  | val_0_auc: 0.5601  | val_1_auc: 0.58126 |  0:01:46s\n",
      "epoch 26 | loss: 0.36901 | val_0_auc: 0.56736 | val_1_auc: 0.55951 |  0:01:51s\n",
      "epoch 27 | loss: 0.36337 | val_0_auc: 0.57096 | val_1_auc: 0.56753 |  0:01:56s\n",
      "epoch 28 | loss: 0.36275 | val_0_auc: 0.56432 | val_1_auc: 0.56609 |  0:02:01s\n",
      "epoch 29 | loss: 0.36811 | val_0_auc: 0.56894 | val_1_auc: 0.57246 |  0:02:07s\n",
      "epoch 30 | loss: 0.35988 | val_0_auc: 0.58592 | val_1_auc: 0.5574  |  0:02:12s\n",
      "epoch 31 | loss: 0.36054 | val_0_auc: 0.5874  | val_1_auc: 0.58399 |  0:02:17s\n",
      "epoch 32 | loss: 0.36062 | val_0_auc: 0.58767 | val_1_auc: 0.56711 |  0:02:22s\n",
      "epoch 33 | loss: 0.35924 | val_0_auc: 0.57671 | val_1_auc: 0.56882 |  0:02:26s\n",
      "epoch 34 | loss: 0.3547  | val_0_auc: 0.59093 | val_1_auc: 0.56516 |  0:02:30s\n",
      "epoch 35 | loss: 0.35545 | val_0_auc: 0.59888 | val_1_auc: 0.55794 |  0:02:35s\n",
      "epoch 36 | loss: 0.35556 | val_0_auc: 0.59881 | val_1_auc: 0.5791  |  0:02:39s\n",
      "epoch 37 | loss: 0.3575  | val_0_auc: 0.59013 | val_1_auc: 0.58374 |  0:02:43s\n",
      "epoch 38 | loss: 0.35273 | val_0_auc: 0.5942  | val_1_auc: 0.57782 |  0:02:47s\n",
      "epoch 39 | loss: 0.35029 | val_0_auc: 0.60666 | val_1_auc: 0.60448 |  0:02:51s\n",
      "epoch 40 | loss: 0.34988 | val_0_auc: 0.6098  | val_1_auc: 0.60569 |  0:02:55s\n",
      "epoch 41 | loss: 0.35046 | val_0_auc: 0.60662 | val_1_auc: 0.5798  |  0:02:59s\n",
      "epoch 42 | loss: 0.3508  | val_0_auc: 0.60262 | val_1_auc: 0.57789 |  0:03:03s\n",
      "epoch 43 | loss: 0.34689 | val_0_auc: 0.60442 | val_1_auc: 0.59415 |  0:03:06s\n",
      "epoch 44 | loss: 0.3479  | val_0_auc: 0.60351 | val_1_auc: 0.59918 |  0:03:10s\n",
      "epoch 45 | loss: 0.34694 | val_0_auc: 0.60724 | val_1_auc: 0.59528 |  0:03:14s\n",
      "epoch 46 | loss: 0.34488 | val_0_auc: 0.60252 | val_1_auc: 0.6168  |  0:03:18s\n",
      "epoch 47 | loss: 0.34687 | val_0_auc: 0.59723 | val_1_auc: 0.62541 |  0:03:22s\n",
      "epoch 48 | loss: 0.34362 | val_0_auc: 0.60516 | val_1_auc: 0.61895 |  0:03:26s\n",
      "epoch 49 | loss: 0.34252 | val_0_auc: 0.60163 | val_1_auc: 0.62737 |  0:03:29s\n",
      "epoch 50 | loss: 0.34345 | val_0_auc: 0.60483 | val_1_auc: 0.62027 |  0:03:33s\n",
      "epoch 51 | loss: 0.34242 | val_0_auc: 0.60834 | val_1_auc: 0.61798 |  0:03:37s\n",
      "epoch 52 | loss: 0.3412  | val_0_auc: 0.61824 | val_1_auc: 0.61222 |  0:03:41s\n",
      "epoch 53 | loss: 0.34009 | val_0_auc: 0.61823 | val_1_auc: 0.64594 |  0:03:45s\n",
      "epoch 54 | loss: 0.34128 | val_0_auc: 0.61446 | val_1_auc: 0.63572 |  0:03:48s\n",
      "epoch 55 | loss: 0.34079 | val_0_auc: 0.62138 | val_1_auc: 0.62282 |  0:03:52s\n",
      "epoch 56 | loss: 0.33966 | val_0_auc: 0.60946 | val_1_auc: 0.61257 |  0:03:56s\n",
      "epoch 57 | loss: 0.33914 | val_0_auc: 0.61542 | val_1_auc: 0.61327 |  0:04:01s\n",
      "epoch 58 | loss: 0.33823 | val_0_auc: 0.61385 | val_1_auc: 0.62799 |  0:04:04s\n",
      "epoch 59 | loss: 0.33908 | val_0_auc: 0.61416 | val_1_auc: 0.62788 |  0:04:08s\n",
      "epoch 60 | loss: 0.33921 | val_0_auc: 0.628   | val_1_auc: 0.62466 |  0:04:12s\n",
      "epoch 61 | loss: 0.34004 | val_0_auc: 0.6207  | val_1_auc: 0.64251 |  0:04:16s\n",
      "epoch 62 | loss: 0.33784 | val_0_auc: 0.62194 | val_1_auc: 0.63655 |  0:04:20s\n",
      "epoch 63 | loss: 0.33725 | val_0_auc: 0.62241 | val_1_auc: 0.6197  |  0:04:24s\n",
      "epoch 64 | loss: 0.33849 | val_0_auc: 0.62372 | val_1_auc: 0.61699 |  0:04:28s\n",
      "epoch 65 | loss: 0.3359  | val_0_auc: 0.62649 | val_1_auc: 0.60721 |  0:04:32s\n",
      "epoch 66 | loss: 0.33569 | val_0_auc: 0.61912 | val_1_auc: 0.62445 |  0:04:36s\n",
      "epoch 67 | loss: 0.33441 | val_0_auc: 0.62723 | val_1_auc: 0.62674 |  0:04:40s\n",
      "epoch 68 | loss: 0.33568 | val_0_auc: 0.62467 | val_1_auc: 0.62243 |  0:04:44s\n",
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 53 and best_val_1_auc = 0.64594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:55:09,889] Trial 19 finished with value: 0.6459398132134211 and parameters: {'n_d': 28, 'n_a': 47, 'n_steps': 5, 'gamma': 1.035343883216773, 'lambda_sparse': 0.0001421104606298977, 'lr': 0.0007393906080326256}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.25728 | val_0_auc: 0.49058 | val_1_auc: 0.5361  |  0:00:04s\n",
      "epoch 1  | loss: 1.18946 | val_0_auc: 0.48208 | val_1_auc: 0.51313 |  0:00:09s\n",
      "epoch 2  | loss: 1.11451 | val_0_auc: 0.49054 | val_1_auc: 0.50608 |  0:00:14s\n",
      "epoch 3  | loss: 1.06723 | val_0_auc: 0.48909 | val_1_auc: 0.49499 |  0:00:18s\n",
      "epoch 4  | loss: 0.98376 | val_0_auc: 0.48516 | val_1_auc: 0.51313 |  0:00:23s\n",
      "epoch 5  | loss: 0.93636 | val_0_auc: 0.48786 | val_1_auc: 0.48158 |  0:00:28s\n",
      "epoch 6  | loss: 0.87994 | val_0_auc: 0.48309 | val_1_auc: 0.50444 |  0:00:33s\n",
      "epoch 7  | loss: 0.8373  | val_0_auc: 0.47971 | val_1_auc: 0.48167 |  0:00:38s\n",
      "epoch 8  | loss: 0.80742 | val_0_auc: 0.48513 | val_1_auc: 0.48345 |  0:00:42s\n",
      "epoch 9  | loss: 0.75895 | val_0_auc: 0.48196 | val_1_auc: 0.47714 |  0:00:47s\n",
      "epoch 10 | loss: 0.74184 | val_0_auc: 0.47659 | val_1_auc: 0.47842 |  0:00:52s\n",
      "epoch 11 | loss: 0.69749 | val_0_auc: 0.48111 | val_1_auc: 0.48486 |  0:00:56s\n",
      "epoch 12 | loss: 0.6608  | val_0_auc: 0.47582 | val_1_auc: 0.47431 |  0:01:01s\n",
      "epoch 13 | loss: 0.63952 | val_0_auc: 0.46748 | val_1_auc: 0.45766 |  0:01:06s\n",
      "epoch 14 | loss: 0.61869 | val_0_auc: 0.47731 | val_1_auc: 0.45994 |  0:01:11s\n",
      "epoch 15 | loss: 0.58287 | val_0_auc: 0.47104 | val_1_auc: 0.46261 |  0:01:15s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 0 and best_val_1_auc = 0.5361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:56:27,553] Trial 20 finished with value: 0.5361010031131097 and parameters: {'n_d': 38, 'n_a': 57, 'n_steps': 6, 'gamma': 1.0249869701702776, 'lambda_sparse': 0.000973025737869804, 'lr': 0.00010328867086041895}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.48796 | val_0_auc: 0.47077 | val_1_auc: 0.47286 |  0:00:02s\n",
      "epoch 1  | loss: 0.42692 | val_0_auc: 0.49719 | val_1_auc: 0.49996 |  0:00:05s\n",
      "epoch 2  | loss: 0.39384 | val_0_auc: 0.51568 | val_1_auc: 0.55101 |  0:00:07s\n",
      "epoch 3  | loss: 0.38127 | val_0_auc: 0.52165 | val_1_auc: 0.54561 |  0:00:10s\n",
      "epoch 4  | loss: 0.37244 | val_0_auc: 0.53664 | val_1_auc: 0.56332 |  0:00:13s\n",
      "epoch 5  | loss: 0.36157 | val_0_auc: 0.53816 | val_1_auc: 0.56141 |  0:00:15s\n",
      "epoch 6  | loss: 0.3587  | val_0_auc: 0.56151 | val_1_auc: 0.56429 |  0:00:18s\n",
      "epoch 7  | loss: 0.35367 | val_0_auc: 0.56579 | val_1_auc: 0.57295 |  0:00:21s\n",
      "epoch 8  | loss: 0.35137 | val_0_auc: 0.58188 | val_1_auc: 0.58148 |  0:00:24s\n",
      "epoch 9  | loss: 0.34558 | val_0_auc: 0.59735 | val_1_auc: 0.57853 |  0:00:26s\n",
      "epoch 10 | loss: 0.34256 | val_0_auc: 0.60753 | val_1_auc: 0.57551 |  0:00:29s\n",
      "epoch 11 | loss: 0.34033 | val_0_auc: 0.60844 | val_1_auc: 0.57225 |  0:00:32s\n",
      "epoch 12 | loss: 0.34183 | val_0_auc: 0.61392 | val_1_auc: 0.58432 |  0:00:34s\n",
      "epoch 13 | loss: 0.33887 | val_0_auc: 0.61014 | val_1_auc: 0.57511 |  0:00:37s\n",
      "epoch 14 | loss: 0.33979 | val_0_auc: 0.60901 | val_1_auc: 0.58844 |  0:00:41s\n",
      "epoch 15 | loss: 0.33867 | val_0_auc: 0.62126 | val_1_auc: 0.59505 |  0:00:44s\n",
      "epoch 16 | loss: 0.3351  | val_0_auc: 0.62795 | val_1_auc: 0.60117 |  0:00:47s\n",
      "epoch 17 | loss: 0.33403 | val_0_auc: 0.6314  | val_1_auc: 0.59237 |  0:00:50s\n",
      "epoch 18 | loss: 0.33408 | val_0_auc: 0.61228 | val_1_auc: 0.59835 |  0:00:53s\n",
      "epoch 19 | loss: 0.33358 | val_0_auc: 0.62197 | val_1_auc: 0.59917 |  0:00:56s\n",
      "epoch 20 | loss: 0.33144 | val_0_auc: 0.62291 | val_1_auc: 0.60128 |  0:00:59s\n",
      "epoch 21 | loss: 0.33338 | val_0_auc: 0.61992 | val_1_auc: 0.59823 |  0:01:02s\n",
      "epoch 22 | loss: 0.33117 | val_0_auc: 0.62638 | val_1_auc: 0.60659 |  0:01:05s\n",
      "epoch 23 | loss: 0.32958 | val_0_auc: 0.63142 | val_1_auc: 0.60206 |  0:01:08s\n",
      "epoch 24 | loss: 0.32773 | val_0_auc: 0.62973 | val_1_auc: 0.61167 |  0:01:11s\n",
      "epoch 25 | loss: 0.32818 | val_0_auc: 0.63425 | val_1_auc: 0.60393 |  0:01:14s\n",
      "epoch 26 | loss: 0.32783 | val_0_auc: 0.63435 | val_1_auc: 0.60508 |  0:01:17s\n",
      "epoch 27 | loss: 0.32658 | val_0_auc: 0.62514 | val_1_auc: 0.61113 |  0:01:20s\n",
      "epoch 28 | loss: 0.32607 | val_0_auc: 0.63926 | val_1_auc: 0.61008 |  0:01:23s\n",
      "epoch 29 | loss: 0.32624 | val_0_auc: 0.63741 | val_1_auc: 0.61685 |  0:01:26s\n",
      "epoch 30 | loss: 0.32461 | val_0_auc: 0.63671 | val_1_auc: 0.61737 |  0:01:29s\n",
      "epoch 31 | loss: 0.32706 | val_0_auc: 0.62823 | val_1_auc: 0.60893 |  0:01:32s\n",
      "epoch 32 | loss: 0.32515 | val_0_auc: 0.63297 | val_1_auc: 0.62103 |  0:01:35s\n",
      "epoch 33 | loss: 0.32428 | val_0_auc: 0.63614 | val_1_auc: 0.62785 |  0:01:38s\n",
      "epoch 34 | loss: 0.32465 | val_0_auc: 0.64303 | val_1_auc: 0.63549 |  0:01:41s\n",
      "epoch 35 | loss: 0.32414 | val_0_auc: 0.64488 | val_1_auc: 0.62761 |  0:01:44s\n",
      "epoch 36 | loss: 0.32452 | val_0_auc: 0.64707 | val_1_auc: 0.61955 |  0:01:47s\n",
      "epoch 37 | loss: 0.32266 | val_0_auc: 0.65235 | val_1_auc: 0.62275 |  0:01:50s\n",
      "epoch 38 | loss: 0.32194 | val_0_auc: 0.65448 | val_1_auc: 0.61915 |  0:01:53s\n",
      "epoch 39 | loss: 0.32379 | val_0_auc: 0.64786 | val_1_auc: 0.62198 |  0:01:56s\n",
      "epoch 40 | loss: 0.32316 | val_0_auc: 0.64869 | val_1_auc: 0.6192  |  0:01:59s\n",
      "epoch 41 | loss: 0.32282 | val_0_auc: 0.65127 | val_1_auc: 0.61925 |  0:02:02s\n",
      "epoch 42 | loss: 0.31973 | val_0_auc: 0.6554  | val_1_auc: 0.62631 |  0:02:05s\n",
      "epoch 43 | loss: 0.32273 | val_0_auc: 0.65371 | val_1_auc: 0.62493 |  0:02:08s\n",
      "epoch 44 | loss: 0.32219 | val_0_auc: 0.65327 | val_1_auc: 0.62623 |  0:02:11s\n",
      "epoch 45 | loss: 0.3212  | val_0_auc: 0.65765 | val_1_auc: 0.63186 |  0:02:14s\n",
      "epoch 46 | loss: 0.3215  | val_0_auc: 0.6528  | val_1_auc: 0.62449 |  0:02:17s\n",
      "epoch 47 | loss: 0.31789 | val_0_auc: 0.65575 | val_1_auc: 0.6232  |  0:02:20s\n",
      "epoch 48 | loss: 0.31922 | val_0_auc: 0.65379 | val_1_auc: 0.61979 |  0:02:23s\n",
      "epoch 49 | loss: 0.31851 | val_0_auc: 0.64989 | val_1_auc: 0.63654 |  0:02:26s\n",
      "epoch 50 | loss: 0.31768 | val_0_auc: 0.65062 | val_1_auc: 0.63442 |  0:02:29s\n",
      "epoch 51 | loss: 0.31838 | val_0_auc: 0.64651 | val_1_auc: 0.63076 |  0:02:32s\n",
      "epoch 52 | loss: 0.3187  | val_0_auc: 0.65183 | val_1_auc: 0.63429 |  0:02:35s\n",
      "epoch 53 | loss: 0.31803 | val_0_auc: 0.6533  | val_1_auc: 0.62581 |  0:02:38s\n",
      "epoch 54 | loss: 0.3163  | val_0_auc: 0.65317 | val_1_auc: 0.62137 |  0:02:41s\n",
      "epoch 55 | loss: 0.31689 | val_0_auc: 0.64799 | val_1_auc: 0.61676 |  0:02:44s\n",
      "epoch 56 | loss: 0.31737 | val_0_auc: 0.65516 | val_1_auc: 0.61913 |  0:02:48s\n",
      "epoch 57 | loss: 0.31609 | val_0_auc: 0.65368 | val_1_auc: 0.62549 |  0:02:50s\n",
      "epoch 58 | loss: 0.31523 | val_0_auc: 0.65864 | val_1_auc: 0.62977 |  0:02:53s\n",
      "epoch 59 | loss: 0.31748 | val_0_auc: 0.65759 | val_1_auc: 0.62426 |  0:02:56s\n",
      "epoch 60 | loss: 0.31512 | val_0_auc: 0.658   | val_1_auc: 0.62497 |  0:02:59s\n",
      "epoch 61 | loss: 0.31591 | val_0_auc: 0.65625 | val_1_auc: 0.6192  |  0:03:03s\n",
      "epoch 62 | loss: 0.31486 | val_0_auc: 0.65536 | val_1_auc: 0.62672 |  0:03:06s\n",
      "epoch 63 | loss: 0.31399 | val_0_auc: 0.65463 | val_1_auc: 0.63024 |  0:03:09s\n",
      "epoch 64 | loss: 0.31504 | val_0_auc: 0.65319 | val_1_auc: 0.62604 |  0:03:12s\n",
      "\n",
      "Early stopping occurred at epoch 64 with best_epoch = 49 and best_val_1_auc = 0.63654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 13:59:40,802] Trial 21 finished with value: 0.6365396056727776 and parameters: {'n_d': 47, 'n_a': 45, 'n_steps': 3, 'gamma': 1.5800268504426207, 'lambda_sparse': 0.0003577566395603807, 'lr': 0.0017348382171154188}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.49905 | val_0_auc: 0.46529 | val_1_auc: 0.48217 |  0:00:02s\n",
      "epoch 1  | loss: 0.42889 | val_0_auc: 0.49206 | val_1_auc: 0.52847 |  0:00:06s\n",
      "epoch 2  | loss: 0.39455 | val_0_auc: 0.50297 | val_1_auc: 0.53114 |  0:00:09s\n",
      "epoch 3  | loss: 0.37192 | val_0_auc: 0.50788 | val_1_auc: 0.54878 |  0:00:12s\n",
      "epoch 4  | loss: 0.36497 | val_0_auc: 0.52001 | val_1_auc: 0.5518  |  0:00:15s\n",
      "epoch 5  | loss: 0.35747 | val_0_auc: 0.52818 | val_1_auc: 0.54497 |  0:00:18s\n",
      "epoch 6  | loss: 0.35089 | val_0_auc: 0.55017 | val_1_auc: 0.56386 |  0:00:21s\n",
      "epoch 7  | loss: 0.34788 | val_0_auc: 0.55902 | val_1_auc: 0.5689  |  0:00:24s\n",
      "epoch 8  | loss: 0.34456 | val_0_auc: 0.56922 | val_1_auc: 0.57762 |  0:00:27s\n",
      "epoch 9  | loss: 0.34051 | val_0_auc: 0.5732  | val_1_auc: 0.61152 |  0:00:30s\n",
      "epoch 10 | loss: 0.33832 | val_0_auc: 0.59438 | val_1_auc: 0.62078 |  0:00:33s\n",
      "epoch 11 | loss: 0.33493 | val_0_auc: 0.59934 | val_1_auc: 0.63949 |  0:00:36s\n",
      "epoch 12 | loss: 0.33484 | val_0_auc: 0.61552 | val_1_auc: 0.63252 |  0:00:39s\n",
      "epoch 13 | loss: 0.33172 | val_0_auc: 0.61693 | val_1_auc: 0.61399 |  0:00:42s\n",
      "epoch 14 | loss: 0.33275 | val_0_auc: 0.62563 | val_1_auc: 0.6171  |  0:00:45s\n",
      "epoch 15 | loss: 0.33182 | val_0_auc: 0.62599 | val_1_auc: 0.62387 |  0:00:48s\n",
      "epoch 16 | loss: 0.32902 | val_0_auc: 0.63282 | val_1_auc: 0.6346  |  0:00:51s\n",
      "epoch 17 | loss: 0.32897 | val_0_auc: 0.6305  | val_1_auc: 0.63979 |  0:00:54s\n",
      "epoch 18 | loss: 0.32758 | val_0_auc: 0.63292 | val_1_auc: 0.64174 |  0:00:57s\n",
      "epoch 19 | loss: 0.32737 | val_0_auc: 0.6375  | val_1_auc: 0.64452 |  0:01:00s\n",
      "epoch 20 | loss: 0.32738 | val_0_auc: 0.64051 | val_1_auc: 0.6394  |  0:01:03s\n",
      "epoch 21 | loss: 0.32736 | val_0_auc: 0.64751 | val_1_auc: 0.65766 |  0:01:06s\n",
      "epoch 22 | loss: 0.3257  | val_0_auc: 0.64346 | val_1_auc: 0.64706 |  0:01:09s\n",
      "epoch 23 | loss: 0.32545 | val_0_auc: 0.64934 | val_1_auc: 0.64566 |  0:01:12s\n",
      "epoch 24 | loss: 0.32303 | val_0_auc: 0.6499  | val_1_auc: 0.64322 |  0:01:15s\n",
      "epoch 25 | loss: 0.32293 | val_0_auc: 0.64853 | val_1_auc: 0.64852 |  0:01:18s\n",
      "epoch 26 | loss: 0.32097 | val_0_auc: 0.64986 | val_1_auc: 0.64456 |  0:01:21s\n",
      "epoch 27 | loss: 0.32081 | val_0_auc: 0.65013 | val_1_auc: 0.65132 |  0:01:24s\n",
      "epoch 28 | loss: 0.32071 | val_0_auc: 0.65064 | val_1_auc: 0.64075 |  0:01:27s\n",
      "epoch 29 | loss: 0.32126 | val_0_auc: 0.65969 | val_1_auc: 0.64145 |  0:01:30s\n",
      "epoch 30 | loss: 0.31897 | val_0_auc: 0.65546 | val_1_auc: 0.64524 |  0:01:33s\n",
      "epoch 31 | loss: 0.31978 | val_0_auc: 0.6507  | val_1_auc: 0.63689 |  0:01:36s\n",
      "epoch 32 | loss: 0.31974 | val_0_auc: 0.64765 | val_1_auc: 0.63414 |  0:01:39s\n",
      "epoch 33 | loss: 0.31899 | val_0_auc: 0.65883 | val_1_auc: 0.64627 |  0:01:42s\n",
      "epoch 34 | loss: 0.31839 | val_0_auc: 0.65703 | val_1_auc: 0.64691 |  0:01:45s\n",
      "epoch 35 | loss: 0.31938 | val_0_auc: 0.65572 | val_1_auc: 0.65303 |  0:01:48s\n",
      "epoch 36 | loss: 0.31728 | val_0_auc: 0.65293 | val_1_auc: 0.65958 |  0:01:51s\n",
      "epoch 37 | loss: 0.31757 | val_0_auc: 0.65786 | val_1_auc: 0.66044 |  0:01:54s\n",
      "epoch 38 | loss: 0.31724 | val_0_auc: 0.6533  | val_1_auc: 0.64251 |  0:01:57s\n",
      "epoch 39 | loss: 0.31715 | val_0_auc: 0.65989 | val_1_auc: 0.64362 |  0:02:00s\n",
      "epoch 40 | loss: 0.31509 | val_0_auc: 0.65813 | val_1_auc: 0.65091 |  0:02:03s\n",
      "epoch 41 | loss: 0.3169  | val_0_auc: 0.65249 | val_1_auc: 0.65878 |  0:02:06s\n",
      "epoch 42 | loss: 0.31612 | val_0_auc: 0.65646 | val_1_auc: 0.66014 |  0:02:09s\n",
      "epoch 43 | loss: 0.31356 | val_0_auc: 0.65741 | val_1_auc: 0.66052 |  0:02:12s\n",
      "epoch 44 | loss: 0.3162  | val_0_auc: 0.66292 | val_1_auc: 0.66523 |  0:02:15s\n",
      "epoch 45 | loss: 0.31492 | val_0_auc: 0.66084 | val_1_auc: 0.66948 |  0:02:17s\n",
      "epoch 46 | loss: 0.31292 | val_0_auc: 0.6608  | val_1_auc: 0.67433 |  0:02:20s\n",
      "epoch 47 | loss: 0.31174 | val_0_auc: 0.66299 | val_1_auc: 0.67955 |  0:02:23s\n",
      "epoch 48 | loss: 0.31275 | val_0_auc: 0.65705 | val_1_auc: 0.68182 |  0:02:26s\n",
      "epoch 49 | loss: 0.31132 | val_0_auc: 0.65506 | val_1_auc: 0.67542 |  0:02:29s\n",
      "epoch 50 | loss: 0.3109  | val_0_auc: 0.66116 | val_1_auc: 0.67101 |  0:02:32s\n",
      "epoch 51 | loss: 0.31267 | val_0_auc: 0.66251 | val_1_auc: 0.66652 |  0:02:35s\n",
      "epoch 52 | loss: 0.31204 | val_0_auc: 0.66375 | val_1_auc: 0.67837 |  0:02:38s\n",
      "epoch 53 | loss: 0.31124 | val_0_auc: 0.66764 | val_1_auc: 0.67479 |  0:02:41s\n",
      "epoch 54 | loss: 0.31166 | val_0_auc: 0.66972 | val_1_auc: 0.67045 |  0:02:44s\n",
      "epoch 55 | loss: 0.31067 | val_0_auc: 0.67056 | val_1_auc: 0.67263 |  0:02:47s\n",
      "epoch 56 | loss: 0.30799 | val_0_auc: 0.67313 | val_1_auc: 0.66367 |  0:02:50s\n",
      "epoch 57 | loss: 0.31048 | val_0_auc: 0.6754  | val_1_auc: 0.66437 |  0:02:53s\n",
      "epoch 58 | loss: 0.30897 | val_0_auc: 0.6715  | val_1_auc: 0.66516 |  0:02:56s\n",
      "epoch 59 | loss: 0.30738 | val_0_auc: 0.66948 | val_1_auc: 0.66273 |  0:02:59s\n",
      "epoch 60 | loss: 0.30848 | val_0_auc: 0.66587 | val_1_auc: 0.66882 |  0:03:02s\n",
      "epoch 61 | loss: 0.30801 | val_0_auc: 0.6652  | val_1_auc: 0.66872 |  0:03:05s\n",
      "epoch 62 | loss: 0.30805 | val_0_auc: 0.66645 | val_1_auc: 0.66439 |  0:03:08s\n",
      "epoch 63 | loss: 0.30685 | val_0_auc: 0.66389 | val_1_auc: 0.66291 |  0:03:11s\n",
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 48 and best_val_1_auc = 0.68182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:02:53,352] Trial 22 finished with value: 0.6818152888273954 and parameters: {'n_d': 40, 'n_a': 38, 'n_steps': 3, 'gamma': 2.021646732823936, 'lambda_sparse': 0.0003203240871864178, 'lr': 0.002985471263173699}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.243   | val_0_auc: 0.47355 | val_1_auc: 0.49595 |  0:00:04s\n",
      "epoch 1  | loss: 0.53182 | val_0_auc: 0.48918 | val_1_auc: 0.48698 |  0:00:08s\n",
      "epoch 2  | loss: 0.49518 | val_0_auc: 0.51701 | val_1_auc: 0.47054 |  0:00:13s\n",
      "epoch 3  | loss: 0.44078 | val_0_auc: 0.52329 | val_1_auc: 0.51433 |  0:00:17s\n",
      "epoch 4  | loss: 0.41194 | val_0_auc: 0.52981 | val_1_auc: 0.52163 |  0:00:22s\n",
      "epoch 5  | loss: 0.39902 | val_0_auc: 0.54794 | val_1_auc: 0.52874 |  0:00:26s\n",
      "epoch 6  | loss: 0.39276 | val_0_auc: 0.51836 | val_1_auc: 0.51113 |  0:00:31s\n",
      "epoch 7  | loss: 0.38581 | val_0_auc: 0.5519  | val_1_auc: 0.54271 |  0:00:35s\n",
      "epoch 8  | loss: 0.38111 | val_0_auc: 0.5285  | val_1_auc: 0.55823 |  0:00:40s\n",
      "epoch 9  | loss: 0.3708  | val_0_auc: 0.53824 | val_1_auc: 0.54962 |  0:00:44s\n",
      "epoch 10 | loss: 0.37237 | val_0_auc: 0.55343 | val_1_auc: 0.57833 |  0:00:49s\n",
      "epoch 11 | loss: 0.36421 | val_0_auc: 0.55833 | val_1_auc: 0.5854  |  0:00:53s\n",
      "epoch 12 | loss: 0.359   | val_0_auc: 0.56889 | val_1_auc: 0.57552 |  0:00:57s\n",
      "epoch 13 | loss: 0.35884 | val_0_auc: 0.56861 | val_1_auc: 0.57136 |  0:01:02s\n",
      "epoch 14 | loss: 0.35804 | val_0_auc: 0.59304 | val_1_auc: 0.58991 |  0:01:06s\n",
      "epoch 15 | loss: 0.35291 | val_0_auc: 0.6005  | val_1_auc: 0.59813 |  0:01:11s\n",
      "epoch 16 | loss: 0.34957 | val_0_auc: 0.6062  | val_1_auc: 0.59465 |  0:01:15s\n",
      "epoch 17 | loss: 0.34901 | val_0_auc: 0.60493 | val_1_auc: 0.58565 |  0:01:20s\n",
      "epoch 18 | loss: 0.34546 | val_0_auc: 0.61148 | val_1_auc: 0.59063 |  0:01:24s\n",
      "epoch 19 | loss: 0.34604 | val_0_auc: 0.60555 | val_1_auc: 0.59454 |  0:01:29s\n",
      "epoch 20 | loss: 0.34681 | val_0_auc: 0.61163 | val_1_auc: 0.57578 |  0:01:33s\n",
      "epoch 21 | loss: 0.34213 | val_0_auc: 0.61061 | val_1_auc: 0.5811  |  0:01:38s\n",
      "epoch 22 | loss: 0.3439  | val_0_auc: 0.60147 | val_1_auc: 0.59277 |  0:01:42s\n",
      "epoch 23 | loss: 0.34074 | val_0_auc: 0.61293 | val_1_auc: 0.6014  |  0:01:46s\n",
      "epoch 24 | loss: 0.33928 | val_0_auc: 0.6218  | val_1_auc: 0.6099  |  0:01:51s\n",
      "epoch 25 | loss: 0.34013 | val_0_auc: 0.60534 | val_1_auc: 0.61043 |  0:01:56s\n",
      "epoch 26 | loss: 0.33836 | val_0_auc: 0.59569 | val_1_auc: 0.618   |  0:02:01s\n",
      "epoch 27 | loss: 0.3359  | val_0_auc: 0.60055 | val_1_auc: 0.59288 |  0:02:05s\n",
      "epoch 28 | loss: 0.33905 | val_0_auc: 0.58945 | val_1_auc: 0.57794 |  0:02:10s\n",
      "epoch 29 | loss: 0.33775 | val_0_auc: 0.58879 | val_1_auc: 0.5719  |  0:02:14s\n",
      "epoch 30 | loss: 0.33872 | val_0_auc: 0.6095  | val_1_auc: 0.60299 |  0:02:19s\n",
      "epoch 31 | loss: 0.33402 | val_0_auc: 0.61002 | val_1_auc: 0.61999 |  0:02:23s\n",
      "epoch 32 | loss: 0.33465 | val_0_auc: 0.62275 | val_1_auc: 0.61388 |  0:02:28s\n",
      "epoch 33 | loss: 0.3374  | val_0_auc: 0.62438 | val_1_auc: 0.61128 |  0:02:32s\n",
      "epoch 34 | loss: 0.33351 | val_0_auc: 0.61789 | val_1_auc: 0.62436 |  0:02:36s\n",
      "epoch 35 | loss: 0.33459 | val_0_auc: 0.6184  | val_1_auc: 0.61268 |  0:02:41s\n",
      "epoch 36 | loss: 0.33628 | val_0_auc: 0.60267 | val_1_auc: 0.61823 |  0:02:45s\n",
      "epoch 37 | loss: 0.3399  | val_0_auc: 0.63011 | val_1_auc: 0.64341 |  0:02:50s\n",
      "epoch 38 | loss: 0.33396 | val_0_auc: 0.63248 | val_1_auc: 0.64429 |  0:02:54s\n",
      "epoch 39 | loss: 0.33119 | val_0_auc: 0.64055 | val_1_auc: 0.6412  |  0:02:59s\n",
      "epoch 40 | loss: 0.33144 | val_0_auc: 0.64024 | val_1_auc: 0.64268 |  0:03:03s\n",
      "epoch 41 | loss: 0.33337 | val_0_auc: 0.63581 | val_1_auc: 0.6302  |  0:03:07s\n",
      "epoch 42 | loss: 0.33122 | val_0_auc: 0.62699 | val_1_auc: 0.63975 |  0:03:12s\n",
      "epoch 43 | loss: 0.33402 | val_0_auc: 0.6464  | val_1_auc: 0.6491  |  0:03:16s\n",
      "epoch 44 | loss: 0.33181 | val_0_auc: 0.6429  | val_1_auc: 0.64343 |  0:03:20s\n",
      "epoch 45 | loss: 0.33232 | val_0_auc: 0.64137 | val_1_auc: 0.63593 |  0:03:25s\n",
      "epoch 46 | loss: 0.32928 | val_0_auc: 0.63016 | val_1_auc: 0.63299 |  0:03:29s\n",
      "epoch 47 | loss: 0.33046 | val_0_auc: 0.64271 | val_1_auc: 0.62753 |  0:03:34s\n",
      "epoch 48 | loss: 0.33014 | val_0_auc: 0.63281 | val_1_auc: 0.63771 |  0:03:38s\n",
      "epoch 49 | loss: 0.33007 | val_0_auc: 0.6391  | val_1_auc: 0.63945 |  0:03:43s\n",
      "epoch 50 | loss: 0.33023 | val_0_auc: 0.63714 | val_1_auc: 0.6185  |  0:03:47s\n",
      "epoch 51 | loss: 0.32957 | val_0_auc: 0.63926 | val_1_auc: 0.61718 |  0:03:51s\n",
      "epoch 52 | loss: 0.33034 | val_0_auc: 0.64448 | val_1_auc: 0.61093 |  0:03:56s\n",
      "epoch 53 | loss: 0.33029 | val_0_auc: 0.63363 | val_1_auc: 0.61556 |  0:04:00s\n",
      "epoch 54 | loss: 0.32868 | val_0_auc: 0.63846 | val_1_auc: 0.62388 |  0:04:05s\n",
      "epoch 55 | loss: 0.32581 | val_0_auc: 0.62715 | val_1_auc: 0.63609 |  0:04:09s\n",
      "epoch 56 | loss: 0.32801 | val_0_auc: 0.64178 | val_1_auc: 0.62352 |  0:04:14s\n",
      "epoch 57 | loss: 0.32642 | val_0_auc: 0.64564 | val_1_auc: 0.62898 |  0:04:18s\n",
      "epoch 58 | loss: 0.32844 | val_0_auc: 0.64243 | val_1_auc: 0.63221 |  0:04:23s\n",
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 43 and best_val_1_auc = 0.6491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:07:18,019] Trial 23 finished with value: 0.6490958145970251 and parameters: {'n_d': 47, 'n_a': 37, 'n_steps': 5, 'gamma': 2.1046011735050776, 'lambda_sparse': 6.251841571064439e-05, 'lr': 0.0028484234910258724}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.55696 | val_0_auc: 0.50591 | val_1_auc: 0.52278 |  0:00:03s\n",
      "epoch 1  | loss: 0.52957 | val_0_auc: 0.47742 | val_1_auc: 0.46012 |  0:00:06s\n",
      "epoch 2  | loss: 0.41589 | val_0_auc: 0.51035 | val_1_auc: 0.50576 |  0:00:09s\n",
      "epoch 3  | loss: 0.39484 | val_0_auc: 0.55671 | val_1_auc: 0.53208 |  0:00:12s\n",
      "epoch 4  | loss: 0.37396 | val_0_auc: 0.55329 | val_1_auc: 0.53218 |  0:00:15s\n",
      "epoch 5  | loss: 0.36547 | val_0_auc: 0.56123 | val_1_auc: 0.5419  |  0:00:18s\n",
      "epoch 6  | loss: 0.35932 | val_0_auc: 0.5778  | val_1_auc: 0.56696 |  0:00:21s\n",
      "epoch 7  | loss: 0.3541  | val_0_auc: 0.56005 | val_1_auc: 0.55648 |  0:00:24s\n",
      "epoch 8  | loss: 0.35246 | val_0_auc: 0.58184 | val_1_auc: 0.58172 |  0:00:27s\n",
      "epoch 9  | loss: 0.34787 | val_0_auc: 0.59018 | val_1_auc: 0.59259 |  0:00:30s\n",
      "epoch 10 | loss: 0.34623 | val_0_auc: 0.59876 | val_1_auc: 0.59884 |  0:00:33s\n",
      "epoch 11 | loss: 0.34273 | val_0_auc: 0.60068 | val_1_auc: 0.56272 |  0:00:36s\n",
      "epoch 12 | loss: 0.34099 | val_0_auc: 0.58499 | val_1_auc: 0.56474 |  0:00:39s\n",
      "epoch 13 | loss: 0.34116 | val_0_auc: 0.6049  | val_1_auc: 0.59667 |  0:00:42s\n",
      "epoch 14 | loss: 0.33647 | val_0_auc: 0.62799 | val_1_auc: 0.6013  |  0:00:45s\n",
      "epoch 15 | loss: 0.33707 | val_0_auc: 0.62276 | val_1_auc: 0.612   |  0:00:48s\n",
      "epoch 16 | loss: 0.33517 | val_0_auc: 0.63025 | val_1_auc: 0.61574 |  0:00:51s\n",
      "epoch 17 | loss: 0.33597 | val_0_auc: 0.64416 | val_1_auc: 0.61212 |  0:00:54s\n",
      "epoch 18 | loss: 0.33099 | val_0_auc: 0.63022 | val_1_auc: 0.62422 |  0:00:57s\n",
      "epoch 19 | loss: 0.33239 | val_0_auc: 0.63269 | val_1_auc: 0.62788 |  0:00:59s\n",
      "epoch 20 | loss: 0.33193 | val_0_auc: 0.63732 | val_1_auc: 0.63498 |  0:01:02s\n",
      "epoch 21 | loss: 0.33201 | val_0_auc: 0.62871 | val_1_auc: 0.63624 |  0:01:05s\n",
      "epoch 22 | loss: 0.33036 | val_0_auc: 0.62709 | val_1_auc: 0.62873 |  0:01:08s\n",
      "epoch 23 | loss: 0.33068 | val_0_auc: 0.63518 | val_1_auc: 0.63222 |  0:01:11s\n",
      "epoch 24 | loss: 0.33146 | val_0_auc: 0.63519 | val_1_auc: 0.63193 |  0:01:14s\n",
      "epoch 25 | loss: 0.33157 | val_0_auc: 0.63718 | val_1_auc: 0.63483 |  0:01:17s\n",
      "epoch 26 | loss: 0.32906 | val_0_auc: 0.64859 | val_1_auc: 0.63254 |  0:01:20s\n",
      "epoch 27 | loss: 0.33064 | val_0_auc: 0.64794 | val_1_auc: 0.64109 |  0:01:23s\n",
      "epoch 28 | loss: 0.32845 | val_0_auc: 0.63766 | val_1_auc: 0.63105 |  0:01:26s\n",
      "epoch 29 | loss: 0.32774 | val_0_auc: 0.63658 | val_1_auc: 0.64612 |  0:01:29s\n",
      "epoch 30 | loss: 0.32864 | val_0_auc: 0.63676 | val_1_auc: 0.62851 |  0:01:32s\n",
      "epoch 31 | loss: 0.32672 | val_0_auc: 0.64769 | val_1_auc: 0.63563 |  0:01:35s\n",
      "epoch 32 | loss: 0.32458 | val_0_auc: 0.6445  | val_1_auc: 0.62974 |  0:01:38s\n",
      "epoch 33 | loss: 0.3238  | val_0_auc: 0.64987 | val_1_auc: 0.62213 |  0:01:41s\n",
      "epoch 34 | loss: 0.32523 | val_0_auc: 0.6534  | val_1_auc: 0.64128 |  0:01:44s\n",
      "epoch 35 | loss: 0.32257 | val_0_auc: 0.65808 | val_1_auc: 0.64031 |  0:01:47s\n",
      "epoch 36 | loss: 0.32304 | val_0_auc: 0.65195 | val_1_auc: 0.63633 |  0:01:50s\n",
      "epoch 37 | loss: 0.32232 | val_0_auc: 0.65234 | val_1_auc: 0.64742 |  0:01:53s\n",
      "epoch 38 | loss: 0.32158 | val_0_auc: 0.64332 | val_1_auc: 0.64481 |  0:01:56s\n",
      "epoch 39 | loss: 0.32176 | val_0_auc: 0.64365 | val_1_auc: 0.64585 |  0:01:59s\n",
      "epoch 40 | loss: 0.31984 | val_0_auc: 0.65244 | val_1_auc: 0.64975 |  0:02:02s\n",
      "epoch 41 | loss: 0.32059 | val_0_auc: 0.65179 | val_1_auc: 0.65596 |  0:02:05s\n",
      "epoch 42 | loss: 0.31911 | val_0_auc: 0.65345 | val_1_auc: 0.64443 |  0:02:08s\n",
      "epoch 43 | loss: 0.32045 | val_0_auc: 0.6553  | val_1_auc: 0.64159 |  0:02:11s\n",
      "epoch 44 | loss: 0.31794 | val_0_auc: 0.66121 | val_1_auc: 0.64952 |  0:02:14s\n",
      "epoch 45 | loss: 0.31844 | val_0_auc: 0.66199 | val_1_auc: 0.65909 |  0:02:17s\n",
      "epoch 46 | loss: 0.31887 | val_0_auc: 0.65155 | val_1_auc: 0.65675 |  0:02:20s\n",
      "epoch 47 | loss: 0.31817 | val_0_auc: 0.65238 | val_1_auc: 0.65646 |  0:02:23s\n",
      "epoch 48 | loss: 0.31784 | val_0_auc: 0.65502 | val_1_auc: 0.65814 |  0:02:26s\n",
      "epoch 49 | loss: 0.31801 | val_0_auc: 0.65677 | val_1_auc: 0.65608 |  0:02:29s\n",
      "epoch 50 | loss: 0.3178  | val_0_auc: 0.64818 | val_1_auc: 0.64989 |  0:02:32s\n",
      "epoch 51 | loss: 0.31727 | val_0_auc: 0.65127 | val_1_auc: 0.65462 |  0:02:35s\n",
      "epoch 52 | loss: 0.3174  | val_0_auc: 0.6581  | val_1_auc: 0.64564 |  0:02:38s\n",
      "epoch 53 | loss: 0.31578 | val_0_auc: 0.65904 | val_1_auc: 0.64861 |  0:02:41s\n",
      "epoch 54 | loss: 0.31617 | val_0_auc: 0.65262 | val_1_auc: 0.65752 |  0:02:44s\n",
      "epoch 55 | loss: 0.31375 | val_0_auc: 0.64991 | val_1_auc: 0.6595  |  0:02:47s\n",
      "epoch 56 | loss: 0.31494 | val_0_auc: 0.64768 | val_1_auc: 0.66091 |  0:02:50s\n",
      "epoch 57 | loss: 0.31594 | val_0_auc: 0.64996 | val_1_auc: 0.65951 |  0:02:53s\n",
      "epoch 58 | loss: 0.31391 | val_0_auc: 0.65269 | val_1_auc: 0.65942 |  0:02:56s\n",
      "epoch 59 | loss: 0.31434 | val_0_auc: 0.65656 | val_1_auc: 0.66436 |  0:02:59s\n",
      "epoch 60 | loss: 0.31361 | val_0_auc: 0.66016 | val_1_auc: 0.64922 |  0:03:02s\n",
      "epoch 61 | loss: 0.31251 | val_0_auc: 0.66012 | val_1_auc: 0.642   |  0:03:05s\n",
      "epoch 62 | loss: 0.31282 | val_0_auc: 0.661   | val_1_auc: 0.64959 |  0:03:08s\n",
      "epoch 63 | loss: 0.31379 | val_0_auc: 0.6601  | val_1_auc: 0.65504 |  0:03:11s\n",
      "epoch 64 | loss: 0.31329 | val_0_auc: 0.66108 | val_1_auc: 0.65685 |  0:03:14s\n",
      "epoch 65 | loss: 0.31203 | val_0_auc: 0.66066 | val_1_auc: 0.65192 |  0:03:16s\n",
      "epoch 66 | loss: 0.31404 | val_0_auc: 0.6618  | val_1_auc: 0.65422 |  0:03:19s\n",
      "epoch 67 | loss: 0.31206 | val_0_auc: 0.6612  | val_1_auc: 0.66215 |  0:03:22s\n",
      "epoch 68 | loss: 0.31113 | val_0_auc: 0.66243 | val_1_auc: 0.66386 |  0:03:25s\n",
      "epoch 69 | loss: 0.31066 | val_0_auc: 0.65786 | val_1_auc: 0.65114 |  0:03:28s\n",
      "epoch 70 | loss: 0.31074 | val_0_auc: 0.65832 | val_1_auc: 0.65295 |  0:03:31s\n",
      "epoch 71 | loss: 0.30821 | val_0_auc: 0.65935 | val_1_auc: 0.65569 |  0:03:34s\n",
      "epoch 72 | loss: 0.30926 | val_0_auc: 0.65877 | val_1_auc: 0.65944 |  0:03:37s\n",
      "epoch 73 | loss: 0.3075  | val_0_auc: 0.65835 | val_1_auc: 0.65583 |  0:03:40s\n",
      "epoch 74 | loss: 0.30746 | val_0_auc: 0.65898 | val_1_auc: 0.65084 |  0:03:43s\n",
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 59 and best_val_1_auc = 0.66436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:11:03,084] Trial 24 finished with value: 0.6643604289173296 and parameters: {'n_d': 37, 'n_a': 40, 'n_steps': 3, 'gamma': 1.9770901132662067, 'lambda_sparse': 0.0002673829297933083, 'lr': 0.0029677514528114284}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.58816 | val_0_auc: 0.44301 | val_1_auc: 0.45285 |  0:00:03s\n",
      "epoch 1  | loss: 0.55276 | val_0_auc: 0.43592 | val_1_auc: 0.43679 |  0:00:07s\n",
      "epoch 2  | loss: 0.51872 | val_0_auc: 0.44787 | val_1_auc: 0.45222 |  0:00:10s\n",
      "epoch 3  | loss: 0.50356 | val_0_auc: 0.45487 | val_1_auc: 0.44135 |  0:00:14s\n",
      "epoch 4  | loss: 0.47409 | val_0_auc: 0.45956 | val_1_auc: 0.44396 |  0:00:18s\n",
      "epoch 5  | loss: 0.46289 | val_0_auc: 0.4719  | val_1_auc: 0.46054 |  0:00:21s\n",
      "epoch 6  | loss: 0.4507  | val_0_auc: 0.46911 | val_1_auc: 0.47912 |  0:00:25s\n",
      "epoch 7  | loss: 0.43379 | val_0_auc: 0.48521 | val_1_auc: 0.49551 |  0:00:28s\n",
      "epoch 8  | loss: 0.42649 | val_0_auc: 0.48762 | val_1_auc: 0.48331 |  0:00:32s\n",
      "epoch 9  | loss: 0.41992 | val_0_auc: 0.48178 | val_1_auc: 0.49819 |  0:00:36s\n",
      "epoch 10 | loss: 0.41585 | val_0_auc: 0.4844  | val_1_auc: 0.49294 |  0:00:39s\n",
      "epoch 11 | loss: 0.4086  | val_0_auc: 0.48106 | val_1_auc: 0.4993  |  0:00:43s\n",
      "epoch 12 | loss: 0.40218 | val_0_auc: 0.49931 | val_1_auc: 0.49907 |  0:00:46s\n",
      "epoch 13 | loss: 0.38938 | val_0_auc: 0.49681 | val_1_auc: 0.50457 |  0:00:50s\n",
      "epoch 14 | loss: 0.3921  | val_0_auc: 0.49961 | val_1_auc: 0.4982  |  0:00:53s\n",
      "epoch 15 | loss: 0.38901 | val_0_auc: 0.50747 | val_1_auc: 0.51073 |  0:00:57s\n",
      "epoch 16 | loss: 0.38631 | val_0_auc: 0.51228 | val_1_auc: 0.51222 |  0:01:01s\n",
      "epoch 17 | loss: 0.38173 | val_0_auc: 0.50326 | val_1_auc: 0.49754 |  0:01:04s\n",
      "epoch 18 | loss: 0.37904 | val_0_auc: 0.5106  | val_1_auc: 0.50677 |  0:01:08s\n",
      "epoch 19 | loss: 0.37604 | val_0_auc: 0.51925 | val_1_auc: 0.50521 |  0:01:11s\n",
      "epoch 20 | loss: 0.37258 | val_0_auc: 0.5271  | val_1_auc: 0.5009  |  0:01:15s\n",
      "epoch 21 | loss: 0.3684  | val_0_auc: 0.52695 | val_1_auc: 0.51926 |  0:01:19s\n",
      "epoch 22 | loss: 0.36812 | val_0_auc: 0.53345 | val_1_auc: 0.51672 |  0:01:22s\n",
      "epoch 23 | loss: 0.37107 | val_0_auc: 0.52698 | val_1_auc: 0.50971 |  0:01:26s\n",
      "epoch 24 | loss: 0.36707 | val_0_auc: 0.52359 | val_1_auc: 0.50426 |  0:01:29s\n",
      "epoch 25 | loss: 0.36615 | val_0_auc: 0.52373 | val_1_auc: 0.50334 |  0:01:33s\n",
      "epoch 26 | loss: 0.36503 | val_0_auc: 0.53089 | val_1_auc: 0.50444 |  0:01:37s\n",
      "epoch 27 | loss: 0.363   | val_0_auc: 0.53206 | val_1_auc: 0.49886 |  0:01:41s\n",
      "epoch 28 | loss: 0.35988 | val_0_auc: 0.54974 | val_1_auc: 0.50636 |  0:01:44s\n",
      "epoch 29 | loss: 0.3584  | val_0_auc: 0.53684 | val_1_auc: 0.51633 |  0:01:48s\n",
      "epoch 30 | loss: 0.35705 | val_0_auc: 0.54764 | val_1_auc: 0.5269  |  0:01:52s\n",
      "epoch 31 | loss: 0.35661 | val_0_auc: 0.5647  | val_1_auc: 0.54237 |  0:01:55s\n",
      "epoch 32 | loss: 0.35595 | val_0_auc: 0.55789 | val_1_auc: 0.53691 |  0:01:59s\n",
      "epoch 33 | loss: 0.35372 | val_0_auc: 0.55996 | val_1_auc: 0.54218 |  0:02:02s\n",
      "epoch 34 | loss: 0.35442 | val_0_auc: 0.54749 | val_1_auc: 0.54211 |  0:02:06s\n",
      "epoch 35 | loss: 0.35125 | val_0_auc: 0.55525 | val_1_auc: 0.54319 |  0:02:10s\n",
      "epoch 36 | loss: 0.35122 | val_0_auc: 0.55384 | val_1_auc: 0.53844 |  0:02:13s\n",
      "epoch 37 | loss: 0.35112 | val_0_auc: 0.55894 | val_1_auc: 0.55383 |  0:02:17s\n",
      "epoch 38 | loss: 0.34997 | val_0_auc: 0.56617 | val_1_auc: 0.54276 |  0:02:21s\n",
      "epoch 39 | loss: 0.34997 | val_0_auc: 0.56247 | val_1_auc: 0.54471 |  0:02:25s\n",
      "epoch 40 | loss: 0.34781 | val_0_auc: 0.56862 | val_1_auc: 0.56365 |  0:02:29s\n",
      "epoch 41 | loss: 0.34701 | val_0_auc: 0.57021 | val_1_auc: 0.56629 |  0:02:32s\n",
      "epoch 42 | loss: 0.34886 | val_0_auc: 0.57591 | val_1_auc: 0.57409 |  0:02:36s\n",
      "epoch 43 | loss: 0.34492 | val_0_auc: 0.56698 | val_1_auc: 0.58709 |  0:02:39s\n",
      "epoch 44 | loss: 0.34897 | val_0_auc: 0.56184 | val_1_auc: 0.5645  |  0:02:43s\n",
      "epoch 45 | loss: 0.34682 | val_0_auc: 0.57341 | val_1_auc: 0.57231 |  0:02:47s\n",
      "epoch 46 | loss: 0.34513 | val_0_auc: 0.57062 | val_1_auc: 0.55321 |  0:02:50s\n",
      "epoch 47 | loss: 0.34314 | val_0_auc: 0.5806  | val_1_auc: 0.5599  |  0:02:54s\n",
      "epoch 48 | loss: 0.344   | val_0_auc: 0.57912 | val_1_auc: 0.55913 |  0:02:57s\n",
      "epoch 49 | loss: 0.34275 | val_0_auc: 0.58177 | val_1_auc: 0.56662 |  0:03:01s\n",
      "epoch 50 | loss: 0.34306 | val_0_auc: 0.57406 | val_1_auc: 0.55691 |  0:03:05s\n",
      "epoch 51 | loss: 0.34299 | val_0_auc: 0.58804 | val_1_auc: 0.56698 |  0:03:08s\n",
      "epoch 52 | loss: 0.34225 | val_0_auc: 0.57615 | val_1_auc: 0.57276 |  0:03:12s\n",
      "epoch 53 | loss: 0.34192 | val_0_auc: 0.57684 | val_1_auc: 0.56901 |  0:03:15s\n",
      "epoch 54 | loss: 0.34139 | val_0_auc: 0.57177 | val_1_auc: 0.58275 |  0:03:19s\n",
      "epoch 55 | loss: 0.34205 | val_0_auc: 0.57637 | val_1_auc: 0.57931 |  0:03:22s\n",
      "epoch 56 | loss: 0.33978 | val_0_auc: 0.58566 | val_1_auc: 0.58484 |  0:03:26s\n",
      "epoch 57 | loss: 0.33914 | val_0_auc: 0.59561 | val_1_auc: 0.58476 |  0:03:30s\n",
      "epoch 58 | loss: 0.33882 | val_0_auc: 0.59013 | val_1_auc: 0.58938 |  0:03:33s\n",
      "epoch 59 | loss: 0.34124 | val_0_auc: 0.57625 | val_1_auc: 0.60375 |  0:03:37s\n",
      "epoch 60 | loss: 0.33996 | val_0_auc: 0.59087 | val_1_auc: 0.59701 |  0:03:40s\n",
      "epoch 61 | loss: 0.34272 | val_0_auc: 0.58596 | val_1_auc: 0.58504 |  0:03:44s\n",
      "epoch 62 | loss: 0.33829 | val_0_auc: 0.60219 | val_1_auc: 0.59112 |  0:03:48s\n",
      "epoch 63 | loss: 0.33862 | val_0_auc: 0.60801 | val_1_auc: 0.58647 |  0:03:52s\n",
      "epoch 64 | loss: 0.33827 | val_0_auc: 0.59979 | val_1_auc: 0.58566 |  0:03:56s\n",
      "epoch 65 | loss: 0.33646 | val_0_auc: 0.59202 | val_1_auc: 0.58867 |  0:03:59s\n",
      "epoch 66 | loss: 0.33849 | val_0_auc: 0.60507 | val_1_auc: 0.58091 |  0:04:03s\n",
      "epoch 67 | loss: 0.33719 | val_0_auc: 0.60443 | val_1_auc: 0.58485 |  0:04:06s\n",
      "epoch 68 | loss: 0.33667 | val_0_auc: 0.60482 | val_1_auc: 0.58954 |  0:04:10s\n",
      "epoch 69 | loss: 0.33749 | val_0_auc: 0.60508 | val_1_auc: 0.59651 |  0:04:13s\n",
      "epoch 70 | loss: 0.33656 | val_0_auc: 0.61213 | val_1_auc: 0.59946 |  0:04:17s\n",
      "epoch 71 | loss: 0.34002 | val_0_auc: 0.6085  | val_1_auc: 0.59908 |  0:04:21s\n",
      "epoch 72 | loss: 0.33816 | val_0_auc: 0.61298 | val_1_auc: 0.6066  |  0:04:24s\n",
      "epoch 73 | loss: 0.33598 | val_0_auc: 0.6018  | val_1_auc: 0.5946  |  0:04:28s\n",
      "epoch 74 | loss: 0.33649 | val_0_auc: 0.61265 | val_1_auc: 0.60374 |  0:04:31s\n",
      "epoch 75 | loss: 0.33609 | val_0_auc: 0.60685 | val_1_auc: 0.60442 |  0:04:35s\n",
      "epoch 76 | loss: 0.33513 | val_0_auc: 0.61005 | val_1_auc: 0.58851 |  0:04:39s\n",
      "epoch 77 | loss: 0.33676 | val_0_auc: 0.60335 | val_1_auc: 0.60604 |  0:04:42s\n",
      "epoch 78 | loss: 0.33564 | val_0_auc: 0.61249 | val_1_auc: 0.59857 |  0:04:46s\n",
      "epoch 79 | loss: 0.33559 | val_0_auc: 0.61526 | val_1_auc: 0.5903  |  0:04:50s\n",
      "epoch 80 | loss: 0.33637 | val_0_auc: 0.61996 | val_1_auc: 0.59819 |  0:04:53s\n",
      "epoch 81 | loss: 0.33587 | val_0_auc: 0.61225 | val_1_auc: 0.59445 |  0:04:57s\n",
      "epoch 82 | loss: 0.33523 | val_0_auc: 0.60744 | val_1_auc: 0.5964  |  0:05:00s\n",
      "epoch 83 | loss: 0.33504 | val_0_auc: 0.60679 | val_1_auc: 0.59932 |  0:05:04s\n",
      "epoch 84 | loss: 0.33348 | val_0_auc: 0.60658 | val_1_auc: 0.59569 |  0:05:07s\n",
      "epoch 85 | loss: 0.33412 | val_0_auc: 0.6094  | val_1_auc: 0.60163 |  0:05:11s\n",
      "epoch 86 | loss: 0.33441 | val_0_auc: 0.60716 | val_1_auc: 0.60608 |  0:05:15s\n",
      "epoch 87 | loss: 0.33383 | val_0_auc: 0.61122 | val_1_auc: 0.59262 |  0:05:18s\n",
      "\n",
      "Early stopping occurred at epoch 87 with best_epoch = 72 and best_val_1_auc = 0.6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:16:23,203] Trial 25 finished with value: 0.606602559667935 and parameters: {'n_d': 29, 'n_a': 37, 'n_steps': 4, 'gamma': 2.3290191381268275, 'lambda_sparse': 0.0005614100755504608, 'lr': 0.0008853489319299451}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.60976 | val_0_auc: 0.47976 | val_1_auc: 0.46134 |  0:00:05s\n",
      "epoch 1  | loss: 0.53249 | val_0_auc: 0.49344 | val_1_auc: 0.49589 |  0:00:10s\n",
      "epoch 2  | loss: 0.48344 | val_0_auc: 0.51454 | val_1_auc: 0.48297 |  0:00:16s\n",
      "epoch 3  | loss: 0.45351 | val_0_auc: 0.50641 | val_1_auc: 0.49804 |  0:00:21s\n",
      "epoch 4  | loss: 0.44038 | val_0_auc: 0.50557 | val_1_auc: 0.49677 |  0:00:27s\n",
      "epoch 5  | loss: 0.41398 | val_0_auc: 0.5221  | val_1_auc: 0.50883 |  0:00:32s\n",
      "epoch 6  | loss: 0.40557 | val_0_auc: 0.53129 | val_1_auc: 0.53571 |  0:00:38s\n",
      "epoch 7  | loss: 0.39417 | val_0_auc: 0.55373 | val_1_auc: 0.51485 |  0:00:43s\n",
      "epoch 8  | loss: 0.39216 | val_0_auc: 0.54745 | val_1_auc: 0.5617  |  0:00:48s\n",
      "epoch 9  | loss: 0.38133 | val_0_auc: 0.5448  | val_1_auc: 0.53474 |  0:00:54s\n",
      "epoch 10 | loss: 0.3761  | val_0_auc: 0.55658 | val_1_auc: 0.57377 |  0:01:00s\n",
      "epoch 11 | loss: 0.37439 | val_0_auc: 0.54334 | val_1_auc: 0.53801 |  0:01:05s\n",
      "epoch 12 | loss: 0.37066 | val_0_auc: 0.56934 | val_1_auc: 0.57567 |  0:01:11s\n",
      "epoch 13 | loss: 0.36303 | val_0_auc: 0.58077 | val_1_auc: 0.59066 |  0:01:16s\n",
      "epoch 14 | loss: 0.36099 | val_0_auc: 0.56736 | val_1_auc: 0.57256 |  0:01:21s\n",
      "epoch 15 | loss: 0.35749 | val_0_auc: 0.59212 | val_1_auc: 0.57916 |  0:01:27s\n",
      "epoch 16 | loss: 0.35829 | val_0_auc: 0.56922 | val_1_auc: 0.56727 |  0:01:32s\n",
      "epoch 17 | loss: 0.35689 | val_0_auc: 0.56317 | val_1_auc: 0.56522 |  0:01:37s\n",
      "epoch 18 | loss: 0.35338 | val_0_auc: 0.56999 | val_1_auc: 0.55385 |  0:01:43s\n",
      "epoch 19 | loss: 0.34917 | val_0_auc: 0.56039 | val_1_auc: 0.55036 |  0:01:48s\n",
      "epoch 20 | loss: 0.34901 | val_0_auc: 0.57235 | val_1_auc: 0.59765 |  0:01:53s\n",
      "epoch 21 | loss: 0.34983 | val_0_auc: 0.56951 | val_1_auc: 0.57959 |  0:01:59s\n",
      "epoch 22 | loss: 0.34684 | val_0_auc: 0.57746 | val_1_auc: 0.58764 |  0:02:04s\n",
      "epoch 23 | loss: 0.34462 | val_0_auc: 0.58406 | val_1_auc: 0.58849 |  0:02:10s\n",
      "epoch 24 | loss: 0.34138 | val_0_auc: 0.58272 | val_1_auc: 0.61805 |  0:02:15s\n",
      "epoch 25 | loss: 0.34544 | val_0_auc: 0.5811  | val_1_auc: 0.62634 |  0:02:20s\n",
      "epoch 26 | loss: 0.34111 | val_0_auc: 0.58076 | val_1_auc: 0.59694 |  0:02:26s\n",
      "epoch 27 | loss: 0.3412  | val_0_auc: 0.5933  | val_1_auc: 0.60575 |  0:02:31s\n",
      "epoch 28 | loss: 0.33986 | val_0_auc: 0.57984 | val_1_auc: 0.58789 |  0:02:37s\n",
      "epoch 29 | loss: 0.33951 | val_0_auc: 0.5817  | val_1_auc: 0.60731 |  0:02:42s\n",
      "epoch 30 | loss: 0.34045 | val_0_auc: 0.5827  | val_1_auc: 0.59323 |  0:02:47s\n",
      "epoch 31 | loss: 0.3398  | val_0_auc: 0.58587 | val_1_auc: 0.59275 |  0:02:53s\n",
      "epoch 32 | loss: 0.34007 | val_0_auc: 0.59824 | val_1_auc: 0.59804 |  0:02:59s\n",
      "epoch 33 | loss: 0.33874 | val_0_auc: 0.59598 | val_1_auc: 0.58015 |  0:03:04s\n",
      "epoch 34 | loss: 0.33779 | val_0_auc: 0.58815 | val_1_auc: 0.60083 |  0:03:10s\n",
      "epoch 35 | loss: 0.33683 | val_0_auc: 0.59457 | val_1_auc: 0.6177  |  0:03:15s\n",
      "epoch 36 | loss: 0.33402 | val_0_auc: 0.60413 | val_1_auc: 0.61588 |  0:03:20s\n",
      "epoch 37 | loss: 0.33653 | val_0_auc: 0.61573 | val_1_auc: 0.61423 |  0:03:26s\n",
      "epoch 38 | loss: 0.33282 | val_0_auc: 0.61028 | val_1_auc: 0.60215 |  0:03:31s\n",
      "epoch 39 | loss: 0.33559 | val_0_auc: 0.60515 | val_1_auc: 0.62569 |  0:03:37s\n",
      "epoch 40 | loss: 0.33413 | val_0_auc: 0.60889 | val_1_auc: 0.61669 |  0:03:42s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 25 and best_val_1_auc = 0.62634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:20:07,549] Trial 26 finished with value: 0.6263355240401245 and parameters: {'n_d': 52, 'n_a': 47, 'n_steps': 6, 'gamma': 2.198653537952649, 'lambda_sparse': 0.0001258076148640636, 'lr': 0.001815916288125258}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.60729 | val_0_auc: 0.46964 | val_1_auc: 0.45313 |  0:00:03s\n",
      "epoch 1  | loss: 0.47056 | val_0_auc: 0.47324 | val_1_auc: 0.44863 |  0:00:07s\n",
      "epoch 2  | loss: 0.4322  | val_0_auc: 0.48891 | val_1_auc: 0.4601  |  0:00:10s\n",
      "epoch 3  | loss: 0.41448 | val_0_auc: 0.49414 | val_1_auc: 0.49307 |  0:00:14s\n",
      "epoch 4  | loss: 0.39673 | val_0_auc: 0.50423 | val_1_auc: 0.49117 |  0:00:18s\n",
      "epoch 5  | loss: 0.38658 | val_0_auc: 0.524   | val_1_auc: 0.51822 |  0:00:21s\n",
      "epoch 6  | loss: 0.37544 | val_0_auc: 0.53794 | val_1_auc: 0.5146  |  0:00:25s\n",
      "epoch 7  | loss: 0.36628 | val_0_auc: 0.55779 | val_1_auc: 0.5071  |  0:00:29s\n",
      "epoch 8  | loss: 0.35989 | val_0_auc: 0.55062 | val_1_auc: 0.52794 |  0:00:32s\n",
      "epoch 9  | loss: 0.35837 | val_0_auc: 0.55136 | val_1_auc: 0.54523 |  0:00:36s\n",
      "epoch 10 | loss: 0.35758 | val_0_auc: 0.54668 | val_1_auc: 0.56598 |  0:00:40s\n",
      "epoch 11 | loss: 0.34969 | val_0_auc: 0.55688 | val_1_auc: 0.56052 |  0:00:44s\n",
      "epoch 12 | loss: 0.35164 | val_0_auc: 0.58056 | val_1_auc: 0.53588 |  0:00:48s\n",
      "epoch 13 | loss: 0.35017 | val_0_auc: 0.56121 | val_1_auc: 0.54117 |  0:00:51s\n",
      "epoch 14 | loss: 0.3484  | val_0_auc: 0.58048 | val_1_auc: 0.54694 |  0:00:55s\n",
      "epoch 15 | loss: 0.34829 | val_0_auc: 0.59177 | val_1_auc: 0.57083 |  0:00:59s\n",
      "epoch 16 | loss: 0.34644 | val_0_auc: 0.58983 | val_1_auc: 0.56811 |  0:01:03s\n",
      "epoch 17 | loss: 0.34523 | val_0_auc: 0.57611 | val_1_auc: 0.55469 |  0:01:06s\n",
      "epoch 18 | loss: 0.34405 | val_0_auc: 0.58274 | val_1_auc: 0.55666 |  0:01:10s\n",
      "epoch 19 | loss: 0.34436 | val_0_auc: 0.58608 | val_1_auc: 0.56774 |  0:01:14s\n",
      "epoch 20 | loss: 0.34193 | val_0_auc: 0.58363 | val_1_auc: 0.58246 |  0:01:17s\n",
      "epoch 21 | loss: 0.34179 | val_0_auc: 0.61053 | val_1_auc: 0.58779 |  0:01:21s\n",
      "epoch 22 | loss: 0.34113 | val_0_auc: 0.60179 | val_1_auc: 0.59201 |  0:01:25s\n",
      "epoch 23 | loss: 0.34138 | val_0_auc: 0.59987 | val_1_auc: 0.59603 |  0:01:28s\n",
      "epoch 24 | loss: 0.34017 | val_0_auc: 0.59985 | val_1_auc: 0.59377 |  0:01:32s\n",
      "epoch 25 | loss: 0.33947 | val_0_auc: 0.61218 | val_1_auc: 0.59737 |  0:01:36s\n",
      "epoch 26 | loss: 0.33815 | val_0_auc: 0.62075 | val_1_auc: 0.60398 |  0:01:39s\n",
      "epoch 27 | loss: 0.33654 | val_0_auc: 0.62982 | val_1_auc: 0.60457 |  0:01:43s\n",
      "epoch 28 | loss: 0.3359  | val_0_auc: 0.63396 | val_1_auc: 0.6008  |  0:01:47s\n",
      "epoch 29 | loss: 0.33659 | val_0_auc: 0.62624 | val_1_auc: 0.61629 |  0:01:50s\n",
      "epoch 30 | loss: 0.33629 | val_0_auc: 0.62662 | val_1_auc: 0.60905 |  0:01:54s\n",
      "epoch 31 | loss: 0.33598 | val_0_auc: 0.62913 | val_1_auc: 0.60195 |  0:01:58s\n",
      "epoch 32 | loss: 0.33449 | val_0_auc: 0.6307  | val_1_auc: 0.60587 |  0:02:01s\n",
      "epoch 33 | loss: 0.33567 | val_0_auc: 0.63433 | val_1_auc: 0.60454 |  0:02:05s\n",
      "epoch 34 | loss: 0.33355 | val_0_auc: 0.63619 | val_1_auc: 0.60677 |  0:02:08s\n",
      "epoch 35 | loss: 0.33509 | val_0_auc: 0.62221 | val_1_auc: 0.59929 |  0:02:12s\n",
      "epoch 36 | loss: 0.33492 | val_0_auc: 0.6211  | val_1_auc: 0.59719 |  0:02:16s\n",
      "epoch 37 | loss: 0.33251 | val_0_auc: 0.61401 | val_1_auc: 0.59942 |  0:02:20s\n",
      "epoch 38 | loss: 0.33509 | val_0_auc: 0.6152  | val_1_auc: 0.59037 |  0:02:24s\n",
      "epoch 39 | loss: 0.33105 | val_0_auc: 0.62274 | val_1_auc: 0.59951 |  0:02:27s\n",
      "epoch 40 | loss: 0.33257 | val_0_auc: 0.62199 | val_1_auc: 0.61086 |  0:02:31s\n",
      "epoch 41 | loss: 0.33047 | val_0_auc: 0.62254 | val_1_auc: 0.62306 |  0:02:35s\n",
      "epoch 42 | loss: 0.33123 | val_0_auc: 0.63275 | val_1_auc: 0.63541 |  0:02:38s\n",
      "epoch 43 | loss: 0.33145 | val_0_auc: 0.63151 | val_1_auc: 0.62578 |  0:02:42s\n",
      "epoch 44 | loss: 0.33224 | val_0_auc: 0.62282 | val_1_auc: 0.61991 |  0:02:46s\n",
      "epoch 45 | loss: 0.3306  | val_0_auc: 0.62473 | val_1_auc: 0.61747 |  0:02:50s\n",
      "epoch 46 | loss: 0.3314  | val_0_auc: 0.63758 | val_1_auc: 0.62462 |  0:02:53s\n",
      "epoch 47 | loss: 0.32992 | val_0_auc: 0.64243 | val_1_auc: 0.63587 |  0:02:57s\n",
      "epoch 48 | loss: 0.32981 | val_0_auc: 0.63962 | val_1_auc: 0.63592 |  0:03:01s\n",
      "epoch 49 | loss: 0.32803 | val_0_auc: 0.65438 | val_1_auc: 0.62008 |  0:03:04s\n",
      "epoch 50 | loss: 0.3316  | val_0_auc: 0.6471  | val_1_auc: 0.61084 |  0:03:08s\n",
      "epoch 51 | loss: 0.32876 | val_0_auc: 0.65169 | val_1_auc: 0.61654 |  0:03:12s\n",
      "epoch 52 | loss: 0.32595 | val_0_auc: 0.6494  | val_1_auc: 0.62376 |  0:03:15s\n",
      "epoch 53 | loss: 0.32781 | val_0_auc: 0.64411 | val_1_auc: 0.63306 |  0:03:19s\n",
      "epoch 54 | loss: 0.32636 | val_0_auc: 0.64173 | val_1_auc: 0.63067 |  0:03:23s\n",
      "epoch 55 | loss: 0.32706 | val_0_auc: 0.6367  | val_1_auc: 0.65254 |  0:03:26s\n",
      "epoch 56 | loss: 0.32837 | val_0_auc: 0.64224 | val_1_auc: 0.64534 |  0:03:30s\n",
      "epoch 57 | loss: 0.32569 | val_0_auc: 0.64324 | val_1_auc: 0.64501 |  0:03:34s\n",
      "epoch 58 | loss: 0.32702 | val_0_auc: 0.65195 | val_1_auc: 0.65483 |  0:03:38s\n",
      "epoch 59 | loss: 0.32609 | val_0_auc: 0.65948 | val_1_auc: 0.64759 |  0:03:41s\n",
      "epoch 60 | loss: 0.32686 | val_0_auc: 0.65779 | val_1_auc: 0.63825 |  0:03:45s\n",
      "epoch 61 | loss: 0.32738 | val_0_auc: 0.64178 | val_1_auc: 0.62708 |  0:03:48s\n",
      "epoch 62 | loss: 0.32689 | val_0_auc: 0.64663 | val_1_auc: 0.63841 |  0:03:52s\n",
      "epoch 63 | loss: 0.32639 | val_0_auc: 0.64837 | val_1_auc: 0.65899 |  0:03:56s\n",
      "epoch 64 | loss: 0.32488 | val_0_auc: 0.65038 | val_1_auc: 0.65898 |  0:03:59s\n",
      "epoch 65 | loss: 0.32538 | val_0_auc: 0.65117 | val_1_auc: 0.64776 |  0:04:03s\n",
      "epoch 66 | loss: 0.32557 | val_0_auc: 0.65211 | val_1_auc: 0.64494 |  0:04:06s\n",
      "epoch 67 | loss: 0.32519 | val_0_auc: 0.65302 | val_1_auc: 0.6383  |  0:04:10s\n",
      "epoch 68 | loss: 0.32452 | val_0_auc: 0.64815 | val_1_auc: 0.63965 |  0:04:14s\n",
      "epoch 69 | loss: 0.32396 | val_0_auc: 0.65246 | val_1_auc: 0.64506 |  0:04:18s\n",
      "epoch 70 | loss: 0.32468 | val_0_auc: 0.64497 | val_1_auc: 0.64965 |  0:04:22s\n",
      "epoch 71 | loss: 0.32424 | val_0_auc: 0.6472  | val_1_auc: 0.6642  |  0:04:25s\n",
      "epoch 72 | loss: 0.32282 | val_0_auc: 0.65054 | val_1_auc: 0.65552 |  0:04:29s\n",
      "epoch 73 | loss: 0.32274 | val_0_auc: 0.64359 | val_1_auc: 0.65431 |  0:04:33s\n",
      "epoch 74 | loss: 0.32466 | val_0_auc: 0.6463  | val_1_auc: 0.65683 |  0:04:36s\n",
      "epoch 75 | loss: 0.32206 | val_0_auc: 0.6445  | val_1_auc: 0.6653  |  0:04:40s\n",
      "epoch 76 | loss: 0.32348 | val_0_auc: 0.64563 | val_1_auc: 0.66582 |  0:04:44s\n",
      "epoch 77 | loss: 0.32246 | val_0_auc: 0.64731 | val_1_auc: 0.66679 |  0:04:48s\n",
      "epoch 78 | loss: 0.32101 | val_0_auc: 0.65554 | val_1_auc: 0.67771 |  0:04:51s\n",
      "epoch 79 | loss: 0.32057 | val_0_auc: 0.65372 | val_1_auc: 0.67979 |  0:04:55s\n",
      "epoch 80 | loss: 0.32015 | val_0_auc: 0.65193 | val_1_auc: 0.6768  |  0:04:59s\n",
      "epoch 81 | loss: 0.32069 | val_0_auc: 0.65321 | val_1_auc: 0.67812 |  0:05:03s\n",
      "epoch 82 | loss: 0.32161 | val_0_auc: 0.6482  | val_1_auc: 0.66766 |  0:05:06s\n",
      "epoch 83 | loss: 0.32192 | val_0_auc: 0.6511  | val_1_auc: 0.66503 |  0:05:10s\n",
      "epoch 84 | loss: 0.32092 | val_0_auc: 0.64345 | val_1_auc: 0.66557 |  0:05:13s\n",
      "epoch 85 | loss: 0.32233 | val_0_auc: 0.6489  | val_1_auc: 0.68278 |  0:05:17s\n",
      "epoch 86 | loss: 0.32096 | val_0_auc: 0.65106 | val_1_auc: 0.67801 |  0:05:21s\n",
      "epoch 87 | loss: 0.32141 | val_0_auc: 0.65434 | val_1_auc: 0.67376 |  0:05:24s\n",
      "epoch 88 | loss: 0.32147 | val_0_auc: 0.65458 | val_1_auc: 0.6745  |  0:05:28s\n",
      "epoch 89 | loss: 0.32041 | val_0_auc: 0.65187 | val_1_auc: 0.66887 |  0:05:31s\n",
      "epoch 90 | loss: 0.3207  | val_0_auc: 0.65009 | val_1_auc: 0.66103 |  0:05:35s\n",
      "epoch 91 | loss: 0.31985 | val_0_auc: 0.65001 | val_1_auc: 0.6649  |  0:05:39s\n",
      "epoch 92 | loss: 0.32114 | val_0_auc: 0.64603 | val_1_auc: 0.66509 |  0:05:43s\n",
      "epoch 93 | loss: 0.32062 | val_0_auc: 0.65164 | val_1_auc: 0.66934 |  0:05:46s\n",
      "epoch 94 | loss: 0.32076 | val_0_auc: 0.65443 | val_1_auc: 0.67065 |  0:05:50s\n",
      "epoch 95 | loss: 0.31934 | val_0_auc: 0.65242 | val_1_auc: 0.66787 |  0:05:54s\n",
      "epoch 96 | loss: 0.32004 | val_0_auc: 0.65421 | val_1_auc: 0.66958 |  0:05:57s\n",
      "epoch 97 | loss: 0.3196  | val_0_auc: 0.66176 | val_1_auc: 0.66786 |  0:06:01s\n",
      "epoch 98 | loss: 0.3187  | val_0_auc: 0.65859 | val_1_auc: 0.66562 |  0:06:04s\n",
      "epoch 99 | loss: 0.31959 | val_0_auc: 0.65836 | val_1_auc: 0.6652  |  0:06:08s\n",
      "epoch 100| loss: 0.31998 | val_0_auc: 0.65771 | val_1_auc: 0.66278 |  0:06:12s\n",
      "\n",
      "Early stopping occurred at epoch 100 with best_epoch = 85 and best_val_1_auc = 0.68278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:26:20,967] Trial 27 finished with value: 0.6827810446212383 and parameters: {'n_d': 43, 'n_a': 32, 'n_steps': 4, 'gamma': 2.000653267506565, 'lambda_sparse': 6.503905432011778e-05, 'lr': 0.0022383272848983766}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.54677 | val_0_auc: 0.4846  | val_1_auc: 0.4903  |  0:00:03s\n",
      "epoch 1  | loss: 0.46553 | val_0_auc: 0.48095 | val_1_auc: 0.4696  |  0:00:05s\n",
      "epoch 2  | loss: 0.43251 | val_0_auc: 0.49239 | val_1_auc: 0.4934  |  0:00:08s\n",
      "epoch 3  | loss: 0.41077 | val_0_auc: 0.50843 | val_1_auc: 0.50574 |  0:00:11s\n",
      "epoch 4  | loss: 0.39007 | val_0_auc: 0.52    | val_1_auc: 0.51892 |  0:00:14s\n",
      "epoch 5  | loss: 0.38555 | val_0_auc: 0.53476 | val_1_auc: 0.51957 |  0:00:17s\n",
      "epoch 6  | loss: 0.37731 | val_0_auc: 0.53937 | val_1_auc: 0.52477 |  0:00:20s\n",
      "epoch 7  | loss: 0.36768 | val_0_auc: 0.53614 | val_1_auc: 0.52988 |  0:00:23s\n",
      "epoch 8  | loss: 0.36567 | val_0_auc: 0.54637 | val_1_auc: 0.53379 |  0:00:26s\n",
      "epoch 9  | loss: 0.36133 | val_0_auc: 0.56851 | val_1_auc: 0.54171 |  0:00:29s\n",
      "epoch 10 | loss: 0.35332 | val_0_auc: 0.57019 | val_1_auc: 0.55397 |  0:00:32s\n",
      "epoch 11 | loss: 0.35082 | val_0_auc: 0.58859 | val_1_auc: 0.55526 |  0:00:34s\n",
      "epoch 12 | loss: 0.35363 | val_0_auc: 0.58274 | val_1_auc: 0.57677 |  0:00:37s\n",
      "epoch 13 | loss: 0.34637 | val_0_auc: 0.57458 | val_1_auc: 0.57631 |  0:00:40s\n",
      "epoch 14 | loss: 0.34463 | val_0_auc: 0.5949  | val_1_auc: 0.58397 |  0:00:43s\n",
      "epoch 15 | loss: 0.34492 | val_0_auc: 0.59869 | val_1_auc: 0.5942  |  0:00:46s\n",
      "epoch 16 | loss: 0.34304 | val_0_auc: 0.59334 | val_1_auc: 0.60955 |  0:00:49s\n",
      "epoch 17 | loss: 0.33781 | val_0_auc: 0.58962 | val_1_auc: 0.60309 |  0:00:52s\n",
      "epoch 18 | loss: 0.34014 | val_0_auc: 0.59385 | val_1_auc: 0.59343 |  0:00:55s\n",
      "epoch 19 | loss: 0.33851 | val_0_auc: 0.60518 | val_1_auc: 0.60144 |  0:00:58s\n",
      "epoch 20 | loss: 0.33614 | val_0_auc: 0.60647 | val_1_auc: 0.60111 |  0:01:01s\n",
      "epoch 21 | loss: 0.33495 | val_0_auc: 0.59413 | val_1_auc: 0.59003 |  0:01:04s\n",
      "epoch 22 | loss: 0.33631 | val_0_auc: 0.59657 | val_1_auc: 0.6011  |  0:01:07s\n",
      "epoch 23 | loss: 0.33365 | val_0_auc: 0.60543 | val_1_auc: 0.59064 |  0:01:10s\n",
      "epoch 24 | loss: 0.33245 | val_0_auc: 0.60362 | val_1_auc: 0.59542 |  0:01:13s\n",
      "epoch 25 | loss: 0.33209 | val_0_auc: 0.59292 | val_1_auc: 0.58552 |  0:01:16s\n",
      "epoch 26 | loss: 0.33137 | val_0_auc: 0.59879 | val_1_auc: 0.60649 |  0:01:19s\n",
      "epoch 27 | loss: 0.32895 | val_0_auc: 0.61124 | val_1_auc: 0.60494 |  0:01:22s\n",
      "epoch 28 | loss: 0.32895 | val_0_auc: 0.60792 | val_1_auc: 0.60348 |  0:01:25s\n",
      "epoch 29 | loss: 0.3303  | val_0_auc: 0.60668 | val_1_auc: 0.60251 |  0:01:28s\n",
      "epoch 30 | loss: 0.33056 | val_0_auc: 0.61484 | val_1_auc: 0.60351 |  0:01:30s\n",
      "epoch 31 | loss: 0.32824 | val_0_auc: 0.61294 | val_1_auc: 0.61147 |  0:01:33s\n",
      "epoch 32 | loss: 0.32814 | val_0_auc: 0.62455 | val_1_auc: 0.61231 |  0:01:36s\n",
      "epoch 33 | loss: 0.32752 | val_0_auc: 0.62062 | val_1_auc: 0.60962 |  0:01:39s\n",
      "epoch 34 | loss: 0.32604 | val_0_auc: 0.62232 | val_1_auc: 0.60609 |  0:01:42s\n",
      "epoch 35 | loss: 0.32689 | val_0_auc: 0.62615 | val_1_auc: 0.61308 |  0:01:45s\n",
      "epoch 36 | loss: 0.32597 | val_0_auc: 0.62777 | val_1_auc: 0.6111  |  0:01:48s\n",
      "epoch 37 | loss: 0.32496 | val_0_auc: 0.63234 | val_1_auc: 0.60838 |  0:01:50s\n",
      "epoch 38 | loss: 0.32588 | val_0_auc: 0.63317 | val_1_auc: 0.60964 |  0:01:53s\n",
      "epoch 39 | loss: 0.32546 | val_0_auc: 0.62882 | val_1_auc: 0.61183 |  0:01:56s\n",
      "epoch 40 | loss: 0.32538 | val_0_auc: 0.62976 | val_1_auc: 0.61369 |  0:01:59s\n",
      "epoch 41 | loss: 0.32575 | val_0_auc: 0.63128 | val_1_auc: 0.61075 |  0:02:02s\n",
      "epoch 42 | loss: 0.32667 | val_0_auc: 0.62919 | val_1_auc: 0.60062 |  0:02:05s\n",
      "epoch 43 | loss: 0.32542 | val_0_auc: 0.6262  | val_1_auc: 0.60145 |  0:02:08s\n",
      "epoch 44 | loss: 0.32415 | val_0_auc: 0.63041 | val_1_auc: 0.59763 |  0:02:11s\n",
      "epoch 45 | loss: 0.32255 | val_0_auc: 0.63657 | val_1_auc: 0.5992  |  0:02:14s\n",
      "epoch 46 | loss: 0.32141 | val_0_auc: 0.63228 | val_1_auc: 0.60988 |  0:02:16s\n",
      "epoch 47 | loss: 0.32131 | val_0_auc: 0.63286 | val_1_auc: 0.61162 |  0:02:19s\n",
      "epoch 48 | loss: 0.32186 | val_0_auc: 0.6347  | val_1_auc: 0.60892 |  0:02:22s\n",
      "epoch 49 | loss: 0.3218  | val_0_auc: 0.6362  | val_1_auc: 0.61511 |  0:02:25s\n",
      "epoch 50 | loss: 0.32438 | val_0_auc: 0.63256 | val_1_auc: 0.61684 |  0:02:28s\n",
      "epoch 51 | loss: 0.32258 | val_0_auc: 0.62507 | val_1_auc: 0.60833 |  0:02:31s\n",
      "epoch 52 | loss: 0.32138 | val_0_auc: 0.62384 | val_1_auc: 0.60884 |  0:02:34s\n",
      "epoch 53 | loss: 0.32128 | val_0_auc: 0.63302 | val_1_auc: 0.61481 |  0:02:37s\n",
      "epoch 54 | loss: 0.32196 | val_0_auc: 0.63526 | val_1_auc: 0.61839 |  0:02:40s\n",
      "epoch 55 | loss: 0.31998 | val_0_auc: 0.64349 | val_1_auc: 0.61739 |  0:02:43s\n",
      "epoch 56 | loss: 0.31881 | val_0_auc: 0.64366 | val_1_auc: 0.61605 |  0:02:46s\n",
      "epoch 57 | loss: 0.31928 | val_0_auc: 0.63977 | val_1_auc: 0.61843 |  0:02:48s\n",
      "epoch 58 | loss: 0.32048 | val_0_auc: 0.63759 | val_1_auc: 0.62449 |  0:02:51s\n",
      "epoch 59 | loss: 0.31893 | val_0_auc: 0.63613 | val_1_auc: 0.62567 |  0:02:54s\n",
      "epoch 60 | loss: 0.31805 | val_0_auc: 0.63721 | val_1_auc: 0.62198 |  0:02:57s\n",
      "epoch 61 | loss: 0.31756 | val_0_auc: 0.64157 | val_1_auc: 0.62642 |  0:03:00s\n",
      "epoch 62 | loss: 0.31781 | val_0_auc: 0.63821 | val_1_auc: 0.62957 |  0:03:03s\n",
      "epoch 63 | loss: 0.31968 | val_0_auc: 0.63977 | val_1_auc: 0.63312 |  0:03:06s\n",
      "epoch 64 | loss: 0.31738 | val_0_auc: 0.64061 | val_1_auc: 0.62483 |  0:03:09s\n",
      "epoch 65 | loss: 0.31725 | val_0_auc: 0.63968 | val_1_auc: 0.62858 |  0:03:11s\n",
      "epoch 66 | loss: 0.31629 | val_0_auc: 0.64259 | val_1_auc: 0.63151 |  0:03:14s\n",
      "epoch 67 | loss: 0.31891 | val_0_auc: 0.64997 | val_1_auc: 0.63831 |  0:03:17s\n",
      "epoch 68 | loss: 0.31697 | val_0_auc: 0.64557 | val_1_auc: 0.64133 |  0:03:20s\n",
      "epoch 69 | loss: 0.31723 | val_0_auc: 0.64066 | val_1_auc: 0.64397 |  0:03:23s\n",
      "epoch 70 | loss: 0.31658 | val_0_auc: 0.64122 | val_1_auc: 0.64022 |  0:03:26s\n",
      "epoch 71 | loss: 0.31675 | val_0_auc: 0.63659 | val_1_auc: 0.64254 |  0:03:30s\n",
      "epoch 72 | loss: 0.31661 | val_0_auc: 0.64605 | val_1_auc: 0.64021 |  0:03:33s\n",
      "epoch 73 | loss: 0.31619 | val_0_auc: 0.65004 | val_1_auc: 0.64493 |  0:03:35s\n",
      "epoch 74 | loss: 0.31537 | val_0_auc: 0.65255 | val_1_auc: 0.64701 |  0:03:38s\n",
      "epoch 75 | loss: 0.31632 | val_0_auc: 0.64983 | val_1_auc: 0.65021 |  0:03:42s\n",
      "epoch 76 | loss: 0.31477 | val_0_auc: 0.6572  | val_1_auc: 0.65644 |  0:03:45s\n",
      "epoch 77 | loss: 0.31401 | val_0_auc: 0.65503 | val_1_auc: 0.65569 |  0:03:48s\n",
      "epoch 78 | loss: 0.31497 | val_0_auc: 0.65225 | val_1_auc: 0.658   |  0:03:51s\n",
      "epoch 79 | loss: 0.31388 | val_0_auc: 0.6508  | val_1_auc: 0.65508 |  0:03:53s\n",
      "epoch 80 | loss: 0.31292 | val_0_auc: 0.65001 | val_1_auc: 0.65246 |  0:03:56s\n",
      "epoch 81 | loss: 0.31418 | val_0_auc: 0.65436 | val_1_auc: 0.6504  |  0:03:59s\n",
      "epoch 82 | loss: 0.31029 | val_0_auc: 0.65383 | val_1_auc: 0.65229 |  0:04:02s\n",
      "epoch 83 | loss: 0.31115 | val_0_auc: 0.65559 | val_1_auc: 0.65112 |  0:04:05s\n",
      "epoch 84 | loss: 0.31159 | val_0_auc: 0.65501 | val_1_auc: 0.65331 |  0:04:08s\n",
      "epoch 85 | loss: 0.3119  | val_0_auc: 0.65647 | val_1_auc: 0.66214 |  0:04:11s\n",
      "epoch 86 | loss: 0.30986 | val_0_auc: 0.65464 | val_1_auc: 0.65301 |  0:04:14s\n",
      "epoch 87 | loss: 0.30945 | val_0_auc: 0.65147 | val_1_auc: 0.65719 |  0:04:17s\n",
      "epoch 88 | loss: 0.31028 | val_0_auc: 0.65302 | val_1_auc: 0.66039 |  0:04:20s\n",
      "epoch 89 | loss: 0.30809 | val_0_auc: 0.65715 | val_1_auc: 0.66066 |  0:04:23s\n",
      "epoch 90 | loss: 0.30905 | val_0_auc: 0.66039 | val_1_auc: 0.66239 |  0:04:26s\n",
      "epoch 91 | loss: 0.31075 | val_0_auc: 0.65567 | val_1_auc: 0.66381 |  0:04:29s\n",
      "epoch 92 | loss: 0.31025 | val_0_auc: 0.65708 | val_1_auc: 0.6724  |  0:04:31s\n",
      "epoch 93 | loss: 0.30993 | val_0_auc: 0.65027 | val_1_auc: 0.67145 |  0:04:34s\n",
      "epoch 94 | loss: 0.30911 | val_0_auc: 0.65085 | val_1_auc: 0.66349 |  0:04:37s\n",
      "epoch 95 | loss: 0.3076  | val_0_auc: 0.65123 | val_1_auc: 0.66509 |  0:04:40s\n",
      "epoch 96 | loss: 0.30816 | val_0_auc: 0.65216 | val_1_auc: 0.65658 |  0:04:43s\n",
      "epoch 97 | loss: 0.30864 | val_0_auc: 0.64726 | val_1_auc: 0.65494 |  0:04:46s\n",
      "epoch 98 | loss: 0.30796 | val_0_auc: 0.64571 | val_1_auc: 0.65276 |  0:04:49s\n",
      "epoch 99 | loss: 0.30806 | val_0_auc: 0.64894 | val_1_auc: 0.66128 |  0:04:52s\n",
      "epoch 100| loss: 0.30856 | val_0_auc: 0.6504  | val_1_auc: 0.65224 |  0:04:55s\n",
      "epoch 101| loss: 0.30668 | val_0_auc: 0.65196 | val_1_auc: 0.65034 |  0:04:58s\n",
      "epoch 102| loss: 0.30846 | val_0_auc: 0.64973 | val_1_auc: 0.65251 |  0:05:01s\n",
      "epoch 103| loss: 0.308   | val_0_auc: 0.64996 | val_1_auc: 0.65351 |  0:05:04s\n",
      "epoch 104| loss: 0.30705 | val_0_auc: 0.64989 | val_1_auc: 0.65123 |  0:05:07s\n",
      "epoch 105| loss: 0.30557 | val_0_auc: 0.65011 | val_1_auc: 0.64546 |  0:05:10s\n",
      "epoch 106| loss: 0.30458 | val_0_auc: 0.64657 | val_1_auc: 0.64833 |  0:05:12s\n",
      "epoch 107| loss: 0.30716 | val_0_auc: 0.65365 | val_1_auc: 0.6472  |  0:05:15s\n",
      "\n",
      "Early stopping occurred at epoch 107 with best_epoch = 92 and best_val_1_auc = 0.6724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:31:37,846] Trial 28 finished with value: 0.672399861639571 and parameters: {'n_d': 41, 'n_a': 28, 'n_steps': 3, 'gamma': 1.9291820322799802, 'lambda_sparse': 7.460416260483667e-05, 'lr': 0.0016484867067921157}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.61604 | val_0_auc: 0.47587 | val_1_auc: 0.49654 |  0:00:03s\n",
      "epoch 1  | loss: 0.54761 | val_0_auc: 0.48579 | val_1_auc: 0.49934 |  0:00:07s\n",
      "epoch 2  | loss: 0.5178  | val_0_auc: 0.49662 | val_1_auc: 0.49554 |  0:00:11s\n",
      "epoch 3  | loss: 0.46315 | val_0_auc: 0.49958 | val_1_auc: 0.51051 |  0:00:14s\n",
      "epoch 4  | loss: 0.45853 | val_0_auc: 0.50757 | val_1_auc: 0.4943  |  0:00:18s\n",
      "epoch 5  | loss: 0.43666 | val_0_auc: 0.53118 | val_1_auc: 0.49131 |  0:00:22s\n",
      "epoch 6  | loss: 0.41492 | val_0_auc: 0.53436 | val_1_auc: 0.50822 |  0:00:25s\n",
      "epoch 7  | loss: 0.41537 | val_0_auc: 0.52785 | val_1_auc: 0.50876 |  0:00:29s\n",
      "epoch 8  | loss: 0.40068 | val_0_auc: 0.52288 | val_1_auc: 0.52842 |  0:00:32s\n",
      "epoch 9  | loss: 0.398   | val_0_auc: 0.53667 | val_1_auc: 0.50995 |  0:00:36s\n",
      "epoch 10 | loss: 0.39096 | val_0_auc: 0.55148 | val_1_auc: 0.52111 |  0:00:40s\n",
      "epoch 11 | loss: 0.38772 | val_0_auc: 0.55229 | val_1_auc: 0.51054 |  0:00:44s\n",
      "epoch 12 | loss: 0.38959 | val_0_auc: 0.55509 | val_1_auc: 0.53465 |  0:00:48s\n",
      "epoch 13 | loss: 0.37563 | val_0_auc: 0.54639 | val_1_auc: 0.54753 |  0:00:51s\n",
      "epoch 14 | loss: 0.37822 | val_0_auc: 0.55261 | val_1_auc: 0.57061 |  0:00:55s\n",
      "epoch 15 | loss: 0.38118 | val_0_auc: 0.56186 | val_1_auc: 0.55058 |  0:00:59s\n",
      "epoch 16 | loss: 0.37158 | val_0_auc: 0.55832 | val_1_auc: 0.55378 |  0:01:02s\n",
      "epoch 17 | loss: 0.36631 | val_0_auc: 0.56891 | val_1_auc: 0.53691 |  0:01:06s\n",
      "epoch 18 | loss: 0.36747 | val_0_auc: 0.56356 | val_1_auc: 0.53456 |  0:01:10s\n",
      "epoch 19 | loss: 0.3647  | val_0_auc: 0.55612 | val_1_auc: 0.56043 |  0:01:14s\n",
      "epoch 20 | loss: 0.36167 | val_0_auc: 0.56245 | val_1_auc: 0.55446 |  0:01:17s\n",
      "epoch 21 | loss: 0.36054 | val_0_auc: 0.57566 | val_1_auc: 0.56344 |  0:01:21s\n",
      "epoch 22 | loss: 0.35918 | val_0_auc: 0.56493 | val_1_auc: 0.54059 |  0:01:25s\n",
      "epoch 23 | loss: 0.35712 | val_0_auc: 0.56999 | val_1_auc: 0.57937 |  0:01:28s\n",
      "epoch 24 | loss: 0.35576 | val_0_auc: 0.57897 | val_1_auc: 0.57862 |  0:01:33s\n",
      "epoch 25 | loss: 0.35911 | val_0_auc: 0.58159 | val_1_auc: 0.56583 |  0:01:36s\n",
      "epoch 26 | loss: 0.35615 | val_0_auc: 0.57922 | val_1_auc: 0.557   |  0:01:40s\n",
      "epoch 27 | loss: 0.3525  | val_0_auc: 0.58969 | val_1_auc: 0.57015 |  0:01:44s\n",
      "epoch 28 | loss: 0.35515 | val_0_auc: 0.58221 | val_1_auc: 0.55629 |  0:01:47s\n",
      "epoch 29 | loss: 0.35281 | val_0_auc: 0.59852 | val_1_auc: 0.55892 |  0:01:51s\n",
      "epoch 30 | loss: 0.35318 | val_0_auc: 0.60036 | val_1_auc: 0.557   |  0:01:55s\n",
      "epoch 31 | loss: 0.35356 | val_0_auc: 0.60461 | val_1_auc: 0.56035 |  0:01:58s\n",
      "epoch 32 | loss: 0.34663 | val_0_auc: 0.59878 | val_1_auc: 0.56455 |  0:02:02s\n",
      "epoch 33 | loss: 0.35115 | val_0_auc: 0.60657 | val_1_auc: 0.56478 |  0:02:06s\n",
      "epoch 34 | loss: 0.34455 | val_0_auc: 0.6021  | val_1_auc: 0.56499 |  0:02:09s\n",
      "epoch 35 | loss: 0.34516 | val_0_auc: 0.61494 | val_1_auc: 0.55613 |  0:02:13s\n",
      "epoch 36 | loss: 0.34467 | val_0_auc: 0.61556 | val_1_auc: 0.5707  |  0:02:17s\n",
      "epoch 37 | loss: 0.34434 | val_0_auc: 0.62114 | val_1_auc: 0.58669 |  0:02:20s\n",
      "epoch 38 | loss: 0.3452  | val_0_auc: 0.61105 | val_1_auc: 0.57893 |  0:02:24s\n",
      "epoch 39 | loss: 0.34334 | val_0_auc: 0.61449 | val_1_auc: 0.58844 |  0:02:28s\n",
      "epoch 40 | loss: 0.34097 | val_0_auc: 0.62056 | val_1_auc: 0.60923 |  0:02:31s\n",
      "epoch 41 | loss: 0.34167 | val_0_auc: 0.62148 | val_1_auc: 0.60152 |  0:02:35s\n",
      "epoch 42 | loss: 0.3451  | val_0_auc: 0.60324 | val_1_auc: 0.57539 |  0:02:38s\n",
      "epoch 43 | loss: 0.34046 | val_0_auc: 0.60264 | val_1_auc: 0.58859 |  0:02:42s\n",
      "epoch 44 | loss: 0.34144 | val_0_auc: 0.6148  | val_1_auc: 0.59431 |  0:02:46s\n",
      "epoch 45 | loss: 0.3411  | val_0_auc: 0.62076 | val_1_auc: 0.58691 |  0:02:50s\n",
      "epoch 46 | loss: 0.33861 | val_0_auc: 0.61641 | val_1_auc: 0.59801 |  0:02:53s\n",
      "epoch 47 | loss: 0.33953 | val_0_auc: 0.62137 | val_1_auc: 0.60241 |  0:02:57s\n",
      "epoch 48 | loss: 0.3397  | val_0_auc: 0.61469 | val_1_auc: 0.60454 |  0:03:01s\n",
      "epoch 49 | loss: 0.33923 | val_0_auc: 0.6072  | val_1_auc: 0.60843 |  0:03:04s\n",
      "epoch 50 | loss: 0.33611 | val_0_auc: 0.61415 | val_1_auc: 0.61566 |  0:03:08s\n",
      "epoch 51 | loss: 0.33563 | val_0_auc: 0.60976 | val_1_auc: 0.60798 |  0:03:12s\n",
      "epoch 52 | loss: 0.33695 | val_0_auc: 0.62074 | val_1_auc: 0.62698 |  0:03:15s\n",
      "epoch 53 | loss: 0.33566 | val_0_auc: 0.60933 | val_1_auc: 0.6107  |  0:03:19s\n",
      "epoch 54 | loss: 0.33593 | val_0_auc: 0.6172  | val_1_auc: 0.62648 |  0:03:23s\n",
      "epoch 55 | loss: 0.33445 | val_0_auc: 0.619   | val_1_auc: 0.62172 |  0:03:26s\n",
      "epoch 56 | loss: 0.33473 | val_0_auc: 0.61954 | val_1_auc: 0.61752 |  0:03:30s\n",
      "epoch 57 | loss: 0.33364 | val_0_auc: 0.62419 | val_1_auc: 0.62453 |  0:03:34s\n",
      "epoch 58 | loss: 0.33574 | val_0_auc: 0.62525 | val_1_auc: 0.61531 |  0:03:37s\n",
      "epoch 59 | loss: 0.33523 | val_0_auc: 0.63362 | val_1_auc: 0.61612 |  0:03:41s\n",
      "epoch 60 | loss: 0.33429 | val_0_auc: 0.63137 | val_1_auc: 0.61445 |  0:03:45s\n",
      "epoch 61 | loss: 0.33462 | val_0_auc: 0.63624 | val_1_auc: 0.6285  |  0:03:48s\n",
      "epoch 62 | loss: 0.33401 | val_0_auc: 0.63209 | val_1_auc: 0.6341  |  0:03:52s\n",
      "epoch 63 | loss: 0.33313 | val_0_auc: 0.64124 | val_1_auc: 0.63976 |  0:03:56s\n",
      "epoch 64 | loss: 0.33363 | val_0_auc: 0.63082 | val_1_auc: 0.64191 |  0:03:59s\n",
      "epoch 65 | loss: 0.33384 | val_0_auc: 0.62943 | val_1_auc: 0.63545 |  0:04:03s\n",
      "epoch 66 | loss: 0.32934 | val_0_auc: 0.62934 | val_1_auc: 0.62962 |  0:04:07s\n",
      "epoch 67 | loss: 0.33241 | val_0_auc: 0.63531 | val_1_auc: 0.6227  |  0:04:11s\n",
      "epoch 68 | loss: 0.32974 | val_0_auc: 0.63274 | val_1_auc: 0.62897 |  0:04:14s\n",
      "epoch 69 | loss: 0.33008 | val_0_auc: 0.63457 | val_1_auc: 0.6406  |  0:04:18s\n",
      "epoch 70 | loss: 0.33315 | val_0_auc: 0.63801 | val_1_auc: 0.61515 |  0:04:22s\n",
      "epoch 71 | loss: 0.33213 | val_0_auc: 0.64112 | val_1_auc: 0.6347  |  0:04:25s\n",
      "epoch 72 | loss: 0.3312  | val_0_auc: 0.63109 | val_1_auc: 0.6238  |  0:04:29s\n",
      "epoch 73 | loss: 0.33175 | val_0_auc: 0.6274  | val_1_auc: 0.62307 |  0:04:33s\n",
      "epoch 74 | loss: 0.33147 | val_0_auc: 0.62947 | val_1_auc: 0.62336 |  0:04:36s\n",
      "epoch 75 | loss: 0.33025 | val_0_auc: 0.63518 | val_1_auc: 0.6247  |  0:04:40s\n",
      "epoch 76 | loss: 0.33175 | val_0_auc: 0.63443 | val_1_auc: 0.62154 |  0:04:43s\n",
      "epoch 77 | loss: 0.33143 | val_0_auc: 0.64591 | val_1_auc: 0.59909 |  0:04:47s\n",
      "epoch 78 | loss: 0.33083 | val_0_auc: 0.63381 | val_1_auc: 0.608   |  0:04:51s\n",
      "epoch 79 | loss: 0.33147 | val_0_auc: 0.63058 | val_1_auc: 0.60869 |  0:04:54s\n",
      "\n",
      "Early stopping occurred at epoch 79 with best_epoch = 64 and best_val_1_auc = 0.64191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:36:34,029] Trial 29 finished with value: 0.6419107575233483 and parameters: {'n_d': 44, 'n_a': 32, 'n_steps': 4, 'gamma': 2.017303943424154, 'lambda_sparse': 0.00015963938295045707, 'lr': 0.001186376800145692}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.84685 | val_0_auc: 0.47887 | val_1_auc: 0.46499 |  0:00:03s\n",
      "epoch 1  | loss: 0.47762 | val_0_auc: 0.47336 | val_1_auc: 0.47264 |  0:00:07s\n",
      "epoch 2  | loss: 0.44867 | val_0_auc: 0.50499 | val_1_auc: 0.49271 |  0:00:11s\n",
      "epoch 3  | loss: 0.41925 | val_0_auc: 0.53129 | val_1_auc: 0.47971 |  0:00:15s\n",
      "epoch 4  | loss: 0.39533 | val_0_auc: 0.52952 | val_1_auc: 0.50037 |  0:00:18s\n",
      "epoch 5  | loss: 0.39253 | val_0_auc: 0.52662 | val_1_auc: 0.50527 |  0:00:22s\n",
      "epoch 6  | loss: 0.38001 | val_0_auc: 0.56026 | val_1_auc: 0.53244 |  0:00:26s\n",
      "epoch 7  | loss: 0.37862 | val_0_auc: 0.5505  | val_1_auc: 0.5452  |  0:00:30s\n",
      "epoch 8  | loss: 0.36787 | val_0_auc: 0.56164 | val_1_auc: 0.549   |  0:00:34s\n",
      "epoch 9  | loss: 0.36933 | val_0_auc: 0.56556 | val_1_auc: 0.56147 |  0:00:37s\n",
      "epoch 10 | loss: 0.36271 | val_0_auc: 0.56468 | val_1_auc: 0.53826 |  0:00:41s\n",
      "epoch 11 | loss: 0.36253 | val_0_auc: 0.58724 | val_1_auc: 0.55665 |  0:00:45s\n",
      "epoch 12 | loss: 0.36009 | val_0_auc: 0.57907 | val_1_auc: 0.58312 |  0:00:49s\n",
      "epoch 13 | loss: 0.35657 | val_0_auc: 0.57395 | val_1_auc: 0.58136 |  0:00:53s\n",
      "epoch 14 | loss: 0.35606 | val_0_auc: 0.56457 | val_1_auc: 0.56679 |  0:00:57s\n",
      "epoch 15 | loss: 0.35118 | val_0_auc: 0.58606 | val_1_auc: 0.59678 |  0:01:00s\n",
      "epoch 16 | loss: 0.34953 | val_0_auc: 0.58666 | val_1_auc: 0.60361 |  0:01:04s\n",
      "epoch 17 | loss: 0.35174 | val_0_auc: 0.58206 | val_1_auc: 0.59129 |  0:01:08s\n",
      "epoch 18 | loss: 0.3487  | val_0_auc: 0.57536 | val_1_auc: 0.58815 |  0:01:12s\n",
      "epoch 19 | loss: 0.34904 | val_0_auc: 0.56937 | val_1_auc: 0.60787 |  0:01:15s\n",
      "epoch 20 | loss: 0.3469  | val_0_auc: 0.57823 | val_1_auc: 0.58363 |  0:01:19s\n",
      "epoch 21 | loss: 0.34571 | val_0_auc: 0.59653 | val_1_auc: 0.58557 |  0:01:23s\n",
      "epoch 22 | loss: 0.34327 | val_0_auc: 0.60282 | val_1_auc: 0.61032 |  0:01:26s\n",
      "epoch 23 | loss: 0.34592 | val_0_auc: 0.60026 | val_1_auc: 0.58683 |  0:01:30s\n",
      "epoch 24 | loss: 0.34134 | val_0_auc: 0.58094 | val_1_auc: 0.58772 |  0:01:34s\n",
      "epoch 25 | loss: 0.34179 | val_0_auc: 0.60782 | val_1_auc: 0.61335 |  0:01:37s\n",
      "epoch 26 | loss: 0.3402  | val_0_auc: 0.60267 | val_1_auc: 0.60752 |  0:01:41s\n",
      "epoch 27 | loss: 0.33861 | val_0_auc: 0.60467 | val_1_auc: 0.60605 |  0:01:45s\n",
      "epoch 28 | loss: 0.33798 | val_0_auc: 0.621   | val_1_auc: 0.60945 |  0:01:48s\n",
      "epoch 29 | loss: 0.33998 | val_0_auc: 0.61674 | val_1_auc: 0.62853 |  0:01:52s\n",
      "epoch 30 | loss: 0.33872 | val_0_auc: 0.60771 | val_1_auc: 0.6089  |  0:01:56s\n",
      "epoch 31 | loss: 0.33686 | val_0_auc: 0.61489 | val_1_auc: 0.61248 |  0:02:00s\n",
      "epoch 32 | loss: 0.33717 | val_0_auc: 0.61334 | val_1_auc: 0.60675 |  0:02:03s\n",
      "epoch 33 | loss: 0.33422 | val_0_auc: 0.6243  | val_1_auc: 0.60271 |  0:02:07s\n",
      "epoch 34 | loss: 0.33681 | val_0_auc: 0.61968 | val_1_auc: 0.60854 |  0:02:11s\n",
      "epoch 35 | loss: 0.33429 | val_0_auc: 0.62003 | val_1_auc: 0.59712 |  0:02:15s\n",
      "epoch 36 | loss: 0.33343 | val_0_auc: 0.61999 | val_1_auc: 0.60582 |  0:02:18s\n",
      "epoch 37 | loss: 0.33273 | val_0_auc: 0.61671 | val_1_auc: 0.63384 |  0:02:22s\n",
      "epoch 38 | loss: 0.33263 | val_0_auc: 0.61346 | val_1_auc: 0.61405 |  0:02:26s\n",
      "epoch 39 | loss: 0.33328 | val_0_auc: 0.60582 | val_1_auc: 0.62493 |  0:02:29s\n",
      "epoch 40 | loss: 0.33155 | val_0_auc: 0.62627 | val_1_auc: 0.60699 |  0:02:33s\n",
      "epoch 41 | loss: 0.33184 | val_0_auc: 0.62561 | val_1_auc: 0.62396 |  0:02:37s\n",
      "epoch 42 | loss: 0.32798 | val_0_auc: 0.62144 | val_1_auc: 0.61261 |  0:02:41s\n",
      "epoch 43 | loss: 0.33096 | val_0_auc: 0.61709 | val_1_auc: 0.61806 |  0:02:45s\n",
      "epoch 44 | loss: 0.33081 | val_0_auc: 0.62661 | val_1_auc: 0.63092 |  0:02:49s\n",
      "epoch 45 | loss: 0.33125 | val_0_auc: 0.63526 | val_1_auc: 0.62861 |  0:02:52s\n",
      "epoch 46 | loss: 0.32856 | val_0_auc: 0.6358  | val_1_auc: 0.63074 |  0:02:56s\n",
      "epoch 47 | loss: 0.32989 | val_0_auc: 0.63992 | val_1_auc: 0.64448 |  0:03:00s\n",
      "epoch 48 | loss: 0.32809 | val_0_auc: 0.64847 | val_1_auc: 0.63574 |  0:03:04s\n",
      "epoch 49 | loss: 0.32731 | val_0_auc: 0.65232 | val_1_auc: 0.61064 |  0:03:07s\n",
      "epoch 50 | loss: 0.32814 | val_0_auc: 0.65403 | val_1_auc: 0.61261 |  0:03:11s\n",
      "epoch 51 | loss: 0.32638 | val_0_auc: 0.65574 | val_1_auc: 0.62403 |  0:03:15s\n",
      "epoch 52 | loss: 0.32934 | val_0_auc: 0.64257 | val_1_auc: 0.62211 |  0:03:19s\n",
      "epoch 53 | loss: 0.32735 | val_0_auc: 0.64071 | val_1_auc: 0.61458 |  0:03:23s\n",
      "epoch 54 | loss: 0.32501 | val_0_auc: 0.6448  | val_1_auc: 0.6273  |  0:03:26s\n",
      "epoch 55 | loss: 0.32588 | val_0_auc: 0.64336 | val_1_auc: 0.62407 |  0:03:30s\n",
      "epoch 56 | loss: 0.32796 | val_0_auc: 0.64769 | val_1_auc: 0.626   |  0:03:34s\n",
      "epoch 57 | loss: 0.3274  | val_0_auc: 0.64169 | val_1_auc: 0.61434 |  0:03:38s\n",
      "epoch 58 | loss: 0.32705 | val_0_auc: 0.63322 | val_1_auc: 0.61245 |  0:03:41s\n",
      "epoch 59 | loss: 0.32599 | val_0_auc: 0.63853 | val_1_auc: 0.62059 |  0:03:45s\n",
      "epoch 60 | loss: 0.32556 | val_0_auc: 0.65027 | val_1_auc: 0.63304 |  0:03:49s\n",
      "epoch 61 | loss: 0.32579 | val_0_auc: 0.65183 | val_1_auc: 0.63711 |  0:03:52s\n",
      "epoch 62 | loss: 0.32634 | val_0_auc: 0.65248 | val_1_auc: 0.64187 |  0:03:56s\n",
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 47 and best_val_1_auc = 0.64448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:40:32,029] Trial 30 finished with value: 0.6444814942926324 and parameters: {'n_d': 52, 'n_a': 33, 'n_steps': 4, 'gamma': 1.8320705381394577, 'lambda_sparse': 0.00018914151937401093, 'lr': 0.002178309827926556}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.57432 | val_0_auc: 0.4884  | val_1_auc: 0.48407 |  0:00:03s\n",
      "epoch 1  | loss: 0.51274 | val_0_auc: 0.4973  | val_1_auc: 0.47869 |  0:00:07s\n",
      "epoch 2  | loss: 0.46792 | val_0_auc: 0.49355 | val_1_auc: 0.50786 |  0:00:11s\n",
      "epoch 3  | loss: 0.45079 | val_0_auc: 0.51238 | val_1_auc: 0.51413 |  0:00:16s\n",
      "epoch 4  | loss: 0.43103 | val_0_auc: 0.51043 | val_1_auc: 0.51068 |  0:00:19s\n",
      "epoch 5  | loss: 0.41358 | val_0_auc: 0.51459 | val_1_auc: 0.54365 |  0:00:23s\n",
      "epoch 6  | loss: 0.40038 | val_0_auc: 0.51506 | val_1_auc: 0.52555 |  0:00:27s\n",
      "epoch 7  | loss: 0.3832  | val_0_auc: 0.52035 | val_1_auc: 0.52353 |  0:00:31s\n",
      "epoch 8  | loss: 0.38388 | val_0_auc: 0.53467 | val_1_auc: 0.52942 |  0:00:35s\n",
      "epoch 9  | loss: 0.37586 | val_0_auc: 0.56759 | val_1_auc: 0.5475  |  0:00:39s\n",
      "epoch 10 | loss: 0.36852 | val_0_auc: 0.53934 | val_1_auc: 0.53691 |  0:00:43s\n",
      "epoch 11 | loss: 0.36175 | val_0_auc: 0.54823 | val_1_auc: 0.5719  |  0:00:47s\n",
      "epoch 12 | loss: 0.35863 | val_0_auc: 0.56146 | val_1_auc: 0.58912 |  0:00:51s\n",
      "epoch 13 | loss: 0.35936 | val_0_auc: 0.56085 | val_1_auc: 0.5827  |  0:00:55s\n",
      "epoch 14 | loss: 0.35293 | val_0_auc: 0.58686 | val_1_auc: 0.61393 |  0:00:59s\n",
      "epoch 15 | loss: 0.35124 | val_0_auc: 0.59459 | val_1_auc: 0.59393 |  0:01:03s\n",
      "epoch 16 | loss: 0.34826 | val_0_auc: 0.56873 | val_1_auc: 0.58491 |  0:01:07s\n",
      "epoch 17 | loss: 0.34517 | val_0_auc: 0.58584 | val_1_auc: 0.57295 |  0:01:10s\n",
      "epoch 18 | loss: 0.34687 | val_0_auc: 0.58662 | val_1_auc: 0.5996  |  0:01:14s\n",
      "epoch 19 | loss: 0.34169 | val_0_auc: 0.59013 | val_1_auc: 0.59288 |  0:01:18s\n",
      "epoch 20 | loss: 0.3449  | val_0_auc: 0.59753 | val_1_auc: 0.58085 |  0:01:22s\n",
      "epoch 21 | loss: 0.34311 | val_0_auc: 0.60279 | val_1_auc: 0.59032 |  0:01:26s\n",
      "epoch 22 | loss: 0.34092 | val_0_auc: 0.5956  | val_1_auc: 0.60387 |  0:01:29s\n",
      "epoch 23 | loss: 0.34201 | val_0_auc: 0.61538 | val_1_auc: 0.6059  |  0:01:33s\n",
      "epoch 24 | loss: 0.33816 | val_0_auc: 0.61082 | val_1_auc: 0.58071 |  0:01:37s\n",
      "epoch 25 | loss: 0.33927 | val_0_auc: 0.60569 | val_1_auc: 0.60724 |  0:01:41s\n",
      "epoch 26 | loss: 0.3379  | val_0_auc: 0.60111 | val_1_auc: 0.62749 |  0:01:45s\n",
      "epoch 27 | loss: 0.33658 | val_0_auc: 0.61389 | val_1_auc: 0.62347 |  0:01:49s\n",
      "epoch 28 | loss: 0.33753 | val_0_auc: 0.61307 | val_1_auc: 0.61935 |  0:01:53s\n",
      "epoch 29 | loss: 0.3378  | val_0_auc: 0.60722 | val_1_auc: 0.63184 |  0:01:56s\n",
      "epoch 30 | loss: 0.33866 | val_0_auc: 0.60964 | val_1_auc: 0.63536 |  0:02:00s\n",
      "epoch 31 | loss: 0.33891 | val_0_auc: 0.60841 | val_1_auc: 0.62104 |  0:02:04s\n",
      "epoch 32 | loss: 0.3353  | val_0_auc: 0.62038 | val_1_auc: 0.61352 |  0:02:08s\n",
      "epoch 33 | loss: 0.33523 | val_0_auc: 0.61555 | val_1_auc: 0.62147 |  0:02:12s\n",
      "epoch 34 | loss: 0.33314 | val_0_auc: 0.62117 | val_1_auc: 0.61719 |  0:02:15s\n",
      "epoch 35 | loss: 0.33485 | val_0_auc: 0.61928 | val_1_auc: 0.61955 |  0:02:19s\n",
      "epoch 36 | loss: 0.33355 | val_0_auc: 0.62306 | val_1_auc: 0.61992 |  0:02:23s\n",
      "epoch 37 | loss: 0.33462 | val_0_auc: 0.6302  | val_1_auc: 0.62222 |  0:02:27s\n",
      "epoch 38 | loss: 0.33342 | val_0_auc: 0.64202 | val_1_auc: 0.63132 |  0:02:31s\n",
      "epoch 39 | loss: 0.33299 | val_0_auc: 0.63318 | val_1_auc: 0.63082 |  0:02:35s\n",
      "epoch 40 | loss: 0.33345 | val_0_auc: 0.63529 | val_1_auc: 0.62229 |  0:02:39s\n",
      "epoch 41 | loss: 0.33302 | val_0_auc: 0.64443 | val_1_auc: 0.64797 |  0:02:42s\n",
      "epoch 42 | loss: 0.3315  | val_0_auc: 0.64048 | val_1_auc: 0.64039 |  0:02:46s\n",
      "epoch 43 | loss: 0.33408 | val_0_auc: 0.64339 | val_1_auc: 0.61388 |  0:02:50s\n",
      "epoch 44 | loss: 0.33345 | val_0_auc: 0.64706 | val_1_auc: 0.61682 |  0:02:54s\n",
      "epoch 45 | loss: 0.33005 | val_0_auc: 0.63717 | val_1_auc: 0.63095 |  0:02:58s\n",
      "epoch 46 | loss: 0.3317  | val_0_auc: 0.64703 | val_1_auc: 0.64525 |  0:03:02s\n",
      "epoch 47 | loss: 0.33108 | val_0_auc: 0.64323 | val_1_auc: 0.63999 |  0:03:05s\n",
      "epoch 48 | loss: 0.3298  | val_0_auc: 0.63274 | val_1_auc: 0.63775 |  0:03:09s\n",
      "epoch 49 | loss: 0.33049 | val_0_auc: 0.63403 | val_1_auc: 0.64234 |  0:03:13s\n",
      "epoch 50 | loss: 0.33126 | val_0_auc: 0.63937 | val_1_auc: 0.62772 |  0:03:17s\n",
      "epoch 51 | loss: 0.3309  | val_0_auc: 0.62629 | val_1_auc: 0.62853 |  0:03:21s\n",
      "epoch 52 | loss: 0.33256 | val_0_auc: 0.63273 | val_1_auc: 0.63996 |  0:03:24s\n",
      "epoch 53 | loss: 0.33174 | val_0_auc: 0.63519 | val_1_auc: 0.63404 |  0:03:28s\n",
      "epoch 54 | loss: 0.3296  | val_0_auc: 0.64039 | val_1_auc: 0.61415 |  0:03:32s\n",
      "epoch 55 | loss: 0.33157 | val_0_auc: 0.64003 | val_1_auc: 0.62675 |  0:03:36s\n",
      "epoch 56 | loss: 0.32874 | val_0_auc: 0.64521 | val_1_auc: 0.62322 |  0:03:40s\n",
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 41 and best_val_1_auc = 0.64797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:44:13,653] Trial 31 finished with value: 0.6479695607056382 and parameters: {'n_d': 36, 'n_a': 28, 'n_steps': 5, 'gamma': 2.2798173638456944, 'lambda_sparse': 4.524804249894962e-05, 'lr': 0.00257678705152082}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.51414 | val_0_auc: 0.44549 | val_1_auc: 0.49039 |  0:00:02s\n",
      "epoch 1  | loss: 0.45176 | val_0_auc: 0.47777 | val_1_auc: 0.5048  |  0:00:06s\n",
      "epoch 2  | loss: 0.42754 | val_0_auc: 0.50286 | val_1_auc: 0.52257 |  0:00:09s\n",
      "epoch 3  | loss: 0.39409 | val_0_auc: 0.49978 | val_1_auc: 0.51164 |  0:00:12s\n",
      "epoch 4  | loss: 0.38409 | val_0_auc: 0.5139  | val_1_auc: 0.51366 |  0:00:15s\n",
      "epoch 5  | loss: 0.37441 | val_0_auc: 0.52531 | val_1_auc: 0.522   |  0:00:18s\n",
      "epoch 6  | loss: 0.36264 | val_0_auc: 0.52946 | val_1_auc: 0.512   |  0:00:21s\n",
      "epoch 7  | loss: 0.36208 | val_0_auc: 0.54126 | val_1_auc: 0.54493 |  0:00:24s\n",
      "epoch 8  | loss: 0.36078 | val_0_auc: 0.53469 | val_1_auc: 0.55131 |  0:00:27s\n",
      "epoch 9  | loss: 0.35637 | val_0_auc: 0.5355  | val_1_auc: 0.54718 |  0:00:30s\n",
      "epoch 10 | loss: 0.35569 | val_0_auc: 0.55189 | val_1_auc: 0.57371 |  0:00:33s\n",
      "epoch 11 | loss: 0.34879 | val_0_auc: 0.56153 | val_1_auc: 0.58331 |  0:00:36s\n",
      "epoch 12 | loss: 0.34716 | val_0_auc: 0.56602 | val_1_auc: 0.59505 |  0:00:39s\n",
      "epoch 13 | loss: 0.3458  | val_0_auc: 0.57762 | val_1_auc: 0.57659 |  0:00:42s\n",
      "epoch 14 | loss: 0.34361 | val_0_auc: 0.58331 | val_1_auc: 0.5809  |  0:00:45s\n",
      "epoch 15 | loss: 0.34008 | val_0_auc: 0.57882 | val_1_auc: 0.5918  |  0:00:48s\n",
      "epoch 16 | loss: 0.34256 | val_0_auc: 0.57141 | val_1_auc: 0.5903  |  0:00:51s\n",
      "epoch 17 | loss: 0.34096 | val_0_auc: 0.58807 | val_1_auc: 0.59097 |  0:00:54s\n",
      "epoch 18 | loss: 0.33766 | val_0_auc: 0.59773 | val_1_auc: 0.6063  |  0:00:57s\n",
      "epoch 19 | loss: 0.33619 | val_0_auc: 0.60061 | val_1_auc: 0.60307 |  0:01:00s\n",
      "epoch 20 | loss: 0.33777 | val_0_auc: 0.6062  | val_1_auc: 0.59585 |  0:01:03s\n",
      "epoch 21 | loss: 0.33582 | val_0_auc: 0.61284 | val_1_auc: 0.60631 |  0:01:06s\n",
      "epoch 22 | loss: 0.33745 | val_0_auc: 0.61619 | val_1_auc: 0.59689 |  0:01:09s\n",
      "epoch 23 | loss: 0.33583 | val_0_auc: 0.61547 | val_1_auc: 0.59825 |  0:01:12s\n",
      "epoch 24 | loss: 0.33371 | val_0_auc: 0.61739 | val_1_auc: 0.60128 |  0:01:15s\n",
      "epoch 25 | loss: 0.33367 | val_0_auc: 0.61731 | val_1_auc: 0.61306 |  0:01:18s\n",
      "epoch 26 | loss: 0.3314  | val_0_auc: 0.61764 | val_1_auc: 0.60435 |  0:01:21s\n",
      "epoch 27 | loss: 0.3295  | val_0_auc: 0.62199 | val_1_auc: 0.61096 |  0:01:24s\n",
      "epoch 28 | loss: 0.32919 | val_0_auc: 0.62048 | val_1_auc: 0.60284 |  0:01:27s\n",
      "epoch 29 | loss: 0.32978 | val_0_auc: 0.6254  | val_1_auc: 0.59499 |  0:01:31s\n",
      "epoch 30 | loss: 0.32848 | val_0_auc: 0.63091 | val_1_auc: 0.61305 |  0:01:34s\n",
      "epoch 31 | loss: 0.32827 | val_0_auc: 0.61398 | val_1_auc: 0.62627 |  0:01:37s\n",
      "epoch 32 | loss: 0.32965 | val_0_auc: 0.61895 | val_1_auc: 0.628   |  0:01:40s\n",
      "epoch 33 | loss: 0.32909 | val_0_auc: 0.61428 | val_1_auc: 0.61408 |  0:01:43s\n",
      "epoch 34 | loss: 0.32797 | val_0_auc: 0.62292 | val_1_auc: 0.61911 |  0:01:46s\n",
      "epoch 35 | loss: 0.32675 | val_0_auc: 0.63    | val_1_auc: 0.62826 |  0:01:49s\n",
      "epoch 36 | loss: 0.32679 | val_0_auc: 0.62641 | val_1_auc: 0.61916 |  0:01:51s\n",
      "epoch 37 | loss: 0.32723 | val_0_auc: 0.63132 | val_1_auc: 0.61977 |  0:01:54s\n",
      "epoch 38 | loss: 0.3269  | val_0_auc: 0.62775 | val_1_auc: 0.61059 |  0:01:57s\n",
      "epoch 39 | loss: 0.32595 | val_0_auc: 0.6362  | val_1_auc: 0.6138  |  0:02:00s\n",
      "epoch 40 | loss: 0.32418 | val_0_auc: 0.63203 | val_1_auc: 0.61217 |  0:02:03s\n",
      "epoch 41 | loss: 0.32654 | val_0_auc: 0.62921 | val_1_auc: 0.61635 |  0:02:06s\n",
      "epoch 42 | loss: 0.32532 | val_0_auc: 0.63504 | val_1_auc: 0.61364 |  0:02:09s\n",
      "epoch 43 | loss: 0.32467 | val_0_auc: 0.62774 | val_1_auc: 0.60558 |  0:02:12s\n",
      "epoch 44 | loss: 0.32302 | val_0_auc: 0.64176 | val_1_auc: 0.60506 |  0:02:15s\n",
      "epoch 45 | loss: 0.32542 | val_0_auc: 0.63127 | val_1_auc: 0.61698 |  0:02:18s\n",
      "epoch 46 | loss: 0.32393 | val_0_auc: 0.64174 | val_1_auc: 0.6171  |  0:02:21s\n",
      "epoch 47 | loss: 0.32458 | val_0_auc: 0.64131 | val_1_auc: 0.61792 |  0:02:24s\n",
      "epoch 48 | loss: 0.32272 | val_0_auc: 0.64325 | val_1_auc: 0.62286 |  0:02:27s\n",
      "epoch 49 | loss: 0.32428 | val_0_auc: 0.64225 | val_1_auc: 0.62785 |  0:02:30s\n",
      "epoch 50 | loss: 0.32292 | val_0_auc: 0.64803 | val_1_auc: 0.63255 |  0:02:33s\n",
      "epoch 51 | loss: 0.32234 | val_0_auc: 0.64583 | val_1_auc: 0.62074 |  0:02:36s\n",
      "epoch 52 | loss: 0.32079 | val_0_auc: 0.64306 | val_1_auc: 0.62233 |  0:02:39s\n",
      "epoch 53 | loss: 0.32179 | val_0_auc: 0.65135 | val_1_auc: 0.62188 |  0:02:42s\n",
      "epoch 54 | loss: 0.31922 | val_0_auc: 0.65387 | val_1_auc: 0.62978 |  0:02:45s\n",
      "epoch 55 | loss: 0.32109 | val_0_auc: 0.64592 | val_1_auc: 0.63513 |  0:02:48s\n",
      "epoch 56 | loss: 0.3189  | val_0_auc: 0.64711 | val_1_auc: 0.62132 |  0:02:51s\n",
      "epoch 57 | loss: 0.32102 | val_0_auc: 0.65042 | val_1_auc: 0.63651 |  0:02:54s\n",
      "epoch 58 | loss: 0.31976 | val_0_auc: 0.65245 | val_1_auc: 0.64104 |  0:02:57s\n",
      "epoch 59 | loss: 0.31858 | val_0_auc: 0.6546  | val_1_auc: 0.63868 |  0:03:00s\n",
      "epoch 60 | loss: 0.3209  | val_0_auc: 0.64465 | val_1_auc: 0.64903 |  0:03:03s\n",
      "epoch 61 | loss: 0.32085 | val_0_auc: 0.64098 | val_1_auc: 0.64803 |  0:03:07s\n",
      "epoch 62 | loss: 0.31884 | val_0_auc: 0.63822 | val_1_auc: 0.65129 |  0:03:10s\n",
      "epoch 63 | loss: 0.31785 | val_0_auc: 0.64556 | val_1_auc: 0.64271 |  0:03:13s\n",
      "epoch 64 | loss: 0.31977 | val_0_auc: 0.64665 | val_1_auc: 0.65539 |  0:03:16s\n",
      "epoch 65 | loss: 0.31956 | val_0_auc: 0.64763 | val_1_auc: 0.65061 |  0:03:19s\n",
      "epoch 66 | loss: 0.31849 | val_0_auc: 0.64814 | val_1_auc: 0.65622 |  0:03:22s\n",
      "epoch 67 | loss: 0.31697 | val_0_auc: 0.65106 | val_1_auc: 0.65442 |  0:03:25s\n",
      "epoch 68 | loss: 0.31825 | val_0_auc: 0.65193 | val_1_auc: 0.65755 |  0:03:28s\n",
      "epoch 69 | loss: 0.31724 | val_0_auc: 0.64887 | val_1_auc: 0.65701 |  0:03:31s\n",
      "epoch 70 | loss: 0.31628 | val_0_auc: 0.65379 | val_1_auc: 0.65135 |  0:03:34s\n",
      "epoch 71 | loss: 0.3175  | val_0_auc: 0.65957 | val_1_auc: 0.64426 |  0:03:37s\n",
      "epoch 72 | loss: 0.31611 | val_0_auc: 0.65322 | val_1_auc: 0.6376  |  0:03:40s\n",
      "epoch 73 | loss: 0.31637 | val_0_auc: 0.65214 | val_1_auc: 0.64649 |  0:03:43s\n",
      "epoch 74 | loss: 0.31709 | val_0_auc: 0.65489 | val_1_auc: 0.63117 |  0:03:45s\n",
      "epoch 75 | loss: 0.31694 | val_0_auc: 0.65149 | val_1_auc: 0.62707 |  0:03:48s\n",
      "epoch 76 | loss: 0.31561 | val_0_auc: 0.652   | val_1_auc: 0.63144 |  0:03:51s\n",
      "epoch 77 | loss: 0.31512 | val_0_auc: 0.6504  | val_1_auc: 0.63019 |  0:03:54s\n",
      "epoch 78 | loss: 0.31465 | val_0_auc: 0.6508  | val_1_auc: 0.63383 |  0:03:57s\n",
      "epoch 79 | loss: 0.3125  | val_0_auc: 0.65657 | val_1_auc: 0.62956 |  0:04:00s\n",
      "epoch 80 | loss: 0.31429 | val_0_auc: 0.65904 | val_1_auc: 0.6318  |  0:04:03s\n",
      "epoch 81 | loss: 0.31381 | val_0_auc: 0.65552 | val_1_auc: 0.63132 |  0:04:06s\n",
      "epoch 82 | loss: 0.31335 | val_0_auc: 0.65318 | val_1_auc: 0.6356  |  0:04:09s\n",
      "epoch 83 | loss: 0.31259 | val_0_auc: 0.65482 | val_1_auc: 0.6412  |  0:04:12s\n",
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 68 and best_val_1_auc = 0.65755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:48:27,343] Trial 32 finished with value: 0.6575468695952957 and parameters: {'n_d': 40, 'n_a': 43, 'n_steps': 3, 'gamma': 2.4852833672906436, 'lambda_sparse': 0.0004414563466607896, 'lr': 0.0020442526045423358}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.53032 | val_0_auc: 0.46729 | val_1_auc: 0.46182 |  0:00:03s\n",
      "epoch 1  | loss: 0.45438 | val_0_auc: 0.48748 | val_1_auc: 0.49001 |  0:00:07s\n",
      "epoch 2  | loss: 0.41219 | val_0_auc: 0.5164  | val_1_auc: 0.48742 |  0:00:11s\n",
      "epoch 3  | loss: 0.39701 | val_0_auc: 0.51476 | val_1_auc: 0.52018 |  0:00:14s\n",
      "epoch 4  | loss: 0.38387 | val_0_auc: 0.51929 | val_1_auc: 0.52319 |  0:00:18s\n",
      "epoch 5  | loss: 0.37252 | val_0_auc: 0.53456 | val_1_auc: 0.51728 |  0:00:22s\n",
      "epoch 6  | loss: 0.36644 | val_0_auc: 0.55806 | val_1_auc: 0.51039 |  0:00:26s\n",
      "epoch 7  | loss: 0.36296 | val_0_auc: 0.57204 | val_1_auc: 0.51981 |  0:00:29s\n",
      "epoch 8  | loss: 0.35582 | val_0_auc: 0.56331 | val_1_auc: 0.53003 |  0:00:33s\n",
      "epoch 9  | loss: 0.35615 | val_0_auc: 0.54823 | val_1_auc: 0.5468  |  0:00:37s\n",
      "epoch 10 | loss: 0.35139 | val_0_auc: 0.55856 | val_1_auc: 0.5657  |  0:00:41s\n",
      "epoch 11 | loss: 0.34871 | val_0_auc: 0.57662 | val_1_auc: 0.57991 |  0:00:45s\n",
      "epoch 12 | loss: 0.34558 | val_0_auc: 0.57694 | val_1_auc: 0.60558 |  0:00:48s\n",
      "epoch 13 | loss: 0.34531 | val_0_auc: 0.58256 | val_1_auc: 0.56832 |  0:00:52s\n",
      "epoch 14 | loss: 0.348   | val_0_auc: 0.57326 | val_1_auc: 0.56508 |  0:00:56s\n",
      "epoch 15 | loss: 0.34218 | val_0_auc: 0.58481 | val_1_auc: 0.56613 |  0:01:00s\n",
      "epoch 16 | loss: 0.34094 | val_0_auc: 0.5992  | val_1_auc: 0.55809 |  0:01:04s\n",
      "epoch 17 | loss: 0.34106 | val_0_auc: 0.58663 | val_1_auc: 0.55583 |  0:01:07s\n",
      "epoch 18 | loss: 0.33945 | val_0_auc: 0.58374 | val_1_auc: 0.58865 |  0:01:11s\n",
      "epoch 19 | loss: 0.33891 | val_0_auc: 0.59548 | val_1_auc: 0.61216 |  0:01:15s\n",
      "epoch 20 | loss: 0.3385  | val_0_auc: 0.58955 | val_1_auc: 0.58386 |  0:01:19s\n",
      "epoch 21 | loss: 0.33755 | val_0_auc: 0.60066 | val_1_auc: 0.58542 |  0:01:22s\n",
      "epoch 22 | loss: 0.33839 | val_0_auc: 0.60393 | val_1_auc: 0.5761  |  0:01:26s\n",
      "epoch 23 | loss: 0.33842 | val_0_auc: 0.61158 | val_1_auc: 0.58669 |  0:01:30s\n",
      "epoch 24 | loss: 0.33373 | val_0_auc: 0.61067 | val_1_auc: 0.59291 |  0:01:33s\n",
      "epoch 25 | loss: 0.33418 | val_0_auc: 0.62079 | val_1_auc: 0.59314 |  0:01:37s\n",
      "epoch 26 | loss: 0.33477 | val_0_auc: 0.61327 | val_1_auc: 0.6022  |  0:01:41s\n",
      "epoch 27 | loss: 0.33573 | val_0_auc: 0.61198 | val_1_auc: 0.61679 |  0:01:45s\n",
      "epoch 28 | loss: 0.33528 | val_0_auc: 0.62437 | val_1_auc: 0.63255 |  0:01:48s\n",
      "epoch 29 | loss: 0.33358 | val_0_auc: 0.633   | val_1_auc: 0.63133 |  0:01:52s\n",
      "epoch 30 | loss: 0.33406 | val_0_auc: 0.62792 | val_1_auc: 0.62154 |  0:01:56s\n",
      "epoch 31 | loss: 0.33212 | val_0_auc: 0.62587 | val_1_auc: 0.60411 |  0:01:59s\n",
      "epoch 32 | loss: 0.33388 | val_0_auc: 0.62536 | val_1_auc: 0.59931 |  0:02:03s\n",
      "epoch 33 | loss: 0.33391 | val_0_auc: 0.61978 | val_1_auc: 0.59848 |  0:02:07s\n",
      "epoch 34 | loss: 0.33313 | val_0_auc: 0.61886 | val_1_auc: 0.60916 |  0:02:10s\n",
      "epoch 35 | loss: 0.32998 | val_0_auc: 0.61192 | val_1_auc: 0.61674 |  0:02:14s\n",
      "epoch 36 | loss: 0.33123 | val_0_auc: 0.60167 | val_1_auc: 0.60924 |  0:02:18s\n",
      "epoch 37 | loss: 0.32884 | val_0_auc: 0.59807 | val_1_auc: 0.60054 |  0:02:22s\n",
      "epoch 38 | loss: 0.33297 | val_0_auc: 0.61882 | val_1_auc: 0.61514 |  0:02:26s\n",
      "epoch 39 | loss: 0.33003 | val_0_auc: 0.61881 | val_1_auc: 0.61206 |  0:02:30s\n",
      "epoch 40 | loss: 0.32907 | val_0_auc: 0.62564 | val_1_auc: 0.61063 |  0:02:33s\n",
      "epoch 41 | loss: 0.33048 | val_0_auc: 0.62197 | val_1_auc: 0.61098 |  0:02:37s\n",
      "epoch 42 | loss: 0.33003 | val_0_auc: 0.63005 | val_1_auc: 0.60882 |  0:02:41s\n",
      "epoch 43 | loss: 0.32846 | val_0_auc: 0.63556 | val_1_auc: 0.61113 |  0:02:45s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 28 and best_val_1_auc = 0.63255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:51:14,026] Trial 33 finished with value: 0.6325492909028019 and parameters: {'n_d': 45, 'n_a': 39, 'n_steps': 4, 'gamma': 2.035728205932261, 'lambda_sparse': 9.822713416853279e-05, 'lr': 0.002837130765900336}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.89909 | val_0_auc: 0.51339 | val_1_auc: 0.51406 |  0:00:05s\n",
      "epoch 1  | loss: 1.4693  | val_0_auc: 0.48257 | val_1_auc: 0.46289 |  0:00:11s\n",
      "epoch 2  | loss: 0.77422 | val_0_auc: 0.46667 | val_1_auc: 0.47811 |  0:00:16s\n",
      "epoch 3  | loss: 0.60238 | val_0_auc: 0.47159 | val_1_auc: 0.48792 |  0:00:21s\n",
      "epoch 4  | loss: 0.59434 | val_0_auc: 0.46301 | val_1_auc: 0.47918 |  0:00:26s\n",
      "epoch 5  | loss: 0.55051 | val_0_auc: 0.47522 | val_1_auc: 0.50287 |  0:00:31s\n",
      "epoch 6  | loss: 0.52696 | val_0_auc: 0.50623 | val_1_auc: 0.51416 |  0:00:36s\n",
      "epoch 7  | loss: 0.50764 | val_0_auc: 0.50375 | val_1_auc: 0.50599 |  0:00:42s\n",
      "epoch 8  | loss: 0.48567 | val_0_auc: 0.50521 | val_1_auc: 0.5046  |  0:00:47s\n",
      "epoch 9  | loss: 0.46469 | val_0_auc: 0.5111  | val_1_auc: 0.54452 |  0:00:52s\n",
      "epoch 10 | loss: 0.45473 | val_0_auc: 0.52377 | val_1_auc: 0.54559 |  0:00:57s\n",
      "epoch 11 | loss: 0.43847 | val_0_auc: 0.52308 | val_1_auc: 0.54306 |  0:01:02s\n",
      "epoch 12 | loss: 0.43394 | val_0_auc: 0.51753 | val_1_auc: 0.53379 |  0:01:07s\n",
      "epoch 13 | loss: 0.42839 | val_0_auc: 0.54911 | val_1_auc: 0.55243 |  0:01:12s\n",
      "epoch 14 | loss: 0.41992 | val_0_auc: 0.52824 | val_1_auc: 0.55069 |  0:01:17s\n",
      "epoch 15 | loss: 0.40878 | val_0_auc: 0.53745 | val_1_auc: 0.55182 |  0:01:22s\n",
      "epoch 16 | loss: 0.4052  | val_0_auc: 0.54094 | val_1_auc: 0.5448  |  0:01:28s\n",
      "epoch 17 | loss: 0.40343 | val_0_auc: 0.55889 | val_1_auc: 0.55043 |  0:01:33s\n",
      "epoch 18 | loss: 0.39834 | val_0_auc: 0.53734 | val_1_auc: 0.54739 |  0:01:38s\n",
      "epoch 19 | loss: 0.38683 | val_0_auc: 0.56527 | val_1_auc: 0.56597 |  0:01:43s\n",
      "epoch 20 | loss: 0.38703 | val_0_auc: 0.55603 | val_1_auc: 0.54905 |  0:01:48s\n",
      "epoch 21 | loss: 0.3829  | val_0_auc: 0.54838 | val_1_auc: 0.56908 |  0:01:53s\n",
      "epoch 22 | loss: 0.37953 | val_0_auc: 0.55375 | val_1_auc: 0.56077 |  0:01:59s\n",
      "epoch 23 | loss: 0.38162 | val_0_auc: 0.56559 | val_1_auc: 0.55105 |  0:02:04s\n",
      "epoch 24 | loss: 0.37861 | val_0_auc: 0.57632 | val_1_auc: 0.55783 |  0:02:09s\n",
      "epoch 25 | loss: 0.37206 | val_0_auc: 0.58397 | val_1_auc: 0.56612 |  0:02:14s\n",
      "epoch 26 | loss: 0.36529 | val_0_auc: 0.59868 | val_1_auc: 0.58939 |  0:02:19s\n",
      "epoch 27 | loss: 0.36337 | val_0_auc: 0.5785  | val_1_auc: 0.56862 |  0:02:25s\n",
      "epoch 28 | loss: 0.36992 | val_0_auc: 0.56914 | val_1_auc: 0.55062 |  0:02:30s\n",
      "epoch 29 | loss: 0.36855 | val_0_auc: 0.57296 | val_1_auc: 0.54345 |  0:02:35s\n",
      "epoch 30 | loss: 0.36148 | val_0_auc: 0.58829 | val_1_auc: 0.55093 |  0:02:40s\n",
      "epoch 31 | loss: 0.3563  | val_0_auc: 0.56699 | val_1_auc: 0.57619 |  0:02:45s\n",
      "epoch 32 | loss: 0.35685 | val_0_auc: 0.57742 | val_1_auc: 0.57929 |  0:02:50s\n",
      "epoch 33 | loss: 0.35668 | val_0_auc: 0.58353 | val_1_auc: 0.58334 |  0:02:55s\n",
      "epoch 34 | loss: 0.35861 | val_0_auc: 0.60031 | val_1_auc: 0.59498 |  0:03:00s\n",
      "epoch 35 | loss: 0.35648 | val_0_auc: 0.59201 | val_1_auc: 0.59914 |  0:03:06s\n",
      "epoch 36 | loss: 0.35213 | val_0_auc: 0.61877 | val_1_auc: 0.58708 |  0:03:11s\n",
      "epoch 37 | loss: 0.34751 | val_0_auc: 0.60537 | val_1_auc: 0.59365 |  0:03:16s\n",
      "epoch 38 | loss: 0.34872 | val_0_auc: 0.6029  | val_1_auc: 0.5902  |  0:03:21s\n",
      "epoch 39 | loss: 0.34658 | val_0_auc: 0.59927 | val_1_auc: 0.59294 |  0:03:26s\n",
      "epoch 40 | loss: 0.34535 | val_0_auc: 0.61544 | val_1_auc: 0.60343 |  0:03:32s\n",
      "epoch 41 | loss: 0.34861 | val_0_auc: 0.59943 | val_1_auc: 0.5962  |  0:03:37s\n",
      "epoch 42 | loss: 0.34435 | val_0_auc: 0.61354 | val_1_auc: 0.59756 |  0:03:42s\n",
      "epoch 43 | loss: 0.34382 | val_0_auc: 0.60763 | val_1_auc: 0.58809 |  0:03:47s\n",
      "epoch 44 | loss: 0.34514 | val_0_auc: 0.61118 | val_1_auc: 0.58306 |  0:03:52s\n",
      "epoch 45 | loss: 0.34016 | val_0_auc: 0.61334 | val_1_auc: 0.60077 |  0:03:57s\n",
      "epoch 46 | loss: 0.34054 | val_0_auc: 0.60976 | val_1_auc: 0.60955 |  0:04:03s\n",
      "epoch 47 | loss: 0.33901 | val_0_auc: 0.61545 | val_1_auc: 0.59888 |  0:04:08s\n",
      "epoch 48 | loss: 0.34181 | val_0_auc: 0.61395 | val_1_auc: 0.6088  |  0:04:13s\n",
      "epoch 49 | loss: 0.34072 | val_0_auc: 0.61254 | val_1_auc: 0.58941 |  0:04:18s\n",
      "epoch 50 | loss: 0.34095 | val_0_auc: 0.60488 | val_1_auc: 0.58101 |  0:04:23s\n",
      "epoch 51 | loss: 0.33887 | val_0_auc: 0.59786 | val_1_auc: 0.60647 |  0:04:29s\n",
      "epoch 52 | loss: 0.33825 | val_0_auc: 0.62174 | val_1_auc: 0.61807 |  0:04:35s\n",
      "epoch 53 | loss: 0.3363  | val_0_auc: 0.61512 | val_1_auc: 0.60975 |  0:04:40s\n",
      "epoch 54 | loss: 0.34023 | val_0_auc: 0.61553 | val_1_auc: 0.61819 |  0:04:45s\n",
      "epoch 55 | loss: 0.337   | val_0_auc: 0.62334 | val_1_auc: 0.62577 |  0:04:50s\n",
      "epoch 56 | loss: 0.33669 | val_0_auc: 0.61422 | val_1_auc: 0.62026 |  0:04:55s\n",
      "epoch 57 | loss: 0.33773 | val_0_auc: 0.62111 | val_1_auc: 0.61974 |  0:05:00s\n",
      "epoch 58 | loss: 0.33895 | val_0_auc: 0.60882 | val_1_auc: 0.61053 |  0:05:06s\n",
      "epoch 59 | loss: 0.33722 | val_0_auc: 0.61525 | val_1_auc: 0.61337 |  0:05:11s\n",
      "epoch 60 | loss: 0.33582 | val_0_auc: 0.6312  | val_1_auc: 0.61972 |  0:05:16s\n",
      "epoch 61 | loss: 0.33567 | val_0_auc: 0.62321 | val_1_auc: 0.64307 |  0:05:21s\n",
      "epoch 62 | loss: 0.335   | val_0_auc: 0.61325 | val_1_auc: 0.61358 |  0:05:26s\n",
      "epoch 63 | loss: 0.33438 | val_0_auc: 0.63241 | val_1_auc: 0.62523 |  0:05:31s\n",
      "epoch 64 | loss: 0.33486 | val_0_auc: 0.62292 | val_1_auc: 0.61699 |  0:05:37s\n",
      "epoch 65 | loss: 0.33632 | val_0_auc: 0.6066  | val_1_auc: 0.62354 |  0:05:42s\n",
      "epoch 66 | loss: 0.33429 | val_0_auc: 0.61587 | val_1_auc: 0.64698 |  0:05:47s\n",
      "epoch 67 | loss: 0.3343  | val_0_auc: 0.62027 | val_1_auc: 0.62796 |  0:05:52s\n",
      "epoch 68 | loss: 0.33525 | val_0_auc: 0.62421 | val_1_auc: 0.61197 |  0:05:58s\n",
      "epoch 69 | loss: 0.3336  | val_0_auc: 0.6196  | val_1_auc: 0.62302 |  0:06:03s\n",
      "epoch 70 | loss: 0.33359 | val_0_auc: 0.61973 | val_1_auc: 0.60143 |  0:06:08s\n",
      "epoch 71 | loss: 0.33328 | val_0_auc: 0.64054 | val_1_auc: 0.6228  |  0:06:14s\n",
      "epoch 72 | loss: 0.33341 | val_0_auc: 0.62028 | val_1_auc: 0.59132 |  0:06:19s\n",
      "epoch 73 | loss: 0.3336  | val_0_auc: 0.63238 | val_1_auc: 0.61411 |  0:06:24s\n",
      "epoch 74 | loss: 0.3337  | val_0_auc: 0.62132 | val_1_auc: 0.61214 |  0:06:29s\n",
      "epoch 75 | loss: 0.33506 | val_0_auc: 0.62226 | val_1_auc: 0.61697 |  0:06:34s\n",
      "epoch 76 | loss: 0.33428 | val_0_auc: 0.63643 | val_1_auc: 0.61463 |  0:06:40s\n",
      "epoch 77 | loss: 0.33177 | val_0_auc: 0.63634 | val_1_auc: 0.61492 |  0:06:45s\n",
      "epoch 78 | loss: 0.3332  | val_0_auc: 0.64142 | val_1_auc: 0.6196  |  0:06:50s\n",
      "epoch 79 | loss: 0.33191 | val_0_auc: 0.64225 | val_1_auc: 0.60559 |  0:06:55s\n",
      "epoch 80 | loss: 0.33177 | val_0_auc: 0.65141 | val_1_auc: 0.61779 |  0:07:00s\n",
      "epoch 81 | loss: 0.33085 | val_0_auc: 0.65126 | val_1_auc: 0.61291 |  0:07:05s\n",
      "\n",
      "Early stopping occurred at epoch 81 with best_epoch = 66 and best_val_1_auc = 0.64698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 14:58:21,509] Trial 34 finished with value: 0.6469761328260117 and parameters: {'n_d': 31, 'n_a': 31, 'n_steps': 7, 'gamma': 1.8295934931549964, 'lambda_sparse': 1.4801994038685853e-05, 'lr': 0.0017517614035707533}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.55566 | val_0_auc: 0.48877 | val_1_auc: 0.50536 |  0:00:03s\n",
      "epoch 1  | loss: 0.58589 | val_0_auc: 0.46098 | val_1_auc: 0.4737  |  0:00:07s\n",
      "epoch 2  | loss: 0.53013 | val_0_auc: 0.487   | val_1_auc: 0.47494 |  0:00:11s\n",
      "epoch 3  | loss: 0.46906 | val_0_auc: 0.49411 | val_1_auc: 0.49209 |  0:00:14s\n",
      "epoch 4  | loss: 0.44186 | val_0_auc: 0.50657 | val_1_auc: 0.49121 |  0:00:18s\n",
      "epoch 5  | loss: 0.42092 | val_0_auc: 0.52812 | val_1_auc: 0.49314 |  0:00:22s\n",
      "epoch 6  | loss: 0.40447 | val_0_auc: 0.5367  | val_1_auc: 0.48722 |  0:00:26s\n",
      "epoch 7  | loss: 0.40054 | val_0_auc: 0.54026 | val_1_auc: 0.49792 |  0:00:29s\n",
      "epoch 8  | loss: 0.39099 | val_0_auc: 0.5386  | val_1_auc: 0.52128 |  0:00:33s\n",
      "epoch 9  | loss: 0.38452 | val_0_auc: 0.55491 | val_1_auc: 0.54759 |  0:00:37s\n",
      "epoch 10 | loss: 0.38458 | val_0_auc: 0.54413 | val_1_auc: 0.5214  |  0:00:40s\n",
      "epoch 11 | loss: 0.37023 | val_0_auc: 0.5623  | val_1_auc: 0.53175 |  0:00:44s\n",
      "epoch 12 | loss: 0.36225 | val_0_auc: 0.54558 | val_1_auc: 0.55597 |  0:00:48s\n",
      "epoch 13 | loss: 0.36847 | val_0_auc: 0.54576 | val_1_auc: 0.5496  |  0:00:51s\n",
      "epoch 14 | loss: 0.35967 | val_0_auc: 0.56535 | val_1_auc: 0.53498 |  0:00:55s\n",
      "epoch 15 | loss: 0.36614 | val_0_auc: 0.55572 | val_1_auc: 0.55174 |  0:00:59s\n",
      "epoch 16 | loss: 0.35778 | val_0_auc: 0.56326 | val_1_auc: 0.53923 |  0:01:03s\n",
      "epoch 17 | loss: 0.3542  | val_0_auc: 0.5718  | val_1_auc: 0.53489 |  0:01:07s\n",
      "epoch 18 | loss: 0.35537 | val_0_auc: 0.60201 | val_1_auc: 0.54781 |  0:01:11s\n",
      "epoch 19 | loss: 0.35236 | val_0_auc: 0.592   | val_1_auc: 0.52696 |  0:01:14s\n",
      "epoch 20 | loss: 0.35398 | val_0_auc: 0.57699 | val_1_auc: 0.54082 |  0:01:18s\n",
      "epoch 21 | loss: 0.35343 | val_0_auc: 0.61268 | val_1_auc: 0.54594 |  0:01:22s\n",
      "epoch 22 | loss: 0.35113 | val_0_auc: 0.61065 | val_1_auc: 0.55438 |  0:01:26s\n",
      "epoch 23 | loss: 0.34995 | val_0_auc: 0.59104 | val_1_auc: 0.55769 |  0:01:29s\n",
      "epoch 24 | loss: 0.34648 | val_0_auc: 0.61311 | val_1_auc: 0.59217 |  0:01:33s\n",
      "epoch 25 | loss: 0.34194 | val_0_auc: 0.60875 | val_1_auc: 0.59153 |  0:01:37s\n",
      "epoch 26 | loss: 0.34425 | val_0_auc: 0.61128 | val_1_auc: 0.58203 |  0:01:40s\n",
      "epoch 27 | loss: 0.34505 | val_0_auc: 0.61622 | val_1_auc: 0.59363 |  0:01:44s\n",
      "epoch 28 | loss: 0.34281 | val_0_auc: 0.60868 | val_1_auc: 0.58955 |  0:01:48s\n",
      "epoch 29 | loss: 0.34085 | val_0_auc: 0.60919 | val_1_auc: 0.5907  |  0:01:51s\n",
      "epoch 30 | loss: 0.33818 | val_0_auc: 0.61436 | val_1_auc: 0.59068 |  0:01:55s\n",
      "epoch 31 | loss: 0.33768 | val_0_auc: 0.62833 | val_1_auc: 0.60701 |  0:01:59s\n",
      "epoch 32 | loss: 0.3378  | val_0_auc: 0.62527 | val_1_auc: 0.62301 |  0:02:02s\n",
      "epoch 33 | loss: 0.33539 | val_0_auc: 0.63078 | val_1_auc: 0.62095 |  0:02:06s\n",
      "epoch 34 | loss: 0.3329  | val_0_auc: 0.62127 | val_1_auc: 0.63188 |  0:02:10s\n",
      "epoch 35 | loss: 0.33261 | val_0_auc: 0.622   | val_1_auc: 0.62559 |  0:02:14s\n",
      "epoch 36 | loss: 0.33438 | val_0_auc: 0.62455 | val_1_auc: 0.6226  |  0:02:17s\n",
      "epoch 37 | loss: 0.33536 | val_0_auc: 0.63263 | val_1_auc: 0.61926 |  0:02:21s\n",
      "epoch 38 | loss: 0.33316 | val_0_auc: 0.63397 | val_1_auc: 0.6182  |  0:02:25s\n",
      "epoch 39 | loss: 0.33092 | val_0_auc: 0.6246  | val_1_auc: 0.63141 |  0:02:29s\n",
      "epoch 40 | loss: 0.33036 | val_0_auc: 0.6332  | val_1_auc: 0.63508 |  0:02:33s\n",
      "epoch 41 | loss: 0.33155 | val_0_auc: 0.63669 | val_1_auc: 0.64862 |  0:02:36s\n",
      "epoch 42 | loss: 0.33169 | val_0_auc: 0.64709 | val_1_auc: 0.62697 |  0:02:40s\n",
      "epoch 43 | loss: 0.33116 | val_0_auc: 0.64025 | val_1_auc: 0.62906 |  0:02:44s\n",
      "epoch 44 | loss: 0.32932 | val_0_auc: 0.63941 | val_1_auc: 0.63817 |  0:02:48s\n",
      "epoch 45 | loss: 0.32761 | val_0_auc: 0.64119 | val_1_auc: 0.64988 |  0:02:51s\n",
      "epoch 46 | loss: 0.32747 | val_0_auc: 0.63411 | val_1_auc: 0.65083 |  0:02:55s\n",
      "epoch 47 | loss: 0.32847 | val_0_auc: 0.63629 | val_1_auc: 0.64688 |  0:02:59s\n",
      "epoch 48 | loss: 0.32682 | val_0_auc: 0.61832 | val_1_auc: 0.63638 |  0:03:03s\n",
      "epoch 49 | loss: 0.32701 | val_0_auc: 0.62578 | val_1_auc: 0.64211 |  0:03:06s\n",
      "epoch 50 | loss: 0.3284  | val_0_auc: 0.63595 | val_1_auc: 0.63401 |  0:03:10s\n",
      "epoch 51 | loss: 0.3295  | val_0_auc: 0.63271 | val_1_auc: 0.64196 |  0:03:14s\n",
      "epoch 52 | loss: 0.32548 | val_0_auc: 0.6366  | val_1_auc: 0.65419 |  0:03:17s\n",
      "epoch 53 | loss: 0.32653 | val_0_auc: 0.63132 | val_1_auc: 0.64882 |  0:03:21s\n",
      "epoch 54 | loss: 0.32632 | val_0_auc: 0.65088 | val_1_auc: 0.64528 |  0:03:25s\n",
      "epoch 55 | loss: 0.32489 | val_0_auc: 0.65009 | val_1_auc: 0.63742 |  0:03:28s\n",
      "epoch 56 | loss: 0.32416 | val_0_auc: 0.64421 | val_1_auc: 0.65284 |  0:03:32s\n",
      "epoch 57 | loss: 0.32583 | val_0_auc: 0.64341 | val_1_auc: 0.65642 |  0:03:36s\n",
      "epoch 58 | loss: 0.32328 | val_0_auc: 0.65042 | val_1_auc: 0.65528 |  0:03:39s\n",
      "epoch 59 | loss: 0.32244 | val_0_auc: 0.64609 | val_1_auc: 0.64984 |  0:03:43s\n",
      "epoch 60 | loss: 0.32406 | val_0_auc: 0.65056 | val_1_auc: 0.64481 |  0:03:47s\n",
      "epoch 61 | loss: 0.32407 | val_0_auc: 0.65139 | val_1_auc: 0.64087 |  0:03:50s\n",
      "epoch 62 | loss: 0.32286 | val_0_auc: 0.64887 | val_1_auc: 0.63535 |  0:03:54s\n",
      "epoch 63 | loss: 0.32364 | val_0_auc: 0.65003 | val_1_auc: 0.64111 |  0:03:58s\n",
      "epoch 64 | loss: 0.32347 | val_0_auc: 0.65063 | val_1_auc: 0.63957 |  0:04:02s\n",
      "epoch 65 | loss: 0.32207 | val_0_auc: 0.653   | val_1_auc: 0.64255 |  0:04:05s\n",
      "epoch 66 | loss: 0.32195 | val_0_auc: 0.65527 | val_1_auc: 0.64905 |  0:04:09s\n",
      "epoch 67 | loss: 0.3243  | val_0_auc: 0.6524  | val_1_auc: 0.64287 |  0:04:13s\n",
      "epoch 68 | loss: 0.32258 | val_0_auc: 0.6505  | val_1_auc: 0.63524 |  0:04:17s\n",
      "epoch 69 | loss: 0.32369 | val_0_auc: 0.65786 | val_1_auc: 0.64064 |  0:04:20s\n",
      "epoch 70 | loss: 0.32215 | val_0_auc: 0.66245 | val_1_auc: 0.64863 |  0:04:24s\n",
      "epoch 71 | loss: 0.32136 | val_0_auc: 0.65481 | val_1_auc: 0.64943 |  0:04:28s\n",
      "epoch 72 | loss: 0.32061 | val_0_auc: 0.64907 | val_1_auc: 0.64844 |  0:04:31s\n",
      "\n",
      "Early stopping occurred at epoch 72 with best_epoch = 57 and best_val_1_auc = 0.65642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 15:02:54,776] Trial 35 finished with value: 0.6564150812867521 and parameters: {'n_d': 56, 'n_a': 23, 'n_steps': 4, 'gamma': 1.7015248134442216, 'lambda_sparse': 2.905546589485422e-06, 'lr': 0.0022886035279616507}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.61492 | val_0_auc: 0.51322 | val_1_auc: 0.50623 |  0:00:03s\n",
      "epoch 1  | loss: 2.43936 | val_0_auc: 0.51284 | val_1_auc: 0.52088 |  0:00:07s\n",
      "epoch 2  | loss: 2.28827 | val_0_auc: 0.51778 | val_1_auc: 0.52841 |  0:00:11s\n",
      "epoch 3  | loss: 2.15283 | val_0_auc: 0.52537 | val_1_auc: 0.52721 |  0:00:15s\n",
      "epoch 4  | loss: 1.98766 | val_0_auc: 0.52261 | val_1_auc: 0.51022 |  0:00:19s\n",
      "epoch 5  | loss: 1.8565  | val_0_auc: 0.51129 | val_1_auc: 0.50921 |  0:00:23s\n",
      "epoch 6  | loss: 1.71135 | val_0_auc: 0.51032 | val_1_auc: 0.52455 |  0:00:27s\n",
      "epoch 7  | loss: 1.58084 | val_0_auc: 0.50475 | val_1_auc: 0.52301 |  0:00:31s\n",
      "epoch 8  | loss: 1.45257 | val_0_auc: 0.51115 | val_1_auc: 0.52257 |  0:00:35s\n",
      "epoch 9  | loss: 1.35591 | val_0_auc: 0.50985 | val_1_auc: 0.51799 |  0:00:39s\n",
      "epoch 10 | loss: 1.257   | val_0_auc: 0.50002 | val_1_auc: 0.52831 |  0:00:43s\n",
      "epoch 11 | loss: 1.1442  | val_0_auc: 0.4951  | val_1_auc: 0.5221  |  0:00:46s\n",
      "epoch 12 | loss: 1.08478 | val_0_auc: 0.49497 | val_1_auc: 0.52411 |  0:00:50s\n",
      "epoch 13 | loss: 0.99821 | val_0_auc: 0.48711 | val_1_auc: 0.50628 |  0:00:54s\n",
      "epoch 14 | loss: 0.93445 | val_0_auc: 0.4859  | val_1_auc: 0.52784 |  0:00:58s\n",
      "epoch 15 | loss: 0.84929 | val_0_auc: 0.49823 | val_1_auc: 0.52241 |  0:01:02s\n",
      "epoch 16 | loss: 0.79618 | val_0_auc: 0.49004 | val_1_auc: 0.50336 |  0:01:06s\n",
      "epoch 17 | loss: 0.75798 | val_0_auc: 0.49403 | val_1_auc: 0.52192 |  0:01:09s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 2 and best_val_1_auc = 0.52841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 15:04:06,278] Trial 36 finished with value: 0.528406779661017 and parameters: {'n_d': 27, 'n_a': 36, 'n_steps': 5, 'gamma': 1.2422925053604714, 'lambda_sparse': 0.00020356388004552533, 'lr': 0.00020417379650353979}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.88558 | val_0_auc: 0.50052 | val_1_auc: 0.51281 |  0:00:03s\n",
      "epoch 1  | loss: 0.74215 | val_0_auc: 0.4701  | val_1_auc: 0.50591 |  0:00:07s\n",
      "epoch 2  | loss: 0.63732 | val_0_auc: 0.46967 | val_1_auc: 0.49502 |  0:00:11s\n",
      "epoch 3  | loss: 0.56297 | val_0_auc: 0.46712 | val_1_auc: 0.49746 |  0:00:15s\n",
      "epoch 4  | loss: 0.52347 | val_0_auc: 0.47184 | val_1_auc: 0.48483 |  0:00:19s\n",
      "epoch 5  | loss: 0.49801 | val_0_auc: 0.46721 | val_1_auc: 0.48845 |  0:00:23s\n",
      "epoch 6  | loss: 0.48426 | val_0_auc: 0.47402 | val_1_auc: 0.50159 |  0:00:26s\n",
      "epoch 7  | loss: 0.46952 | val_0_auc: 0.47602 | val_1_auc: 0.50042 |  0:00:30s\n",
      "epoch 8  | loss: 0.46411 | val_0_auc: 0.48454 | val_1_auc: 0.49475 |  0:00:34s\n",
      "epoch 9  | loss: 0.46074 | val_0_auc: 0.48426 | val_1_auc: 0.50624 |  0:00:38s\n",
      "epoch 10 | loss: 0.44983 | val_0_auc: 0.48311 | val_1_auc: 0.49704 |  0:00:42s\n",
      "epoch 11 | loss: 0.43936 | val_0_auc: 0.49132 | val_1_auc: 0.50467 |  0:00:45s\n",
      "epoch 12 | loss: 0.43775 | val_0_auc: 0.4935  | val_1_auc: 0.52196 |  0:00:49s\n",
      "epoch 13 | loss: 0.43288 | val_0_auc: 0.50051 | val_1_auc: 0.51311 |  0:00:53s\n",
      "epoch 14 | loss: 0.42849 | val_0_auc: 0.5016  | val_1_auc: 0.51154 |  0:00:57s\n",
      "epoch 15 | loss: 0.41909 | val_0_auc: 0.49561 | val_1_auc: 0.50632 |  0:01:00s\n",
      "epoch 16 | loss: 0.41847 | val_0_auc: 0.50025 | val_1_auc: 0.5198  |  0:01:04s\n",
      "epoch 17 | loss: 0.41178 | val_0_auc: 0.50989 | val_1_auc: 0.50868 |  0:01:08s\n",
      "epoch 18 | loss: 0.40587 | val_0_auc: 0.51098 | val_1_auc: 0.50332 |  0:01:12s\n",
      "epoch 19 | loss: 0.40354 | val_0_auc: 0.51609 | val_1_auc: 0.50631 |  0:01:16s\n",
      "epoch 20 | loss: 0.40406 | val_0_auc: 0.51326 | val_1_auc: 0.51036 |  0:01:19s\n",
      "epoch 21 | loss: 0.40291 | val_0_auc: 0.51846 | val_1_auc: 0.50982 |  0:01:23s\n",
      "epoch 22 | loss: 0.39661 | val_0_auc: 0.51097 | val_1_auc: 0.5129  |  0:01:27s\n",
      "epoch 23 | loss: 0.39255 | val_0_auc: 0.52061 | val_1_auc: 0.52778 |  0:01:31s\n",
      "epoch 24 | loss: 0.38938 | val_0_auc: 0.52177 | val_1_auc: 0.52554 |  0:01:34s\n",
      "epoch 25 | loss: 0.3877  | val_0_auc: 0.53101 | val_1_auc: 0.53182 |  0:01:38s\n",
      "epoch 26 | loss: 0.39085 | val_0_auc: 0.52824 | val_1_auc: 0.5384  |  0:01:42s\n",
      "epoch 27 | loss: 0.38346 | val_0_auc: 0.52343 | val_1_auc: 0.52594 |  0:01:46s\n",
      "epoch 28 | loss: 0.38456 | val_0_auc: 0.52049 | val_1_auc: 0.53288 |  0:01:49s\n",
      "epoch 29 | loss: 0.37871 | val_0_auc: 0.52503 | val_1_auc: 0.53594 |  0:01:53s\n",
      "epoch 30 | loss: 0.37604 | val_0_auc: 0.5285  | val_1_auc: 0.52221 |  0:01:57s\n",
      "epoch 31 | loss: 0.37225 | val_0_auc: 0.55    | val_1_auc: 0.53999 |  0:02:01s\n",
      "epoch 32 | loss: 0.38064 | val_0_auc: 0.54546 | val_1_auc: 0.53383 |  0:02:04s\n",
      "epoch 33 | loss: 0.37401 | val_0_auc: 0.55002 | val_1_auc: 0.53377 |  0:02:08s\n",
      "epoch 34 | loss: 0.3665  | val_0_auc: 0.55593 | val_1_auc: 0.53726 |  0:02:12s\n",
      "epoch 35 | loss: 0.36892 | val_0_auc: 0.55238 | val_1_auc: 0.54569 |  0:02:16s\n",
      "epoch 36 | loss: 0.37    | val_0_auc: 0.54734 | val_1_auc: 0.54011 |  0:02:20s\n",
      "epoch 37 | loss: 0.36553 | val_0_auc: 0.54677 | val_1_auc: 0.55542 |  0:02:23s\n",
      "epoch 38 | loss: 0.368   | val_0_auc: 0.55604 | val_1_auc: 0.55497 |  0:02:27s\n",
      "epoch 39 | loss: 0.3664  | val_0_auc: 0.55568 | val_1_auc: 0.55633 |  0:02:31s\n",
      "epoch 40 | loss: 0.36614 | val_0_auc: 0.5544  | val_1_auc: 0.5472  |  0:02:35s\n",
      "epoch 41 | loss: 0.36003 | val_0_auc: 0.54434 | val_1_auc: 0.54597 |  0:02:38s\n",
      "epoch 42 | loss: 0.36402 | val_0_auc: 0.55262 | val_1_auc: 0.54541 |  0:02:42s\n",
      "epoch 43 | loss: 0.35745 | val_0_auc: 0.54959 | val_1_auc: 0.54393 |  0:02:46s\n",
      "epoch 44 | loss: 0.36479 | val_0_auc: 0.55024 | val_1_auc: 0.55272 |  0:02:50s\n",
      "epoch 45 | loss: 0.35895 | val_0_auc: 0.56211 | val_1_auc: 0.54534 |  0:02:54s\n",
      "epoch 46 | loss: 0.35722 | val_0_auc: 0.55854 | val_1_auc: 0.55667 |  0:02:57s\n",
      "epoch 47 | loss: 0.35741 | val_0_auc: 0.56412 | val_1_auc: 0.56445 |  0:03:01s\n",
      "epoch 48 | loss: 0.35646 | val_0_auc: 0.55841 | val_1_auc: 0.57377 |  0:03:05s\n",
      "epoch 49 | loss: 0.36103 | val_0_auc: 0.54787 | val_1_auc: 0.56213 |  0:03:09s\n",
      "epoch 50 | loss: 0.35292 | val_0_auc: 0.55182 | val_1_auc: 0.56344 |  0:03:12s\n",
      "epoch 51 | loss: 0.35257 | val_0_auc: 0.54853 | val_1_auc: 0.57528 |  0:03:16s\n",
      "epoch 52 | loss: 0.35504 | val_0_auc: 0.55092 | val_1_auc: 0.57107 |  0:03:20s\n",
      "epoch 53 | loss: 0.35526 | val_0_auc: 0.55226 | val_1_auc: 0.56918 |  0:03:24s\n",
      "epoch 54 | loss: 0.35597 | val_0_auc: 0.55327 | val_1_auc: 0.57044 |  0:03:27s\n",
      "epoch 55 | loss: 0.35224 | val_0_auc: 0.55335 | val_1_auc: 0.5756  |  0:03:31s\n",
      "epoch 56 | loss: 0.35045 | val_0_auc: 0.55831 | val_1_auc: 0.57975 |  0:03:35s\n",
      "epoch 57 | loss: 0.35173 | val_0_auc: 0.55619 | val_1_auc: 0.57989 |  0:03:39s\n",
      "epoch 58 | loss: 0.34928 | val_0_auc: 0.56648 | val_1_auc: 0.5834  |  0:03:43s\n",
      "epoch 59 | loss: 0.35195 | val_0_auc: 0.57614 | val_1_auc: 0.58257 |  0:03:46s\n",
      "epoch 60 | loss: 0.3528  | val_0_auc: 0.57083 | val_1_auc: 0.57176 |  0:03:50s\n",
      "epoch 61 | loss: 0.35235 | val_0_auc: 0.57871 | val_1_auc: 0.56885 |  0:03:54s\n",
      "epoch 62 | loss: 0.34851 | val_0_auc: 0.56051 | val_1_auc: 0.57147 |  0:03:57s\n",
      "epoch 63 | loss: 0.34794 | val_0_auc: 0.56005 | val_1_auc: 0.57814 |  0:04:01s\n",
      "epoch 64 | loss: 0.34711 | val_0_auc: 0.5694  | val_1_auc: 0.5774  |  0:04:05s\n",
      "epoch 65 | loss: 0.34824 | val_0_auc: 0.56398 | val_1_auc: 0.57805 |  0:04:08s\n",
      "epoch 66 | loss: 0.34679 | val_0_auc: 0.56739 | val_1_auc: 0.57225 |  0:04:12s\n",
      "epoch 67 | loss: 0.34699 | val_0_auc: 0.56757 | val_1_auc: 0.56748 |  0:04:16s\n",
      "epoch 68 | loss: 0.35051 | val_0_auc: 0.57175 | val_1_auc: 0.57107 |  0:04:20s\n",
      "epoch 69 | loss: 0.3459  | val_0_auc: 0.57364 | val_1_auc: 0.57743 |  0:04:24s\n",
      "epoch 70 | loss: 0.34534 | val_0_auc: 0.57398 | val_1_auc: 0.57993 |  0:04:28s\n",
      "epoch 71 | loss: 0.34488 | val_0_auc: 0.58134 | val_1_auc: 0.58009 |  0:04:31s\n",
      "epoch 72 | loss: 0.34492 | val_0_auc: 0.57975 | val_1_auc: 0.58873 |  0:04:35s\n",
      "epoch 73 | loss: 0.34303 | val_0_auc: 0.58241 | val_1_auc: 0.60069 |  0:04:39s\n",
      "epoch 74 | loss: 0.34648 | val_0_auc: 0.57444 | val_1_auc: 0.58725 |  0:04:42s\n",
      "epoch 75 | loss: 0.34494 | val_0_auc: 0.57524 | val_1_auc: 0.58217 |  0:04:46s\n",
      "epoch 76 | loss: 0.34199 | val_0_auc: 0.57675 | val_1_auc: 0.58958 |  0:04:50s\n",
      "epoch 77 | loss: 0.34258 | val_0_auc: 0.56789 | val_1_auc: 0.58112 |  0:04:54s\n",
      "epoch 78 | loss: 0.34315 | val_0_auc: 0.58328 | val_1_auc: 0.58404 |  0:04:58s\n",
      "epoch 79 | loss: 0.34241 | val_0_auc: 0.5828  | val_1_auc: 0.57719 |  0:05:01s\n",
      "epoch 80 | loss: 0.34142 | val_0_auc: 0.58254 | val_1_auc: 0.58522 |  0:05:05s\n",
      "epoch 81 | loss: 0.34279 | val_0_auc: 0.58503 | val_1_auc: 0.58057 |  0:05:09s\n",
      "epoch 82 | loss: 0.34238 | val_0_auc: 0.57901 | val_1_auc: 0.57192 |  0:05:13s\n",
      "epoch 83 | loss: 0.33994 | val_0_auc: 0.57338 | val_1_auc: 0.58891 |  0:05:17s\n",
      "epoch 84 | loss: 0.34003 | val_0_auc: 0.57985 | val_1_auc: 0.59423 |  0:05:21s\n",
      "epoch 85 | loss: 0.33917 | val_0_auc: 0.58273 | val_1_auc: 0.59682 |  0:05:25s\n",
      "epoch 86 | loss: 0.3419  | val_0_auc: 0.58194 | val_1_auc: 0.5994  |  0:05:28s\n",
      "epoch 87 | loss: 0.34218 | val_0_auc: 0.58706 | val_1_auc: 0.59669 |  0:05:32s\n",
      "epoch 88 | loss: 0.33981 | val_0_auc: 0.57461 | val_1_auc: 0.58643 |  0:05:36s\n",
      "\n",
      "Early stopping occurred at epoch 88 with best_epoch = 73 and best_val_1_auc = 0.60069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 15:09:44,057] Trial 37 finished with value: 0.6006876513317191 and parameters: {'n_d': 36, 'n_a': 54, 'n_steps': 4, 'gamma': 1.88987621101333, 'lambda_sparse': 4.266050383023502e-05, 'lr': 0.00046043864034844666}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.88352 | val_0_auc: 0.53623 | val_1_auc: 0.52384 |  0:00:06s\n",
      "epoch 1  | loss: 0.56669 | val_0_auc: 0.50524 | val_1_auc: 0.51861 |  0:00:12s\n",
      "epoch 2  | loss: 0.52408 | val_0_auc: 0.51731 | val_1_auc: 0.52423 |  0:00:17s\n",
      "epoch 3  | loss: 0.50575 | val_0_auc: 0.53113 | val_1_auc: 0.53848 |  0:00:23s\n",
      "epoch 4  | loss: 0.47424 | val_0_auc: 0.53797 | val_1_auc: 0.50486 |  0:00:29s\n",
      "epoch 5  | loss: 0.47298 | val_0_auc: 0.52582 | val_1_auc: 0.48571 |  0:00:35s\n",
      "epoch 6  | loss: 0.45804 | val_0_auc: 0.53961 | val_1_auc: 0.50095 |  0:00:41s\n",
      "epoch 7  | loss: 0.43112 | val_0_auc: 0.54375 | val_1_auc: 0.50262 |  0:00:47s\n",
      "epoch 8  | loss: 0.43376 | val_0_auc: 0.56027 | val_1_auc: 0.51774 |  0:00:53s\n",
      "epoch 9  | loss: 0.41961 | val_0_auc: 0.54129 | val_1_auc: 0.53426 |  0:00:59s\n",
      "epoch 10 | loss: 0.41292 | val_0_auc: 0.54878 | val_1_auc: 0.55603 |  0:01:05s\n",
      "epoch 11 | loss: 0.4012  | val_0_auc: 0.55521 | val_1_auc: 0.55643 |  0:01:11s\n",
      "epoch 12 | loss: 0.39399 | val_0_auc: 0.55976 | val_1_auc: 0.54086 |  0:01:17s\n",
      "epoch 13 | loss: 0.39153 | val_0_auc: 0.55339 | val_1_auc: 0.52602 |  0:01:23s\n",
      "epoch 14 | loss: 0.38405 | val_0_auc: 0.5707  | val_1_auc: 0.53332 |  0:01:29s\n",
      "epoch 15 | loss: 0.38225 | val_0_auc: 0.5538  | val_1_auc: 0.56158 |  0:01:35s\n",
      "epoch 16 | loss: 0.37567 | val_0_auc: 0.5823  | val_1_auc: 0.55871 |  0:01:41s\n",
      "epoch 17 | loss: 0.3744  | val_0_auc: 0.56974 | val_1_auc: 0.58967 |  0:01:47s\n",
      "epoch 18 | loss: 0.37346 | val_0_auc: 0.5621  | val_1_auc: 0.55887 |  0:01:53s\n",
      "epoch 19 | loss: 0.37063 | val_0_auc: 0.56419 | val_1_auc: 0.57957 |  0:01:59s\n",
      "epoch 20 | loss: 0.36746 | val_0_auc: 0.57165 | val_1_auc: 0.54199 |  0:02:05s\n",
      "epoch 21 | loss: 0.35887 | val_0_auc: 0.57367 | val_1_auc: 0.56547 |  0:02:11s\n",
      "epoch 22 | loss: 0.36092 | val_0_auc: 0.58675 | val_1_auc: 0.56423 |  0:02:16s\n",
      "epoch 23 | loss: 0.35806 | val_0_auc: 0.5915  | val_1_auc: 0.57493 |  0:02:22s\n",
      "epoch 24 | loss: 0.35796 | val_0_auc: 0.59061 | val_1_auc: 0.5868  |  0:02:28s\n",
      "epoch 25 | loss: 0.35696 | val_0_auc: 0.60902 | val_1_auc: 0.59261 |  0:02:34s\n",
      "epoch 26 | loss: 0.35415 | val_0_auc: 0.60229 | val_1_auc: 0.59005 |  0:02:41s\n",
      "epoch 27 | loss: 0.35113 | val_0_auc: 0.61622 | val_1_auc: 0.55198 |  0:02:47s\n",
      "epoch 28 | loss: 0.35219 | val_0_auc: 0.61151 | val_1_auc: 0.58073 |  0:02:53s\n",
      "epoch 29 | loss: 0.35151 | val_0_auc: 0.61015 | val_1_auc: 0.59496 |  0:02:59s\n",
      "epoch 30 | loss: 0.34635 | val_0_auc: 0.59422 | val_1_auc: 0.58126 |  0:03:05s\n",
      "epoch 31 | loss: 0.34792 | val_0_auc: 0.59802 | val_1_auc: 0.58696 |  0:03:11s\n",
      "epoch 32 | loss: 0.34372 | val_0_auc: 0.61732 | val_1_auc: 0.59369 |  0:03:16s\n",
      "epoch 33 | loss: 0.34713 | val_0_auc: 0.60778 | val_1_auc: 0.56941 |  0:03:22s\n",
      "epoch 34 | loss: 0.34635 | val_0_auc: 0.60976 | val_1_auc: 0.58058 |  0:03:28s\n",
      "epoch 35 | loss: 0.34306 | val_0_auc: 0.60299 | val_1_auc: 0.58065 |  0:03:34s\n",
      "epoch 36 | loss: 0.34248 | val_0_auc: 0.59536 | val_1_auc: 0.58004 |  0:03:40s\n",
      "epoch 37 | loss: 0.34308 | val_0_auc: 0.6172  | val_1_auc: 0.5948  |  0:03:46s\n",
      "epoch 38 | loss: 0.34336 | val_0_auc: 0.60692 | val_1_auc: 0.57322 |  0:03:52s\n",
      "epoch 39 | loss: 0.34278 | val_0_auc: 0.61864 | val_1_auc: 0.58835 |  0:03:58s\n",
      "epoch 40 | loss: 0.34133 | val_0_auc: 0.6151  | val_1_auc: 0.59344 |  0:04:04s\n",
      "epoch 41 | loss: 0.34094 | val_0_auc: 0.62141 | val_1_auc: 0.58813 |  0:04:10s\n",
      "epoch 42 | loss: 0.33774 | val_0_auc: 0.62833 | val_1_auc: 0.59058 |  0:04:16s\n",
      "epoch 43 | loss: 0.33989 | val_0_auc: 0.6278  | val_1_auc: 0.61571 |  0:04:21s\n",
      "epoch 44 | loss: 0.33818 | val_0_auc: 0.6483  | val_1_auc: 0.60139 |  0:04:27s\n",
      "epoch 45 | loss: 0.33541 | val_0_auc: 0.64747 | val_1_auc: 0.60356 |  0:04:33s\n",
      "epoch 46 | loss: 0.33722 | val_0_auc: 0.64794 | val_1_auc: 0.59654 |  0:04:40s\n",
      "epoch 47 | loss: 0.33452 | val_0_auc: 0.62395 | val_1_auc: 0.60186 |  0:04:45s\n",
      "epoch 48 | loss: 0.33673 | val_0_auc: 0.63581 | val_1_auc: 0.62133 |  0:04:51s\n",
      "epoch 49 | loss: 0.33647 | val_0_auc: 0.64494 | val_1_auc: 0.61067 |  0:04:57s\n",
      "epoch 50 | loss: 0.33542 | val_0_auc: 0.64141 | val_1_auc: 0.60972 |  0:05:03s\n",
      "epoch 51 | loss: 0.34195 | val_0_auc: 0.63099 | val_1_auc: 0.60678 |  0:05:09s\n",
      "epoch 52 | loss: 0.33729 | val_0_auc: 0.63989 | val_1_auc: 0.61287 |  0:05:15s\n",
      "epoch 53 | loss: 0.33609 | val_0_auc: 0.63961 | val_1_auc: 0.62701 |  0:05:21s\n",
      "epoch 54 | loss: 0.33397 | val_0_auc: 0.63798 | val_1_auc: 0.61909 |  0:05:27s\n",
      "epoch 55 | loss: 0.33431 | val_0_auc: 0.64237 | val_1_auc: 0.63192 |  0:05:33s\n",
      "epoch 56 | loss: 0.33383 | val_0_auc: 0.63538 | val_1_auc: 0.64281 |  0:05:38s\n",
      "epoch 57 | loss: 0.33076 | val_0_auc: 0.64665 | val_1_auc: 0.62358 |  0:05:44s\n",
      "epoch 58 | loss: 0.33427 | val_0_auc: 0.64754 | val_1_auc: 0.6107  |  0:05:50s\n",
      "epoch 59 | loss: 0.33311 | val_0_auc: 0.64616 | val_1_auc: 0.60247 |  0:05:56s\n",
      "epoch 60 | loss: 0.33344 | val_0_auc: 0.6466  | val_1_auc: 0.61794 |  0:06:03s\n",
      "epoch 61 | loss: 0.33177 | val_0_auc: 0.64158 | val_1_auc: 0.60346 |  0:06:08s\n",
      "epoch 62 | loss: 0.33256 | val_0_auc: 0.64046 | val_1_auc: 0.64109 |  0:06:14s\n",
      "epoch 63 | loss: 0.33243 | val_0_auc: 0.64778 | val_1_auc: 0.62193 |  0:06:20s\n",
      "epoch 64 | loss: 0.33313 | val_0_auc: 0.64463 | val_1_auc: 0.60015 |  0:06:26s\n",
      "epoch 65 | loss: 0.33289 | val_0_auc: 0.64164 | val_1_auc: 0.61775 |  0:06:32s\n",
      "epoch 66 | loss: 0.33072 | val_0_auc: 0.65246 | val_1_auc: 0.62156 |  0:06:38s\n",
      "epoch 67 | loss: 0.33068 | val_0_auc: 0.65656 | val_1_auc: 0.6139  |  0:06:44s\n",
      "epoch 68 | loss: 0.32963 | val_0_auc: 0.65296 | val_1_auc: 0.63667 |  0:06:50s\n",
      "epoch 69 | loss: 0.33022 | val_0_auc: 0.64836 | val_1_auc: 0.62864 |  0:06:56s\n",
      "epoch 70 | loss: 0.33277 | val_0_auc: 0.65028 | val_1_auc: 0.62717 |  0:07:02s\n",
      "epoch 71 | loss: 0.33227 | val_0_auc: 0.65441 | val_1_auc: 0.63194 |  0:07:08s\n",
      "\n",
      "Early stopping occurred at epoch 71 with best_epoch = 56 and best_val_1_auc = 0.64281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 15:16:55,004] Trial 38 finished with value: 0.6428073331027326 and parameters: {'n_d': 40, 'n_a': 42, 'n_steps': 7, 'gamma': 1.4761637107025187, 'lambda_sparse': 9.462659789248206e-05, 'lr': 0.0014249095974565842}. Best is trial 17 with value: 0.6897391905914908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.54143 | val_0_auc: 0.4836  | val_1_auc: 0.47725 |  0:00:02s\n",
      "epoch 1  | loss: 0.49694 | val_0_auc: 0.48019 | val_1_auc: 0.48713 |  0:00:04s\n",
      "epoch 2  | loss: 0.47681 | val_0_auc: 0.48516 | val_1_auc: 0.48924 |  0:00:07s\n",
      "epoch 3  | loss: 0.45545 | val_0_auc: 0.4936  | val_1_auc: 0.49248 |  0:00:10s\n",
      "epoch 4  | loss: 0.44313 | val_0_auc: 0.4934  | val_1_auc: 0.50318 |  0:00:12s\n",
      "epoch 5  | loss: 0.42936 | val_0_auc: 0.49748 | val_1_auc: 0.501   |  0:00:15s\n",
      "epoch 6  | loss: 0.41191 | val_0_auc: 0.50362 | val_1_auc: 0.51451 |  0:00:17s\n",
      "epoch 7  | loss: 0.41188 | val_0_auc: 0.51492 | val_1_auc: 0.51886 |  0:00:20s\n",
      "epoch 8  | loss: 0.40432 | val_0_auc: 0.52072 | val_1_auc: 0.51843 |  0:00:22s\n",
      "epoch 9  | loss: 0.3929  | val_0_auc: 0.52452 | val_1_auc: 0.51495 |  0:00:25s\n",
      "epoch 10 | loss: 0.38468 | val_0_auc: 0.5297  | val_1_auc: 0.52007 |  0:00:28s\n",
      "epoch 11 | loss: 0.38017 | val_0_auc: 0.53582 | val_1_auc: 0.53679 |  0:00:30s\n",
      "epoch 12 | loss: 0.37825 | val_0_auc: 0.54668 | val_1_auc: 0.53823 |  0:00:33s\n",
      "epoch 13 | loss: 0.37131 | val_0_auc: 0.54504 | val_1_auc: 0.54262 |  0:00:35s\n",
      "epoch 14 | loss: 0.36828 | val_0_auc: 0.53942 | val_1_auc: 0.54529 |  0:00:38s\n",
      "epoch 15 | loss: 0.36585 | val_0_auc: 0.54959 | val_1_auc: 0.54929 |  0:00:40s\n",
      "epoch 16 | loss: 0.36578 | val_0_auc: 0.5561  | val_1_auc: 0.5618  |  0:00:43s\n",
      "epoch 17 | loss: 0.36197 | val_0_auc: 0.55696 | val_1_auc: 0.56855 |  0:00:45s\n",
      "epoch 18 | loss: 0.3594  | val_0_auc: 0.55945 | val_1_auc: 0.58084 |  0:00:48s\n",
      "epoch 19 | loss: 0.35827 | val_0_auc: 0.56052 | val_1_auc: 0.58197 |  0:00:50s\n",
      "epoch 20 | loss: 0.35846 | val_0_auc: 0.56676 | val_1_auc: 0.5825  |  0:00:53s\n",
      "epoch 21 | loss: 0.35654 | val_0_auc: 0.56761 | val_1_auc: 0.57878 |  0:00:55s\n",
      "epoch 22 | loss: 0.35309 | val_0_auc: 0.57574 | val_1_auc: 0.58493 |  0:00:58s\n",
      "epoch 23 | loss: 0.3505  | val_0_auc: 0.57735 | val_1_auc: 0.59643 |  0:01:00s\n",
      "epoch 24 | loss: 0.35063 | val_0_auc: 0.58371 | val_1_auc: 0.59462 |  0:01:03s\n",
      "epoch 25 | loss: 0.34929 | val_0_auc: 0.58609 | val_1_auc: 0.59312 |  0:01:05s\n",
      "epoch 26 | loss: 0.34549 | val_0_auc: 0.57961 | val_1_auc: 0.60087 |  0:01:07s\n",
      "epoch 27 | loss: 0.34575 | val_0_auc: 0.58602 | val_1_auc: 0.5966  |  0:01:10s\n",
      "epoch 28 | loss: 0.34495 | val_0_auc: 0.59307 | val_1_auc: 0.60411 |  0:01:12s\n",
      "epoch 29 | loss: 0.34278 | val_0_auc: 0.59873 | val_1_auc: 0.60226 |  0:01:15s\n",
      "epoch 30 | loss: 0.34405 | val_0_auc: 0.59291 | val_1_auc: 0.58971 |  0:01:17s\n",
      "epoch 31 | loss: 0.34343 | val_0_auc: 0.5982  | val_1_auc: 0.60299 |  0:01:20s\n",
      "epoch 32 | loss: 0.34036 | val_0_auc: 0.60175 | val_1_auc: 0.60389 |  0:01:22s\n",
      "epoch 33 | loss: 0.34037 | val_0_auc: 0.604   | val_1_auc: 0.60706 |  0:01:25s\n",
      "epoch 34 | loss: 0.34072 | val_0_auc: 0.60957 | val_1_auc: 0.60785 |  0:01:27s\n",
      "epoch 35 | loss: 0.34033 | val_0_auc: 0.6074  | val_1_auc: 0.60047 |  0:01:30s\n",
      "epoch 36 | loss: 0.34007 | val_0_auc: 0.60613 | val_1_auc: 0.60655 |  0:01:32s\n",
      "epoch 37 | loss: 0.33968 | val_0_auc: 0.60912 | val_1_auc: 0.61174 |  0:01:34s\n",
      "epoch 38 | loss: 0.33821 | val_0_auc: 0.60873 | val_1_auc: 0.60796 |  0:01:37s\n",
      "epoch 39 | loss: 0.33813 | val_0_auc: 0.61637 | val_1_auc: 0.61367 |  0:01:39s\n",
      "epoch 40 | loss: 0.3373  | val_0_auc: 0.61341 | val_1_auc: 0.61034 |  0:01:42s\n",
      "epoch 41 | loss: 0.33604 | val_0_auc: 0.61205 | val_1_auc: 0.60985 |  0:01:44s\n",
      "epoch 42 | loss: 0.33611 | val_0_auc: 0.61235 | val_1_auc: 0.61232 |  0:01:47s\n",
      "epoch 43 | loss: 0.33573 | val_0_auc: 0.61581 | val_1_auc: 0.61172 |  0:01:49s\n",
      "epoch 44 | loss: 0.33732 | val_0_auc: 0.61919 | val_1_auc: 0.6125  |  0:01:52s\n",
      "epoch 45 | loss: 0.33602 | val_0_auc: 0.62184 | val_1_auc: 0.61383 |  0:01:54s\n",
      "epoch 46 | loss: 0.33527 | val_0_auc: 0.6254  | val_1_auc: 0.62054 |  0:01:57s\n",
      "epoch 47 | loss: 0.33308 | val_0_auc: 0.62892 | val_1_auc: 0.61364 |  0:01:59s\n",
      "epoch 48 | loss: 0.33328 | val_0_auc: 0.62951 | val_1_auc: 0.62152 |  0:02:02s\n",
      "epoch 49 | loss: 0.33228 | val_0_auc: 0.6338  | val_1_auc: 0.61251 |  0:02:04s\n",
      "epoch 50 | loss: 0.33278 | val_0_auc: 0.63505 | val_1_auc: 0.6183  |  0:02:07s\n",
      "epoch 51 | loss: 0.33297 | val_0_auc: 0.63407 | val_1_auc: 0.61646 |  0:02:09s\n",
      "epoch 52 | loss: 0.33154 | val_0_auc: 0.63735 | val_1_auc: 0.61624 |  0:02:11s\n",
      "epoch 53 | loss: 0.33317 | val_0_auc: 0.6371  | val_1_auc: 0.62436 |  0:02:14s\n",
      "epoch 54 | loss: 0.33216 | val_0_auc: 0.63512 | val_1_auc: 0.61434 |  0:02:16s\n",
      "epoch 55 | loss: 0.33107 | val_0_auc: 0.63222 | val_1_auc: 0.61995 |  0:02:19s\n",
      "epoch 56 | loss: 0.32987 | val_0_auc: 0.63901 | val_1_auc: 0.61987 |  0:02:22s\n",
      "epoch 57 | loss: 0.33174 | val_0_auc: 0.6312  | val_1_auc: 0.60915 |  0:02:24s\n",
      "epoch 58 | loss: 0.32899 | val_0_auc: 0.63303 | val_1_auc: 0.60485 |  0:02:27s\n",
      "epoch 59 | loss: 0.3297  | val_0_auc: 0.63841 | val_1_auc: 0.60741 |  0:02:30s\n",
      "epoch 60 | loss: 0.32908 | val_0_auc: 0.64009 | val_1_auc: 0.61043 |  0:02:32s\n",
      "epoch 61 | loss: 0.32641 | val_0_auc: 0.64127 | val_1_auc: 0.61556 |  0:02:35s\n",
      "epoch 62 | loss: 0.32884 | val_0_auc: 0.64066 | val_1_auc: 0.6207  |  0:02:37s\n",
      "epoch 63 | loss: 0.32857 | val_0_auc: 0.64487 | val_1_auc: 0.62703 |  0:02:39s\n",
      "epoch 64 | loss: 0.3281  | val_0_auc: 0.64366 | val_1_auc: 0.62952 |  0:02:42s\n",
      "epoch 65 | loss: 0.32793 | val_0_auc: 0.6443  | val_1_auc: 0.62896 |  0:02:44s\n",
      "epoch 66 | loss: 0.32754 | val_0_auc: 0.65275 | val_1_auc: 0.62808 |  0:02:47s\n",
      "epoch 67 | loss: 0.32819 | val_0_auc: 0.65069 | val_1_auc: 0.63091 |  0:02:49s\n",
      "epoch 68 | loss: 0.32794 | val_0_auc: 0.65022 | val_1_auc: 0.63115 |  0:02:52s\n",
      "epoch 69 | loss: 0.32522 | val_0_auc: 0.65141 | val_1_auc: 0.63015 |  0:02:55s\n",
      "epoch 70 | loss: 0.32579 | val_0_auc: 0.64893 | val_1_auc: 0.63571 |  0:02:57s\n",
      "epoch 71 | loss: 0.32767 | val_0_auc: 0.64609 | val_1_auc: 0.63064 |  0:03:00s\n",
      "epoch 72 | loss: 0.32675 | val_0_auc: 0.64822 | val_1_auc: 0.63537 |  0:03:02s\n",
      "epoch 73 | loss: 0.32662 | val_0_auc: 0.65139 | val_1_auc: 0.63864 |  0:03:05s\n",
      "epoch 74 | loss: 0.32764 | val_0_auc: 0.64906 | val_1_auc: 0.6391  |  0:03:07s\n",
      "epoch 75 | loss: 0.32437 | val_0_auc: 0.64727 | val_1_auc: 0.63095 |  0:03:09s\n",
      "epoch 76 | loss: 0.32733 | val_0_auc: 0.64939 | val_1_auc: 0.63439 |  0:03:12s\n",
      "epoch 77 | loss: 0.32674 | val_0_auc: 0.65076 | val_1_auc: 0.63075 |  0:03:14s\n",
      "epoch 78 | loss: 0.3252  | val_0_auc: 0.65321 | val_1_auc: 0.63032 |  0:03:17s\n",
      "epoch 79 | loss: 0.32368 | val_0_auc: 0.65083 | val_1_auc: 0.6371  |  0:03:19s\n",
      "epoch 80 | loss: 0.3266  | val_0_auc: 0.65186 | val_1_auc: 0.63445 |  0:03:22s\n",
      "epoch 81 | loss: 0.32375 | val_0_auc: 0.65195 | val_1_auc: 0.63897 |  0:03:24s\n",
      "epoch 82 | loss: 0.32266 | val_0_auc: 0.65255 | val_1_auc: 0.64142 |  0:03:27s\n",
      "epoch 83 | loss: 0.32342 | val_0_auc: 0.65614 | val_1_auc: 0.64214 |  0:03:29s\n",
      "epoch 84 | loss: 0.32438 | val_0_auc: 0.65773 | val_1_auc: 0.63874 |  0:03:32s\n",
      "epoch 85 | loss: 0.32262 | val_0_auc: 0.65711 | val_1_auc: 0.63326 |  0:03:34s\n",
      "epoch 86 | loss: 0.32309 | val_0_auc: 0.65738 | val_1_auc: 0.63126 |  0:03:37s\n",
      "epoch 87 | loss: 0.32193 | val_0_auc: 0.65956 | val_1_auc: 0.63662 |  0:03:39s\n",
      "epoch 88 | loss: 0.32355 | val_0_auc: 0.65423 | val_1_auc: 0.6409  |  0:03:41s\n",
      "epoch 89 | loss: 0.3208  | val_0_auc: 0.6587  | val_1_auc: 0.64808 |  0:03:44s\n",
      "epoch 90 | loss: 0.32187 | val_0_auc: 0.65471 | val_1_auc: 0.64222 |  0:03:46s\n",
      "epoch 91 | loss: 0.32191 | val_0_auc: 0.65673 | val_1_auc: 0.64164 |  0:03:49s\n",
      "epoch 92 | loss: 0.32095 | val_0_auc: 0.65439 | val_1_auc: 0.63914 |  0:03:52s\n",
      "epoch 93 | loss: 0.32221 | val_0_auc: 0.65672 | val_1_auc: 0.63967 |  0:03:55s\n",
      "epoch 94 | loss: 0.32074 | val_0_auc: 0.65393 | val_1_auc: 0.64235 |  0:03:57s\n",
      "epoch 95 | loss: 0.32257 | val_0_auc: 0.65345 | val_1_auc: 0.63547 |  0:03:59s\n",
      "epoch 96 | loss: 0.32203 | val_0_auc: 0.65366 | val_1_auc: 0.62916 |  0:04:02s\n",
      "epoch 97 | loss: 0.32263 | val_0_auc: 0.65705 | val_1_auc: 0.6314  |  0:04:04s\n",
      "epoch 98 | loss: 0.32094 | val_0_auc: 0.65748 | val_1_auc: 0.63078 |  0:04:07s\n",
      "epoch 99 | loss: 0.32064 | val_0_auc: 0.6586  | val_1_auc: 0.63518 |  0:04:10s\n",
      "epoch 100| loss: 0.3193  | val_0_auc: 0.65866 | val_1_auc: 0.63709 |  0:04:12s\n",
      "epoch 101| loss: 0.32134 | val_0_auc: 0.65987 | val_1_auc: 0.63432 |  0:04:15s\n",
      "epoch 102| loss: 0.32004 | val_0_auc: 0.65646 | val_1_auc: 0.6351  |  0:04:17s\n",
      "epoch 103| loss: 0.31942 | val_0_auc: 0.6545  | val_1_auc: 0.63465 |  0:04:20s\n",
      "epoch 104| loss: 0.32093 | val_0_auc: 0.65563 | val_1_auc: 0.63094 |  0:04:22s\n",
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 89 and best_val_1_auc = 0.64808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-28 15:21:18,850] Trial 39 finished with value: 0.6480837080594949 and parameters: {'n_d': 25, 'n_a': 24, 'n_steps': 3, 'gamma': 1.1064166425359885, 'lambda_sparse': 1.037615512861142e-06, 'lr': 0.0008666777106614362}. Best is trial 17 with value: 0.6897391905914908.\n",
      "Best AUC: 0.6897391905914908\n",
      "Best parameters: {'n_d': 37, 'n_a': 44, 'n_steps': 5, 'gamma': 1.020080536443378, 'lambda_sparse': 0.00017353660543982074, 'lr': 0.0028875987502131164}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=40, show_progress_bar=True)\n",
    "\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best parameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304eed7",
   "metadata": {},
   "source": [
    "Final retraining using the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e3f4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.57335 | val_0_auc: 0.46297 | val_1_auc: 0.47098 |  0:00:03s\n",
      "epoch 1  | loss: 0.50533 | val_0_auc: 0.49664 | val_1_auc: 0.49988 |  0:00:06s\n",
      "epoch 2  | loss: 0.46029 | val_0_auc: 0.51098 | val_1_auc: 0.4882  |  0:00:09s\n",
      "epoch 3  | loss: 0.41973 | val_0_auc: 0.52857 | val_1_auc: 0.49668 |  0:00:12s\n",
      "epoch 4  | loss: 0.40631 | val_0_auc: 0.55205 | val_1_auc: 0.52753 |  0:00:15s\n",
      "epoch 5  | loss: 0.39626 | val_0_auc: 0.54954 | val_1_auc: 0.53482 |  0:00:18s\n",
      "epoch 6  | loss: 0.38969 | val_0_auc: 0.56262 | val_1_auc: 0.55447 |  0:00:22s\n",
      "epoch 7  | loss: 0.37203 | val_0_auc: 0.5545  | val_1_auc: 0.55976 |  0:00:26s\n",
      "epoch 8  | loss: 0.36973 | val_0_auc: 0.56375 | val_1_auc: 0.56695 |  0:00:30s\n",
      "epoch 9  | loss: 0.36433 | val_0_auc: 0.597   | val_1_auc: 0.58885 |  0:00:35s\n",
      "epoch 10 | loss: 0.35547 | val_0_auc: 0.59289 | val_1_auc: 0.57147 |  0:00:40s\n",
      "epoch 11 | loss: 0.35374 | val_0_auc: 0.58789 | val_1_auc: 0.60863 |  0:00:45s\n",
      "epoch 12 | loss: 0.34972 | val_0_auc: 0.60563 | val_1_auc: 0.6003  |  0:00:50s\n",
      "epoch 13 | loss: 0.34676 | val_0_auc: 0.61178 | val_1_auc: 0.61368 |  0:00:56s\n",
      "epoch 14 | loss: 0.34536 | val_0_auc: 0.62698 | val_1_auc: 0.60646 |  0:01:00s\n",
      "epoch 15 | loss: 0.34271 | val_0_auc: 0.61945 | val_1_auc: 0.6255  |  0:01:06s\n",
      "epoch 16 | loss: 0.3413  | val_0_auc: 0.62225 | val_1_auc: 0.61994 |  0:01:12s\n",
      "epoch 17 | loss: 0.34129 | val_0_auc: 0.631   | val_1_auc: 0.60249 |  0:01:17s\n",
      "epoch 18 | loss: 0.34017 | val_0_auc: 0.63156 | val_1_auc: 0.63187 |  0:01:22s\n",
      "epoch 19 | loss: 0.33465 | val_0_auc: 0.64063 | val_1_auc: 0.63117 |  0:01:28s\n",
      "epoch 20 | loss: 0.333   | val_0_auc: 0.63727 | val_1_auc: 0.64428 |  0:01:33s\n",
      "epoch 21 | loss: 0.33439 | val_0_auc: 0.62961 | val_1_auc: 0.64268 |  0:01:38s\n",
      "epoch 22 | loss: 0.33267 | val_0_auc: 0.63937 | val_1_auc: 0.63169 |  0:01:43s\n",
      "epoch 23 | loss: 0.33157 | val_0_auc: 0.6453  | val_1_auc: 0.64123 |  0:01:48s\n",
      "epoch 24 | loss: 0.33031 | val_0_auc: 0.6414  | val_1_auc: 0.63767 |  0:01:53s\n",
      "epoch 25 | loss: 0.32874 | val_0_auc: 0.64989 | val_1_auc: 0.63642 |  0:01:58s\n",
      "epoch 26 | loss: 0.33158 | val_0_auc: 0.65408 | val_1_auc: 0.65012 |  0:02:03s\n",
      "epoch 27 | loss: 0.32861 | val_0_auc: 0.66102 | val_1_auc: 0.65273 |  0:02:08s\n",
      "epoch 28 | loss: 0.32813 | val_0_auc: 0.66222 | val_1_auc: 0.65486 |  0:02:13s\n",
      "epoch 29 | loss: 0.32661 | val_0_auc: 0.65086 | val_1_auc: 0.65206 |  0:02:18s\n",
      "epoch 30 | loss: 0.32392 | val_0_auc: 0.66758 | val_1_auc: 0.66049 |  0:02:23s\n",
      "epoch 31 | loss: 0.32564 | val_0_auc: 0.66658 | val_1_auc: 0.6542  |  0:02:28s\n",
      "epoch 32 | loss: 0.32427 | val_0_auc: 0.66795 | val_1_auc: 0.67339 |  0:02:33s\n",
      "epoch 33 | loss: 0.32492 | val_0_auc: 0.66225 | val_1_auc: 0.66741 |  0:02:38s\n",
      "epoch 34 | loss: 0.32478 | val_0_auc: 0.6565  | val_1_auc: 0.68224 |  0:02:42s\n",
      "epoch 35 | loss: 0.32508 | val_0_auc: 0.65852 | val_1_auc: 0.66458 |  0:02:45s\n",
      "epoch 36 | loss: 0.32366 | val_0_auc: 0.66962 | val_1_auc: 0.66809 |  0:02:49s\n",
      "epoch 37 | loss: 0.32013 | val_0_auc: 0.67185 | val_1_auc: 0.66389 |  0:02:54s\n",
      "epoch 38 | loss: 0.32204 | val_0_auc: 0.67067 | val_1_auc: 0.65276 |  0:02:58s\n",
      "epoch 39 | loss: 0.32334 | val_0_auc: 0.67321 | val_1_auc: 0.66324 |  0:03:03s\n",
      "epoch 40 | loss: 0.32028 | val_0_auc: 0.67776 | val_1_auc: 0.67215 |  0:03:07s\n",
      "epoch 41 | loss: 0.32362 | val_0_auc: 0.67113 | val_1_auc: 0.67489 |  0:03:12s\n",
      "epoch 42 | loss: 0.32136 | val_0_auc: 0.66882 | val_1_auc: 0.6641  |  0:03:16s\n",
      "epoch 43 | loss: 0.32116 | val_0_auc: 0.66786 | val_1_auc: 0.65936 |  0:03:21s\n",
      "epoch 44 | loss: 0.32021 | val_0_auc: 0.66784 | val_1_auc: 0.65813 |  0:03:25s\n",
      "epoch 45 | loss: 0.32086 | val_0_auc: 0.66831 | val_1_auc: 0.6562  |  0:03:30s\n",
      "epoch 46 | loss: 0.31981 | val_0_auc: 0.67168 | val_1_auc: 0.66378 |  0:03:34s\n",
      "epoch 47 | loss: 0.31986 | val_0_auc: 0.68141 | val_1_auc: 0.65884 |  0:03:39s\n",
      "epoch 48 | loss: 0.31936 | val_0_auc: 0.67801 | val_1_auc: 0.6611  |  0:03:43s\n",
      "epoch 49 | loss: 0.31743 | val_0_auc: 0.67746 | val_1_auc: 0.67462 |  0:03:47s\n",
      "epoch 50 | loss: 0.31805 | val_0_auc: 0.66772 | val_1_auc: 0.66691 |  0:03:52s\n",
      "epoch 51 | loss: 0.31622 | val_0_auc: 0.66877 | val_1_auc: 0.67411 |  0:03:56s\n",
      "epoch 52 | loss: 0.3194  | val_0_auc: 0.66637 | val_1_auc: 0.67002 |  0:04:01s\n",
      "epoch 53 | loss: 0.31475 | val_0_auc: 0.66827 | val_1_auc: 0.66141 |  0:04:06s\n",
      "epoch 54 | loss: 0.31773 | val_0_auc: 0.66586 | val_1_auc: 0.65842 |  0:04:10s\n",
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 34 and best_val_1_auc = 0.68224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\miniconda3\\envs\\ML_Torch\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "final_model = TabNetClassifier(\n",
    "    n_d=best_params[\"n_d\"],\n",
    "    n_a=best_params[\"n_a\"],\n",
    "    n_steps=best_params[\"n_steps\"],\n",
    "    gamma=best_params[\"gamma\"],\n",
    "    lambda_sparse=best_params[\"lambda_sparse\"],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_params[\"lr\"]),\n",
    "    mask_type=\"sparsemax\",\n",
    ")\n",
    "\n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val_train, y_val_train), (X_val_test, y_val_test)],\n",
    "    eval_metric=[\"auc\"],\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=2048,\n",
    "    virtual_batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f7df6",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae61a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test AUC: 0.6642736432764657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94      8165\n",
      "           1       0.18      0.00      0.00       982\n",
      "\n",
      "    accuracy                           0.89      9147\n",
      "   macro avg       0.54      0.50      0.47      9147\n",
      "weighted avg       0.82      0.89      0.84      9147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_final = scaler.transform(X_test)\n",
    "\n",
    "proba = final_model.predict_proba(X_test_final)[:, 1]\n",
    "preds = final_model.predict(X_test_final)\n",
    "\n",
    "print(\"Final Test AUC:\", roc_auc_score(y_test, proba))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960fa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

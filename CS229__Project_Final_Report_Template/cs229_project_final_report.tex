\documentclass[journal, 11pt]{IEEEtran}
% \IEEEoverridecommandlockouts
% % The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% %Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage[
    colorlinks = true,
    linkcolor  = blue!60!black,
    citecolor  = blue!60!black,
    urlcolor   = blue!60!black,
    filecolor  = blue!60!black
]{hyperref}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{float}
\usepackage{balance}

\pagestyle{plain}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Early Identification of High-Risk ICU Patients Using Machine Learning}

\author{\IEEEauthorblockN{Rishabh Sharad Pomaje}
\IEEEauthorblockA{\textit{Dept. of Electrical Engineering} \\
\textit{Stanford University}\\
\href{mailto:rishabhp@stanford.edu}{rishabhp@stanford.edu}}
\and
\IEEEauthorblockN{Rutanshu Jhaveri}
\IEEEauthorblockA{\textit{CGOE Student} \\
\href{mailto:rutanshu@stanford.edu}{rutanshu@stanford.edu}}
\and
\IEEEauthorblockN{Shruthi Shekar}
\IEEEauthorblockA{\textit{CGOE Student}\\
\href{mailto:scshekar@stanford.edu}{scshekar@stanford.edu}}}

\maketitle


\begin{abstract}
The abstract is optional, depending on your available space. It should consist of 1 paragraph consisting of the motivation for your paper and a high-level explanation of the methodology you used/results obtained.
\end{abstract}
\section{Introduction}
% Explain the problem and why it is important. Discuss your motivation for pursuing this problem. Give some background if necessary. Clearly state what the input and output is. Be very explicit: “The input to our algorithm is an {image, amplitude, patient age, rainfall measurements, grayscale video, etc.}. We then use a {SVM, neural network, linear regression, etc.} to output a predicted {age, stock price, cancer type, music genre, etc.}.” This is very important since different teams have different inputs/outputs spanning different application domains. Being explicit about this makes it easier for readers. If you are using your project for multiple classes, add a paragraph explaining which components of the project were used for each class.

Hospital readmissions, particularly those that occur shortly after discharge, represent a major clinical and economic burden. In the United States, unplanned readmissions cost the Centers for Medicare and Medicaid Services (CMS) an estimated \$17-26 billion annually \cite{alvarado2023penalty}, while simultaneously straining limited hospital capacity. For patients with chronic conditions such as heart failure or COPD, frequent readmissions are associated with elevated mortality risk and increased psychological and social stress. Early identification of high-risk patients enables targeted interventions, including individualized discharge planning and timely outpatient follow-up. 

With increasing availability of electronic health records (EHRs) and large de-identified datasets such as MIMIC, it is now possible to model heterogeneous patient information spanning demographics, comorbidities, laboratory measurements, vitals, and length of stay. In this work, we experiment with different machine learning models to predict all-cause 30-day readmission using data from a patient's first ICU stay. Our experiments evaluate models of varying complexity, from baseline logistic regression to gradient-boosted decision trees (XGBoost, CatBoost) and Deep Learning models (TabNet). Among these, XGBoost achieves the strongest predictive performance. The input to these models consists of a various laboratory vital test measurements and a few patient details. The models output the probability of readmission which is then used for classification.   

\section{Related Work}
% You should find existing papers, group them into categories based on their approaches, and discuss their strengths and weaknesses, as well as how they are similar to and differ from your work. In your opinion, which approaches were clever/good? What is the state- of-the-art? Do most people perform the task by hand? You should aim to have at least 5 references in the related work. Include previous attempts by others at your problem, previous technical methods, or previous learning algorithms. Google Scholar is very useful for this: https://scholar.google.com/ (you can click “cite” and it generates MLA, APA, BibTeX, etc.)

\section{Dataset and Features}
% Describe your dataset: how many training/validation/test examples do you have? Is there any preprocessing you did? What about normalization or data augmentation? What is the resolution of your images? How is your time-series data discretized? Include a citation on where you obtained your dataset from. Depending on available space, show some examples from your dataset. You should also talk about the features you used. If you extracted features using Fourier transforms, word2vec, histogram of oriented gradients (HOG), PCA, ICA, etc. make sure to talk about it. Try to include examples of your data in the report (e.g. include an image, show a waveform, etc.).

We use the MIMIC-III Clinical Database (PhysioNet) \cite{johnson2016mimic}, which contains de-identified EHR data from more than 40,000 ICU patients at a single center. The dataset includes demographics, laboratory measurements, vital signs, diagnosis codes, clinical notes, and additional patient information. For this study, we focus on a selected set of laboratory test parameters (listed in \autoref{tab:features_chosen}) and construct per-patient feature vectors using the mean, minimum, maximum, and standard deviation of each parameter. These lab values, typically available within 4-12 hours of admission, support early readmission risk assessment. We also incorporate patient age at admission and ICU length of stay during the first visit. The target variable is defined as all-cause ICU readmission within 30 days.

\begin{table*}[t]
    \centering
    \small
    % \setlength{\tabcolsep}{8pt} % adjust column spacing
    % \renewcommand{\arraystretch}{1.2} % improve row spacing
    \begin{tabular}{p{0.30\textwidth} p{0.30\textwidth} p{0.30\textwidth}}
        \toprule
        Anion-Gap & Bicarbonate & Calcium \\
        Chloride & Creatinine & Glucose \\
        Hematocrit & Hemoglobin & MCHC \\
        MCV (Mean Corpuscular Volume) & Magnesium & PT (Prothrombin Time) \\
        Phosphate & Potassium & RDW (Red Cell Distribution Width) \\
        RBC Count & Sodium & Urea-Nitrogen \\
        WBC Count & Age (years) & ICU-time (hrs) \\
        \bottomrule
    \end{tabular}
    \vspace{1em}
    \caption{Static vital parameters and patient attributes used to generate the feature vector for each ICU stay.}
    \label{tab:features_chosen}
\end{table*}

Data extraction from MIMIC-III was a time and compute-intensive components of the project, as the database comprises multiple interlinked tables that must be queried systematically. Following the approach of prior studies \cite{Davis2022, moerschbacher2023building}, we construct a well-defined cohort for model development. Our patient selection criterion are: (i) patients aged 18 years or older; (ii) exclusion of patients who died during their ICU stay; and (iii) exclusion of patients who were eventually readmitted to the ICU but only after a hospital discharge, as these cases do not constitute true readmissions.

\textcolor{red}{TODO: PLEASE EXPLAIN THE REASONING BEHIND THE CRITERION.}

These criteria provide a coarsely refined initial dataset, after which additional preprocessing steps were applied depending on the requirements of each model. In general, we employed standard preprocessing techniques, including imputation for missing values (a common issue in clinical datasets), normalization, data balancing trick,and correlation-based dimensionality reduction. To evaluate the effect of preprocessing, we also trained baseline models without any such adjustments (detailed discussion included in results section). We also dropped non-informative identifiers, such as the ICU stay ID, to prevent introducing unintended bias into the models.

The fully processed dataset was divided into training and test subsets using a 70-30 split. When necessary, the training portion was further partitioned into training and validation sets for hyperparameter optimization. Summary statistics for the final cohort are provided in \autoref{tab:cohort_stats}.

\begin{table*}[t]
    \centering
    \normalsize
    \begin{tabular}{c c c c c}
     \toprule
        & Train Set & Validation Set & Test Set & Total\\
    \midrule
        No. of Entries &  14939 & 6403 & 9147 & 30489\\
        No. of Readmissions & 1605 & 688 & 983 & 3276\\
        (\% Readmissions) & \(\sim\)10.74 & \(\sim\)10.74 & \(\sim\)10.74 & \(\sim\)10.74\\
    \bottomrule
    \end{tabular}
    \vspace{1em}
    \caption{Statistics of our cohort set}
    \label{tab:cohort_stats}
\end{table*}

In all cases, we used stratified splitting to construct the train, validation, and test sets. Stratification preserves the proportion of readmissions across all subsets, which is essential given that the positive class constitutes only about 10.74\% of the cohort.
 
\section{Methods}
% Describe your learning algorithms, proposed algorithm(s), or theoretical proof(s). Make sure to include relevant mathematical notation. For example, you can briefly include the SVM optimization objective/formula or say what the softmax function is. It is okay to use formulas from the lecture notes. For each algorithm, give a short description (\~ 1 para- graph) of how it works. Again, we are looking for your understanding of how these machine learning algorithms work. Although the teaching staff probably know the algorithms, future readers may not (reports will be posted on the class website). Additionally, if you are using a niche or cutting-edge algorithm (e.g. long short-term memory, SURF features, or anything else not covered in the class), you may want to explain your algorithm using 1/2 paragraphs. Note: Theory/algorithms projects may have an appendix showing extended proofs (see Appendix section below).
Prior machine learning-based studies on this task use different datasets or apply different cohort selection criterion prohibiting direct comparison. Consequently, we implemented and evaluated multiple ML Models which we now briefly discuss. 

Prior machine learning studies addressing readmission prediction typically rely on different datasets or adopt distinct cohort selection criteria, making direct comparisons infeasible. To establish a consistent performance baseline for our cohort, we therefore implemented and evaluated multiple machine learning models, briefly described below.

Suppose a patient is represented by the feature vector \(\mathbf{x}\) and \(y\) is used to denote the class (readmission/non-readmission) of that patient.  

\subsection{Logistic Regression}
As a starting point for this binary classification task, we employ logistic regression (LR), a standard linear model widely used in clinical risk prediction (\cite{Tabak2017-hx} uses LR to define a \emph{Readmission Risk Score}). 

Logistic regression models the probability of readmission as
\[
P(y=1 \mid \mathbf{x}) = \sigma(\mathbf{w}^\top \mathbf{x} + b),
\]
where $\sigma(z) = \frac{1}{1 + e^{-z}}$ is the sigmoid function, $\mathbf{w}$ is the weight vector, and $b$ is the bias term. The model parameters are obtained by minimizing a loss function. Despite its simplicity and linear decision boundary, logistic regression provides a strong interpretability advantage and serves as a useful baseline for evaluating more expressive models. 

\subsection{XGBoosted Trees}
While the logistic regression provided a solid starting point, its performance on our data suggested more complex relationships in the data. To capture nonlinear relationships and feature interactions that linear models cannot represent, we employ eXtreme Gradient Boosting (XGBoost) \cite{Chen_2016}, a high-performance ensemble method based on gradient-boosted decision trees. XGBoost constructs a model that cumulatively uses the prediction of multiple decision trees. Mathematically, 
\[
\hat{y} = \sum_{k=1}^{K} f_k(\mathbf{x}), \qquad f_k \in \mathcal{F},
\]
where each \(f_k\) is a regression tree and \(\mathcal{F}\) denotes the space of all possible trees. XGBoost optimizes the following regularized objective:
\[
\mathcal{L} = \sum_{i=1}^{n} l\left(y_i, \hat{y}_i\right) 
+ \sum_{k=1}^{K} \left( \gamma T_k + \frac{1}{2} \lambda \| \mathbf{w}_k \|^2 \right),
\]
where \(l(\cdot)\) is a differentiable loss function, \(T_k\) is the number of leaves in the \(k\)-th tree, \(\mathbf{w}_k\) are the leaf weights, and \(\gamma,\lambda\) are regularization parameters that penalize model complexity. This model is particularly well suited for classification tasks on structured, tabular data, such as the clinical variables used in this project. With appropriate hyperparameter tuning, XGBoost often outperforms more complex architectures while remaining computationally efficient.

\subsection{CatBoost}  
CatBoost \cite{prokhorenkova2019catboostunbiasedboostingcategorical} is another gradient-boosted decision tree algorithm designed to handle categorical features natively without requiring extensive preprocessing. It uses an ordered boosting scheme to reduce overfitting and employs symmetric trees for computational efficiency. Like XGBoost, CatBoost minimizes a regularized loss function. CatBoost is effective for tabular datasets, especially when categorical variables play an important role. It also exhibits superior performance without any parameter tuning, as we observed in our experiments.

\subsection{TabNet}  
TabNet \cite{arik2020tabnetattentiveinterpretabletabular} is a deep learning architecture specifically designed for tabular data. Unlike traditional fully connected networks, TabNet uses sequential attention to choose which features to reason from at each decision step, allowing the model to focus on the most relevant information. It combines feature selection and representation learning in a single framework. The model optimizes a standard classification loss (e.g., binary cross-entropy for readmission prediction) and includes sparse regularization on the feature selection masks to improve interpretability and prevent overfitting. TabNet can capture complex, non-linear interactions among features, making it a competitive alternative to gradient-boosted trees for tabular datasets.

\section{Experiments, Results, and Discussion}
In this section, we discuss and analyze the experiments\footnote{The source code of experiments is available at \href{https://github.com/RishP11/CS229_Final_Project-Predicting-Hospital-Readmission-Risk-using-ML.git}{\texttt{GitHub}}} conducted during this project.  

As mentioned previously, the dataset is imbalanced, with only about 10\% of patients experiencing readmission. Consequently, accuracy is not an appropriate metric for model performance. Instead, we prioritize AUROC for model evaluation. Table~\ref{tab:model_performance} summarizes the performance of all evaluated models. In addition, we report weighted (class-frequency weighted) and macro (equal-weighted) precision and recall metrics.  

\begin{table*}[htbp]
  \centering
  \normalsize
  \begin{tabular}{l c c c c c}
    \toprule
    Model & \multicolumn{2}{c}{Precision} & \multicolumn{2}{c}{Recall} & AUROC \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
          & Weighted & Macro & Weighted & Macro & \\
    \midrule
    LogReg (Base) & 0.54 & 0.78 & 0.50 & 0.86 & 0.670 \(\pm\) 0.025 \\
    LogReg (Balanced) & 0.56 & 0.82 & 0.62 & 0.66 & 0.668 \(\pm\) 0.025 \\
    LogReg (Imputation) & 0.67 & 0.85 & 0.51 & 0.89 & 0.709 \(\pm\) 0.017 \\
    XGBoost (Base) & 0.60 & 0.83 & 0.52 & 0.89 & 0.687 \(\pm\) 0.017 \\
    XGBoost (HyperOpt) & 0.63 & 0.85 & 0.60 & 0.87 & 0.745 \(\pm\) 0.015 \\
    XGBoost (Imputed) & 0.64 & 0.85 & 0.59 & 0.88 & 0.745 \(\pm\) 0.015 \\
    CatBoost & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
    TabNet & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
    \bottomrule
  \end{tabular}
  \vspace{1em}
  \caption{Performance metrics of different machine learning models for ICU readmission prediction.}
  \label{tab:model_performance}
\end{table*}

\subsection{Logistic Regression Experiments}
Logistic regression (LogReg) provides a solid baseline for binary classification. Its simplicity allows rapid experimentation with preprocessing techniques. All LogReg experiments were implemented using the \texttt{scikit-learn} Python library \cite{scikit-learn}. Since logistic regression cannot handle missing values, stricter preprocessing was required, reducing the dataset to 9,774 examples (\(\sim 13\%\) readmission rate). Despite this limitation, the model remains a useful baseline because the data is drawn from the same distribution.  

\begin{enumerate}
  \item \textbf{LogReg (Base)}: The first exploration used default parameters: solver = \texttt{lbfgs}, convergence tolerance = 0.0001, regularization = \(L_2\), maximum iterations = 500,000. This baseline model achieved an AUROC of 0.670 and helped us gain initial insights into the dataset.  

  \item \textbf{LogReg (Balanced)}: To address class imbalance, we experimented with (i) minority oversampling and (ii) majority undersampling with loss upweighting. These strategies had minimal effect, yielding an AUROC of 0.668. The downscaling factor of 4 (i.e., \(\frac{\mathtt{num\_minority}}{\mathtt{num\_majority}} = \frac{1}{4}\)) was determined through trial and error, with similar performance observed for values between 3 and 5.  

  \item \textbf{LogReg (Imputation)}: Missing data was handled using median imputation. To prevent data leakage, the imputer was fitted only on the training set and then applied to the test set. This approach slightly improved performance, achieving an AUROC of 0.709.
\end{enumerate}

Overall, the performance of the LogReg model was consistent with previous studies. The lack of improvement from data balancing techniques reflects the complex distribution and inherent noise characteristic of medical datasets. Hence, we did not apply synthetic sampling methods to the more complex models. In contrast, imputing missing values led to improved performance, likely due to the increased amount of data available for training and testing. This again points to the intricate relationships present in the dataset, which can be better captured when more complete data is provided to the model.

\subsection{XGBoost Experiments}
Motivated by the complex relationships present in the data, we turned to a more powerful model known to perform well on tabular datasets: XGBoost. All experiments were conducted using the \texttt{xgboost} Python library \cite{xgboostlibcite}. We performed three sets of experiments:

\begin{enumerate}
  \item \textbf{XGB (Base)}: The initial model used default hyperparameters\footnote{A detailed list of default parameters is available \href{https://xgboost.readthedocs.io/en/stable/parameter.html\#global-configuration}{here}}, including learning rate = 0.3 and maximum tree depth = 6. This baseline model achieved an AUROC of 0.687, demonstrating its ability to capture non-linear interactions among features out of the box, while also highlighting the potential benefit of hyperparameter tuning.

  \item \textbf{XGB (HyperOpt)}: Hyperparameter tuning was performed using the Optuna library, leveraging the validation set to prevent data leakage. The search space was informed by parameter ranges reported effective in prior studies. The Tree-Structured Parzen Estimator (TPE) algorithm was used for optimization, as it has been shown to efficiently explore hyperparameter spaces \cite{bergstra2011algorithms}. The optimal parameters identified were: learning rate = 0.00105, max tree depth = 17, and minimum  child weight = 15. A complete list of tuned parameters is available in the associated Jupyter notebook on GitHub. The model resulted in an AUROC score of 0.745 on using scaled weights equal to the fraction of readmissions to handle class imbalance.

  \item \textbf{XGB (Imputed)}: This experiment was meant to study the impact of imputation with XGBoost model. As before, the simple median based strategy was applied prior hyperparameter optimization (as described in the previous experiment). 
\end{enumerate}


\textbf{Observations:} Similar to other tree-based models, XGBoost was relatively insensitive to class imbalance techniques. The ability of XGBoost to internally handle missing values and find the optimal splits factoring the missing data improves the performance. The results also indicate the importance of  hyperparameter tuning and careful handling of missing, imbalanced data can meaningfully enhance predictive performance, particularly in complex clinical datasets.

\subsection{CatBoost and TabNet}
The experiments using CatBoost and TabNet were out of pure curiosity. 

\section{Conclusion and Future Work}


\section{AI-Use Disclosure}
The use of large language models (LLMs), such as ChatGPT, was \textbf{in accordance} with the course policy. These tools were employed solely for querying documentation of specialized libraries and for minor coding assistance. Additionally, LLMs were used to improve the grammar, clarity and presentation quality of this report. No intellectual contribution to the content, analysis, or interpretation of results was derived from AI. 

\newpage
\section{Contributions}
\begin{itemize}
    \item \textbf{Rishabh Sharad Pomaje}: Literature review, Fine-grained (minor) data-processing. He designed, implemented and verified LogReg, XGBoost, CatBoost and TabNet experiments. He was responsible for final report writing and presentation.
    \item \textbf{Rutanshu Jhaveri}: Literature review, Study of the database, primary (major) data fetching using Queries and pre-processing, and implemented base XGBoost. 
    \item \textbf{Shruthi Shekhar}: Literature review, feature selection and qualitative (high-level) cohort data selection criterion, qualitative field-specific analysis of the results.
\end{itemize}
\balance
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
